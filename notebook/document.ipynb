{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72ff1bb",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1642cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b356613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'John Doe', 'date_created': '2024-06-15'}, page_content='This is the content of the document.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content=\"This is the content of the document.\",\n",
    "    metadata={\n",
    "        \"source\": \"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"John Doe\",\n",
    "        \"date_created\": \"2024-06-15\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5510873",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf714033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created.\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"../data/text_files/doc1.txt\": \"\"\"LangChain is a framework for developing applications powered by language models.\n",
    "It enables developers to build applications that can understand and generate human-like text.\n",
    "LangChain provides tools for prompt management, memory, and integration with various data sources.\n",
    "    \"\"\",\n",
    "    \"../data/text_files/doc2.txt\": \"\"\"Python is a versatile programming language known for its readability and ease of use.\n",
    "It is widely used in web development, data analysis, artificial intelligence, and scientific computing.\n",
    "Python has a large standard library and a vibrant ecosystem of third-party packages.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "for file_path, content in sample_texts.items():\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27d32a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/doc1.txt'}, page_content='LangChain is a framework for developing applications powered by language models.\\nIt enables developers to build applications that can understand and generate human-like text.\\nLangChain provides tools for prompt management, memory, and integration with various data sources.\\n    ')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/doc1.txt\", encoding=\"utf-8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d070252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1570.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\doc1.txt'}, page_content='LangChain is a framework for developing applications powered by language models.\\nIt enables developers to build applications that can understand and generate human-like text.\\nLangChain provides tools for prompt management, memory, and integration with various data sources.\\n    '), Document(metadata={'source': '..\\\\data\\\\text_files\\\\doc2.txt'}, page_content='Python is a versatile programming language known for its readability and ease of use.\\nIt is widely used in web development, data analysis, artificial intelligence, and scientific computing.\\nPython has a large standard library and a vibrant ecosystem of third-party packages.\\n    ')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\", \n",
    "    glob=\"**/*.txt\", \n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5189ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 16.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 0}, page_content='A Multi-task Model for Sentiment Aided Stance Detection of\\nClimate Change Tweets\\nApoorva Upadhyaya, Marco Fisichella, Wolfgang Nejdl\\nL3S Research Center, Leibniz Universit¨at Hannover, Hannover, Germany\\nupadhyaya@l3s.de, mﬁsichella@l3s.de, nejdl@l3s.de\\nAbstract\\nClimate change has become one of the biggest challenges of\\nour time. Social media platforms such as Twitter play an im-\\nportant role in raising public awareness and spreading knowl-\\nedge about the dangers of the current climate crisis. With the\\nincreasing number of campaigns and communication about\\nclimate change through social media, the information could\\ncreate more awareness and reach the general public and pol-\\nicy makers. However, these Twitter communications lead to\\npolarization of beliefs, opinion-dominated ideologies, and of-\\nten a split into two communities of climate change deniers\\nand believers. In this paper, we propose a framework that\\nhelps identify denier statements on Twitter and thus classi-\\nﬁes the stance of the tweet into one of the two attitudes to-\\nwards climate change (denier/believer). The sentimental as-\\npects of Twitter data on climate change are deeply rooted\\nin general public attitudes toward climate change. There-\\nfore, our work focuses on learning two closely related tasks:\\nStance Detection and Sentiment Analysis of climate change\\ntweets. We propose a multi-task framework that performs\\nstance detection (primary task) and sentiment analysis (aux-\\niliary task) simultaneously. The proposed model incorporates\\nthe feature-speciﬁc and shared-speciﬁc attention frameworks\\nto fuse multiple features and learn the generalized features\\nfor both tasks. The experimental results show that the pro-\\nposed framework increases the performance of the primary\\ntask, i.e., stance detection by beneﬁting from the auxiliary\\ntask, i.e., sentiment analysis compared to its uni-modal and\\nsingle-task variants.\\n1\\nIntroduction\\nClimate change is the burning crisis of our time, and it\\nis happening even faster than we thought. A recent article\\non the BBC News website1 states that many of the effects\\nof global warming are now simply “irreversible” accord-\\ning to the latest assessment and that more than 40% of the\\nworld’s population is “at high risk” from climate. In fact,\\naccording to a report by the Intergovernmental Panel on Cli-\\nmate Change (IPCC), it is very likely that climate change is\\ncaused by man-made activities (Myhre et al. 2013). Despite\\nthe scientiﬁc consensus on the causes and main impacts of\\nCopyright © 2021, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.\\n1https://www.bbc.com/news/science-environment-60525591\\nclimate change, it remains a controversial topic in public dis-\\ncourse. Therefore, understanding public perceptions plays a\\ncritical role in addressing climate change by increasing the\\npublic’s willingness to accept appropriate action on climate\\nchange (Shi, Visschers, and Siegrist 2015).\\nRecently, social media platforms such as Twitter have played\\nan important role in raising public awareness of the current\\nclimate crisis and inﬂuencing public attitudes toward cli-\\nmate change (Lineman et al. 2015). Twitter conversations,\\nhowever, are often inﬂuenced by the polarization of beliefs\\nand, in the case of climate change, are divided into two com-\\npeting groups, one that believes in climate change (Believ-\\ners) and a second that is skeptical or denies that climate\\nchange is occurring (Disbelievers) (Jang and Hart 2015;\\nCann, Weaver, and Williams 2021). The article published\\non the Euronews website2 after the COP26 conference re-\\nvealed that scientists have found that climate change deniers\\nare not only skeptical about climate change, but have also led\\nto the problem of delaying climate change by either shifting\\nresponsibility or eventually capitulating - the idea that it is\\nnot possible to prevent climate change, which often leads to\\nthe spread of misinformation (Zhou and Shen 2021). There-\\nfore, it is important for government agencies, researchers,\\nand technology companies to monitor such content on so-\\ncial media to identify and intervene in tweets from climate\\nchange deniers, which will help combat climate misinfor-\\nmation. This has motivated us to identify such content and\\nunderstand public attitudes toward climate change by per-\\nforming the important task of stance detection.\\nStance detection is the task of automatically identifying\\nthe author’s point of view in relation to the target object\\n(for/against/neutral). It has been used to identify social atti-\\ntudes toward pressing issues (e.g., covid19 vaccination (Ar-\\ngyris et al. 2021), climate change, abortion, feminism (Mo-\\nhammad et al. 2016)). In our study, we focus on climate\\nchange as a target and perform statement-level stance detec-\\ntion. The goal is to predict the attitude expressed in a single\\ntweet, where the stance is for (believers) or against climate\\nchange (deniers).\\nNumerous works have classiﬁed tweets into favor or against\\n2https://www.euronews.com/green/2021/11/18/climate-\\nmisinformation-is-getting-more-sophisticated-and-experts-say-\\ncop26-progress-could-\\narXiv:2211.03533v1  [cs.CL]  7 Nov 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 1}, page_content='the target using the SemEval 2016 benchmark dataset. The\\ndataset contains 5 target topics, including climate change,\\nwith 29 denier and 335 believer tweets (Li and Caragea\\n2019; Wang et al. 2020). However, due to the small number\\nof climate-speciﬁc tweets, these techniques do not focus on\\nunderstanding the speciﬁc characteristics of climate change\\ndeniers and believers. This is because climate change de-\\nniers not only deny climate change, but also disagree with\\nthe solutions to combat climate change, which often leads\\nto public negligence (Zhou and Shen 2021). Other works\\nthat deal with the classiﬁcation of tweets on climate change\\nlack a suitable architecture that efﬁciently performs the clas-\\nsiﬁcation task (Kabaghe and Qin 2019). Therefore, in our\\nwork, we use a multi-tasking approach that uses different\\nattention frameworks to classify the attitude of the climate\\nchange tweet into one of the two polarized classes (de-\\nniers/believers) to identify the denier statements on Twitter.\\nSentiment analysis has helped in many other tasks, such as\\ndetecting hate speech and sarcasm in a multi-task architec-\\nture (Majumder et al. 2019). Studies on climate change have\\nalso justiﬁed the role of sentiment in conversations about cli-\\nmate change, either by assessing sentiment in tweets from\\nclimate change deniers or by examining the emotional im-\\npact of climate change data on Twitter (Dahal, Kumar, and\\nLi 2019; El Barachi et al. 2021). Therefore, in our study, we\\nleverage the sentiment analysis task to decipher the attitude\\nof the tweets.\\nIn addition, we use multiple inputs, i.e., combining tweet\\ntext and topic representations, to build a reliable classiﬁca-\\ntion model that helps identify the sentiment of the tweeter\\nand determine the correct tweet attitude. Topic represen-\\ntations have helped in detecting fake news by providing\\nmore discriminatory power to the model used (Gautam, V,\\nand Masud 2021). In our study, topic embedding provides a\\nglobal context to a single tweet and thus more information\\nand can circumvent the drawback of the short length of the\\ntweet text to efﬁciently train the proposed system.\\nOur work focuses on learning two closely related tasks,\\nstance detection and sentiment analysis of tweets on cli-\\nmate change. Stance detection is our main task, which is\\nsupported by sentiment analysis as an auxiliary task. We\\npropose a multi-task framework that incorporates feature-\\nspeciﬁc and shared-speciﬁc attention frameworks to fuse\\nmultiple features and learn the features for both tasks. Our\\nproposed approach is useful for government agencies and\\ntechnology companies to detect the attitude of posts (de-\\nniers/believers) and curb the spread of such content that de-\\nnies climate change and is false or misleading to combat cli-\\nmate misinformation.\\nWe summarise the contributions of our work as follows: (i)\\nWe create a new dataset3 for the climate change domain\\nconsisting of tweets with the stance and sentiment labels\\nwhich is beneﬁcial for the research community. (ii) We il-\\nlustrate the importance to consider the sentiment associated\\nwith the tweet while categorising the stance of tweet into fa-\\nvor(believers) or against(deniers) climate change. (iii) We\\n3The dataset and code are available at the repository:\\nhttps://github.com/apoorva-upadhyaya/Climate-Change-Tweets.\\npropose a multi-task framework that jointly performs the\\nstance detection (primary) and sentiment analysis (auxil-\\niary) tasks. We integrate feature-speciﬁc and shared-speciﬁc\\nattention frameworks to integrate information across mul-\\ntiple features and shared tasks to learn features that opti-\\nmise task performance. Experimental results indicate that\\nthe proposed framework increases performance of the pri-\\nmary task, i.e., stance detection by beneﬁting from the aux-\\niliary task, i.e., sentiment analysis compared to its uni-modal\\nand single-task variants.\\n2\\nRelated Works\\nClimate change has become one of the greatest challenges\\nof our time. Several works explore the role of psychology in\\nclimate change by examining the impact of climate change\\non people’s well-being and perceptions (Clayton 2020). En-\\ngaging the public is an important part of addressing climate\\nchange. Therefore, social media platforms like Twitter al-\\nlow anyone to explore and report public viewpoints on the\\ncomplex issue of climate change (Lineman et al. 2015; Da-\\nhal, Kumar, and Li 2019). However, debates and discussions\\non Twitter about climate change are widely associated with\\nincreasing polarization and are often divided into climate\\nchange believers and deniers (Jang and Hart 2015). In order\\nto identify and understand public attitudes toward climate\\nchange, the task of identifying attitudes plays an important\\nrole.\\n2.1\\nStance Detection\\nStance detection is about classifying the attitude that the au-\\nthor expresses towards a target object. The author may sup-\\nport the target object, reject it, or have a neutral stance. In our\\nwork, we focus on the climate change target, where opinions\\nare either for climate change (believers) or against climate\\nchange (deniers). There are several climate change speciﬁc\\nstudies where the goal is to predict a user’s attitude (Chen,\\nZou, and Zhao 2019; Tyagi et al. 2020a). It has been found\\nthat multiple tweets from the same user can have different\\nstance classes. In order not to miss the denier attitude of a\\nsingle tweet that could interfere with the implementation of\\nclimate change policies, we focus on detecting statement-\\nlevel stance detection, where the goal is to predict the stance\\ndescribed in a single tweet.\\nThe stance detection of tweets has been studied in a variety\\nof work on the popular SemEVAL 2016 dataset, which in-\\ncludes the 5 targets including climate change (with 364 cli-\\nmate change tweets) (Vychegzhanin and Kotelnikov 2021;\\nWang et al. 2020). However, in these previous studies, lit-\\ntle attention was paid to understanding the characteristics of\\nclimate change denier and believer tweets in particular (only\\n29 climate change denier tweets in the SemEVAL 2016\\ndataset). Recently, (Luo, Card, and Jurafsky 2020) published\\na stance-annotated dataset on global warming and proposed\\nan opinion framing task to explore the discourse used in the\\nglobal warming debate. One of the papers (Kabaghe and Qin\\n2019) classiﬁed tweets into three classes based on their atti-\\ntude toward climate change: −1 (negative belief), 0 (neutral\\nbelief), and 1 (positive belief). However, the lack of an ar-\\nchitectural framework led us to propose an efﬁcient model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 2}, page_content='that can efﬁciently classify a tweet’s attitude toward climate\\nchange into one of two categories (deniers/believers) in real-\\ntime, while taking advantage of the other task.\\n2.2\\nSentiment Analysis\\nSome of the works on stance recognition have emphasized\\nthe importance of sentiment (Wang et al. 2020), while some\\nhave cited the orthogonal relationship between stance and\\nsentiment of the statement (Sen, Fl¨ock, and Wagner 2020).\\nHowever, several works have focused on the sentimental as-\\npects of climate change conversations and justiﬁed their role\\nin climate change (Cody et al. 2015; Jiang et al. 2017). One\\nrecent work (El Barachi et al. 2021) proposes a real-time\\nframework that uses sentiment and emotion analysis to pro-\\nvide meaningful insights into public opinion, and tested the\\nmodel with tweets posted by Greta Thunberg and her follow-\\ners on climate change. These studies motivated us to inves-\\ntigate the role of sentiment in classifying tweets on climate\\nchange.\\n2.3\\nMulti-Task Learning\\nMulti-task learning (MTL) (Caruana 1997) is a learning\\nparadigm that aims to learn multiple related tasks together\\nin the hope of improving generalization performance for all\\ntasks at hand. In our work, we focus on learning stance de-\\ntection task with the help of sentiment analysis in a multi-\\ntasking system. The study by (Li and Caragea 2019) uses\\nsentiment to predict the stance through a multi-task learn-\\ning model. Another work by (Chauhan, Kumar, and Ekbal\\n2019) uses sentiment as an auxiliary task to predict attitude.\\nHowever, in our work, multiple features in the form of text\\nand topic words are used to separate the task-dependent and\\nindependent feature spaces and perform both tasks simul-\\ntaneously by using attention frameworks to focus on the\\nmost important feature representations and discarding the\\nuseless shared features that may affect the performance of\\nboth tasks.\\n3\\nDataset\\nIn this section, we describe the Twitter datasets we use for\\nour experiments. We consider both publicly available tweets\\nabout climate change and live tweets about climate change\\nthat we collect using hashtags from the previous literature.\\nWe ﬁrst discuss data collection strategies, followed by data\\nlabeling and data pre-processing techniques. The qualitative\\naspect and the temporal analysis of the dataset is described\\nin the Supplementary.\\n3.1\\nData Collection Method\\nPrevious works have used hashtags to identify the stances\\nof different groups on social media, creating a method of\\ntagging content by topic (Misra et al. 2016), we also use\\nthis explicit annotation quality of hashtags to collect a larger\\ndataset. First, we select the hashtags for deniers and believ-\\ners (Tyagi et al. 2020a) from the previous literature (see\\nrow 1 of the table 1). We then consider the two publicly\\navailable Twitter datasets on climate change, (i) tweet IDs\\ncollected from September 21, 2017 to May 17, 2019 using\\nSources\\nDenier Hashtags\\nBeliever Hashtags\\nTyagi et al. 2020a\\nClimateHoax, YellowVests,\\nQanon\\nClimateChangeIsReal,\\nClimateActionNow,\\nFactsMatter,\\nScienceMatters,\\nScienceIsReal\\nMost Used & Co-occur\\nfrom public data\\nGlobalWarmingHoax,\\nClimateChangeHoax,\\nClimateDenial,ClimateHoax\\nSaveClimate,\\nActOnClimate\\nTable 1: Denier & Believer Seed Hashtags\\nDataset\\nTotal Tweets\\nAs per seed Hashtag\\nAs per label prop.\\nalgo (out of\\nseed hashtag)\\n-\\n-\\nDenier\\nBeliever\\nDenier\\nBeliever\\nHarvard Data\\n(Littman and\\nWrubel 2019)\\n1322969\\n1595\\n20886\\n1042\\n18421\\nCredibility Data\\n(Samantray\\nand Pin 2019)\\n9672907\\n4087\\n11225\\n2883\\n7735\\nLive tweets\\n5711743\\n10594\\n39787\\n9200\\n34274\\nTotal tweets\\n-\\n16276\\n71898\\n13125\\n60430\\nTable 2: Dataset Statistics for Stance Detection Task\\na set of climate change keywords available in the Harvard\\nDataverse (Littman and Wrubel 2019), and (ii) tweet IDs\\nused in the work (Samantray and Pin 2019) collected from\\n2007 to 2019. We start by retrieving the tweet objects from\\nthese publicly available tweet IDs using the Tweepy API4.\\nWe analyzed the hashtags used in the collected tweets and\\nfound that the hashtags mentioned in row 2 of the table 1\\nare the most frequently used and co-occur with the denier\\nand believer seed hashtags. We draw a random sample of\\n1000 tweets containing these most frequently used hashtags\\nseparately for both categories and identify 98% tweets as\\ndeniers and 99% tweets as believers. We conclude that the\\nﬁnal set of seed hashtags (see table 1) can be used to iden-\\ntify tweets from deniers and believers. Because the publicly\\navailable datasets contain tweets with a large time span that\\nreﬂect Twitter trends and topics related to climate change\\nand cover a wide range of audiences, they helped us identify\\nadditional relevant hashtags associated with climate change\\ndeniers and believers and enriched us with a ﬁnal set of seed\\nhashtags that can be used for further data collection related\\nto climate change deniers and believers. We then select the\\nunique tweets (excluding retweets) from the collected data\\nthat contain either the denier or believer hashtags. We ob-\\ntain a total of 5, 682 denier and 32, 111 believer tweets after\\nthe ﬁltering process based on seed hashtags. The collected\\ndataset appears to be relatively small, with a smaller number\\nof denier tweets, so we collect the real-time tweets from July\\n28 to December 26, 2021 using the live-streaming Tweepy\\nAPI with the ﬁnal set of seed hashtags. The number of tweets\\nﬁltered out as deniers and believers according to the seed\\nhashtags from various sources are listed in the table 2.\\n3.2\\nData Annotation\\nStance Detection\\nTo validate the stances of the collected\\ntweets provided by the hashtag self-annotation technique,\\nwe run a variant of label propagation algorithm (Tyagi et al.\\n4https://docs.tweepy.org/en/stable/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 3}, page_content='Algorithm 1: Label Propagation Algorithm\\ninput: Graph G with nodes n and edges e with eij as\\nedge weight between i ∈n and j ∈n\\ninitialize γ=50/100 and i=0 ;\\nfor each n do\\ndeﬁne l = integer(i/γ); i+=1 ;\\nfor each n do\\nif n not labeled then\\ncompute t = neighbors of n ;\\ncompute tl = labeled neighbors of n ;\\nif tl + l ≥t then\\ninitialize score, c ;\\nfor each ti ∈t do\\nscore+ = label ti ∗enti;\\nc+=enti;\\n2020a,b), which transfers the labels from the seed hashtags\\nto other hashtags (refer Algorithm 1). The authors who pro-\\nvided the label propagation algorithm claim that their ap-\\nproach is similar to various other works (Weber, Garimella,\\nand Batayneh 2013; Garimella et al. 2018). First, we weight\\nthe seed hashtags of believers by +1 and those of deniers\\nby −1 (as in table 1). We create a weighted hashtag∗hashtag\\nco-occurrence graph with all hashtags present in the data,\\nwhere each node represents a hashtag and an edge is created\\nbetween the hashtags that occur in the same tweet, where the\\nweight of the edge is proportional to the frequency of their\\nco-occurrence. The weights of the seed hashtags are then\\ntransferred to other hashtags as speciﬁed in the algorithm 1.\\nThe hashtag scores (believer hashtag = +1, denier hashtag =\\n−1) are then arithmetically summed for all hashtags that oc-\\ncur in each tweet and then averaged. The ﬁnal score is then\\nused to classify tweets into deniers ( score < 0) or believers\\n( score > 0). In total, we found a set of 13, 125 denier and\\n60, 430 believer tweets (refer table 2). Three trained annota-\\ntors drew a random sample of 1000 tweets from both cate-\\ngories and manually annotated them. To determine the con-\\nsistency between the ratings of the annotators, we use the\\nFleiss-Kappa (Spitzer et al. 1967) measure and achieve an\\nagreement score of 0.84, indicating that the annotations are\\nof good quality. We consider the manually annotated tweets\\nas the ground truth and compared them with the annotations\\nfound after the label propagation algorithm. We found that\\n98.40% of denier tweets belong to the denier category and\\n99.6% of believer tweets belong to the believer category.\\nTherefore, to save time and cost, we consider the labels gen-\\nerated by the label propagation algorithm as the ﬁnal labels\\nfor denier and believer tweets. The steps for data collection\\nand data annotation for the stance detection task are brieﬂy\\ndescribed in Fig. 1 in the Supplementary.\\nSentiment Analysis\\nWe leverage weak supervision ap-\\nproach to annotate tweets for sentiment analysis. Similar to\\nprevious work (Singh et al. 2021), we use three sentiment\\nclassiﬁers to generate sentiment labels for each preprocessed\\ntext of the tweet, namely (i) VADER (Hutto and Gilbert\\n2014): a popular lexicon and rule-based sentiment analysis\\nTweets\\nNegative\\nPositive\\nNeutral\\nDenier\\n60.2%\\n18.2%\\n21.6%\\nBeliever\\n24.7%\\n46.2%\\n29.1%\\nTable 3: Data Statistics for Sentiment Task\\ntool that relies on a dictionary to generate sentiment scores,\\n(ii) TextBlob5: a Python-based library that provides an API\\nfor handling common NLP tasks such as sentiment analy-\\nsis, POS tagging, etc., and (iii) NLTK6: a Python bundle that\\nprovides a collection of NLP algorithms such as sentiment\\nanalysis, NER, etc. This results in 3 labels (positive, nega-\\ntive, neutral) per tweet, from which a single label is ﬁnally\\nselected as the sentiment expressed by the tweet based on the\\nmajority voting based ensemble method. The data statistics\\nis mentioned in table 3. Three trained annotators manually\\nevaluated the labels for 1000 randomly selected tweets and\\nobtained an inter-annotator agreement score of 0.81 using\\nthe Fleiss-Kappa measure. We consider the ﬁnal annotations\\ngenerated after inter-annotator agreement as the ground truth\\nand compared them with the annotations generated using the\\nweak supervision approach and found an accuracy of 97.6%.\\nTo save time and cost, we consider the annotations generated\\nusing the weak supervision approach for the sentiment anal-\\nysis task.\\n3.3\\nData pre-processing\\nData pre-processing is important because raw tweets with-\\nout pre-processing are very unstructured and contain redun-\\ndant and often problematic information that affects the per-\\nformance of the model training and classiﬁcation task.\\nText\\nWe remove mentions, URLs, punctuation, spaces,\\nand unwanted characters such as RT (retweet), CC (carbon\\ncopy), and stopwords from the tweet text. We use ekphrasis\\n(Baziotis, Pelekis, and Doulkeridis 2017) to extract hashtags\\nby segmenting long strings into their individual words. For\\nfurther text processing, we use the Python toolkit NLTK.\\nThe NLTK-based tokenizer is used to tokenize tweets. All\\nwords are converted to lowercase letters. Then, we reduce\\nthe inﬂected words by applying the NLTK Wordnet lemma-\\ntizer, and then apply PorterStemmer for stemming.\\nTopic\\nWe ﬁrst remove the seed hashtags used for data col-\\nlection, otherwise topics created can be biased towards the\\nhashtags. The tweet text is pre-processed using the proce-\\ndure described above. In this study, we use BERTopic mod-\\neling, which uses transformer-based embeddings to create\\neasily interpretable topics and their distributions (Grooten-\\ndorst 2020). This modeling technique has recently gained\\npopularity and provided promising results in previous stud-\\nies (Anwar et al. 2021), therefore we focus on using\\nBERTopic that detect semantic similarity and integrate top-\\nics with pre-trained contextual representations. The tweet\\ntext is then fed into the BERTopic library with the calcu-\\nlate probabilities=True, which creates topics from the data\\nand assigns a probability score to each created topic for each\\n5https://textblob.readthedocs.io/en/dev/\\n6https://www.nltk.org/index.html'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 4}, page_content='Category\\nTweet\\nTopic Words\\nDenier\\nCO2 is greening the planet\\nand restoring the rainforest.\\nIts almost like the planet is\\nable to self regulate #ClimateHoax .\\nnonsense, hoax, science,\\ndenial, destroy, ridiculous,\\nmyth, planet, hypocrisy...\\nBeliever\\nGreat format and read #climate\\n#ClimateActionNow\\nwarm, hot, earth, real,\\nemergency, possible, crisis,\\nurgent, sun, warming,...\\nTable 4: Examples with stance, tweet text and topic words\\ntweet sample in the data. We select the m-most similar top-\\nics for each tweet sample, where each topic is represented\\nby the top ‘p’ topic words.\\nRole of Topic words as Feature In table 4, we present two\\nsamples from the dataset that illustrate the importance of\\nconsidering topic words along with the tweet text for the\\nanalysis tasks. It can be observed that the tweet text alone\\nis not helpful in identifying the stance of the tweet in the\\ngiven samples. However, the addition of topic words gives\\nthe tweet more context and information that helps in cor-\\nrectly predicting the denier or believer stance of the tweet.\\nThese examples show that the presence of complementary\\ninformation in the form of topic words aids the process of\\nstance detection.\\n4\\nMethodology\\nIn this section, we outline the working of our proposed\\nmulti-task model for the stance detection (primary task) and\\nthe sentiment analysis (auxiliary task). The proposed model\\nconsists of the following components : Feature Extraction,\\nAttention Framework, and Classiﬁcation Layer. The key fac-\\ntor of multi-task learning is the sharing of features across\\nthe tasks. Therefore, we ﬁrst describe the two variants of the\\nmodel depending on the framework of multi-task learning\\nand then explain the model components in detail.\\n4.1\\nVariants of the Proposed Model\\nWe depict the following 2 variants of the multi-task model\\ndepending on the sharing of features across the tasks:\\n(i)Shared-Only Multi-Task Model (SO-MT) : In the SO-MT\\nmodel, we use single shared layers for the feature extractor\\nand the attention framework to extract features for all tasks,\\nas shown in Fig. 1. The single shared output of the attention\\nframework is then used as an input to the classiﬁcation layer,\\nwhich produces separate outputs for the stance and senti-\\nment tasks. This model focuses on the task-invariant features\\nand ignores the fact that some features are task-dependent.\\n(ii)Shared-Private Multi-Task Model (SP-MT): In the\\nmodel SP-MT, we have two separate feature spaces for the\\ntwo tasks that capture task-speciﬁc features, and a single\\nshared feature space that captures the task-invariant features\\n(refer Fig. 2). The ﬁnal features are the concatenation of the\\nfeatures from the private space and the shared space, which\\nare then fed into the classiﬁcation layer to generate the out-\\nput for both tasks.\\nThe input and output of each model component for both vari-\\nants (Figures 1 and 2) are mentioned in the following sub-\\nsection 4.2. We now describe each of the model components\\nin detail.\\nFigure 1: Architectural diagram of the Shared-Only Multi-\\nTask (SO-MT) Framework.\\n4.2\\nComponents of the Model\\nFeature Extraction\\nText : Each tweet text ‘T’ contains nt number of words,\\nwhere the embedding of each word w1, ..., wnt is acquired\\nfrom BERT (Devlin et al. 2019) with dimension(d) = 768.\\nWe obtain ﬁnal embedding for each tweet text as T\\n∈\\nRnt×d. Since Bi-LSTM has shown excellent performance in\\ntext classiﬁcation due to its ability to learn long-term depen-\\ndencies and incorporate past and future context information\\nwithout retaining duplicate information (Zhou et al. 2016),\\nwe use Bi-LSTM to sequentially encode the embedded input\\ntext representations. The embedded text is fed to Bi-LSTM\\nwith dimension dl, which learns the long-term context de-\\npendent semantic features into hidden states. The ﬁnal hid-\\nden matrix of text is Ht ∈Rnt×2dl.\\nTopic : Each tweet is tagged with the number of m-most\\nsimilar topics based on the probability score assigned to each\\nof the topics, where each topic is represented by the top ‘p’\\ntopic words created using BERTopic (described in section\\n3.3). Here we represent the topic feature as ‘U’ containing\\na set of nu words (nu ≤m × p), where the representa-\\ntion of each word w1, ..., wnu are obtained from BERT with\\nd = 768. We obtain ﬁnal embedding for each tweet topic\\nas U ∈Rnu×d. This representation of topic is then fed to\\nthe Bi-LSTM layer with dl that sequentially encodes these\\nrepresentation, and gives the ﬁnal hidden matrix of topic\\nHu ∈Rnu×2dl.\\nAttention Framework\\nAttention mechanism has been\\nused as an important component across a wide range of NLP\\nmodels (Bahdanau, Cho, and Bengio 2014). Typically, the\\nattention layer concentrates on the relevant part of the input\\nand extracts the most important information from the input.\\nWe apply the attention framework similar to (Vaswani et al.\\n2017), in which the authors consider an attention function\\nas a mapping to a set of queries, keys, and values. To ob-\\ntain queries, keys, and values for the ﬁnal feature represen-\\ntations, we pass the hidden matrix output from the Bi-LSTM\\nlayer of text (Ht) and topic (Hu), respectively, through\\nthree fully connected layers of dimension da. There are two\\ntriplets of query, key, and value for text (Qt, Kt, Vt) and\\ntopic (Qu, Ku, Vu) in the model SO-MT, while we have a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 5}, page_content='Figure 2: Architecture of the Shared-Private Multi-Task\\n(SP-MT) Framework\\nFigure 3: Attention Framework : (Top) Shared-Speciﬁc Atten-\\ntion; (Bottom) Feature-Speciﬁc Attention\\ntotal of four triplets for the model SP-MT, forming two pairs\\nof two triplets each for text and topic, which are used for\\nstance detection ((Qtd, Ktd, Vtd),(Qud, Kud, Vud)) and sen-\\ntiment ((Qts, Kts, Vts),(Qus, Kus, Vus)) tasks respectively.\\nFig. 3 visually shows the two attention frameworks used in\\nour model: feature-speciﬁc attention and shared-speciﬁc at-\\ntention. The lower part of the ﬁgure shows how different\\nqueries, keys, and values are encoded to obtain self-attention\\nand inter-attention, which form the two sub-modules of\\nfeature-speciﬁc attention. The upper part of the ﬁgure shows\\nthe connections between queries, keys, and values to achieve\\nshared-speciﬁc attention. In the following, we describe in\\ndetail the attention mechanisms used in our study.\\nFeature-Speciﬁc Attention We apply two types of attention\\nto the features to capture the most informative parts of them.\\nFig. 3 (bottom) shows the visual representation and connec-\\ntions of feature-speciﬁc attention. Feature-speciﬁc attention\\nis further divided into Self Attention (SA) and Inter Atten-\\ntion (IA).\\nSelf Attention (SA) We use Self Attention (SA) to relate dif-\\nferent positions of a single sequence of say tweet text or\\ntopic to quantify the most important part of that sequence\\n(Vaswani et al. 2017).\\nSAj = softmax(QjKT\\nj )Vj\\n(1)\\nSA scores are calculated using the equation 1, where SAt ∈\\nRnt×da, and SAu ∈Rnu×da. Here, two SA scores are com-\\nputed for SO-MT, while four such SA scores are required for\\nSP-MT model (SAtd, SAud, SAts, SAus) (as shown with\\nblack dotted arrow connections in Fig. 3 (bottom).\\nInter Attention (IA) We ﬁnd out the Inter Attention (IA)\\nscores to learn the interdependence between different fea-\\ntures. IA scores are determined using below equations where\\nquery of one feature is intervened with key and value of the\\nother. IA scores help to reveal the signiﬁcant contributions\\namongst different inputs to learn optimal features for both\\ntasks. The equations that represent the IA scores for text and\\ntopic (IAtu, IAut) are:\\nIAtu = softmax(QtKT\\nu )Vu,\\n(2)\\nIAut = softmax(QuKT\\nt )Vt,\\n(3)\\nwhere IAtu ∈Rnt×da, and IAut ∈Rnu×da. IA equations\\nare represented graphically with orange and royal red dot-\\nted arrows in Fig. 3 (bottom) part. The SA and IA scores\\nare then concatenated ﬁnally, where A is directly used for\\nshared-only (SO) variant (refer Fig. 1) while average of at-\\ntention vector (Ashared) speciﬁc to stance and sentiment\\ntasks (Ad,As) is used for Shared-private (SP) variant of\\nmodel (mentioned in Fig. 3 (bottom)).\\nA = concat(SAt, SAu, IAtu, IAut),\\n(4)\\nAd = concat(SAtd, SAud, IAtud, IAutd)),\\n(5)\\nAs = concat(SAts, SAus, IAtus, IAuts)),\\n(6)\\nAshared = Average(Ad, As)\\n(7)\\nShared-Speciﬁc Attention Some of the works mentioned in\\nsection 2.2 focused on the orthogonal relationship between\\nsentiment and stance detection. Although climate change de-\\nniers and proponents dominate in sentiment, there are a few\\nexamples where sentiment does not match attitude/stance\\n(see Table 3). (Wu et al. 2019) also mentioned the disad-\\nvantage of the shared-private model of multi-task learning,\\nexplaining that the shared space usually mixes some task-\\nrelevant features, which makes learning different tasks dif-\\nﬁcult. To solve the above problems, we use the (Wu et al.\\n2019) inspired shared-speciﬁc attention, which ﬁlters out the\\nuseless features that interfere with the model prediction and\\nonly pay attention to the selected features from the shared\\nlayer that lead to the correct predictions of the SP-MT model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 6}, page_content='(see Fig. 2). Next, we describe the sub-modules to achieve\\nthe desired result (as shown in Fig. 3 (top))\\nGate Sharing Cell We use similar approach used by authors\\n(Wu et al. 2019) where a single gate mechanism removes\\nthe useless shared features from shared layer. We ﬁrst ex-\\npress the cell with reference to stance detection task. The\\nstance speciﬁc, and shared attention scores (Ad, Ashared)\\nfrom equations (5,7) are passed through dense layers with\\nds units. The weights and biases are captured when passing\\nAd through dense layer which are used for Ashared, and is\\nexpressed as gated sharing cell.\\ngd = σ(Wd.Ashared + bd)\\n(8)\\nwhere, Wd ∈Rnt(ds)×nt(ds) and bd ∈R1×nt(ds). Similar\\nequations are followed for sentiment task also, hence, we do\\nnot iterate here. The ﬁnal output of the shared features for\\nboth the task after ﬁltering will be represented as :\\nGd = gd ⊙Ashared,\\n(9)\\nGs = gs ⊙Ashared\\n(10)\\nwhere ⊙denotes element-wise multiplication. Fig. 3 (top)\\nshows the connections of gate sharing cell for stance detec-\\ntion and sentiment tasks.\\nShared-Private speciﬁc Inter Attention (SPIA) We use sim-\\nilar concept of the inter attention of feature-speciﬁc atten-\\ntion (equations 2, 3). We capture the important shared fea-\\ntures relevant to the speciﬁc task, by using query matrix of\\nthe particular task (stance/sentiment) and keys and values\\nof the shared task. The attention vectors (Ad, As) are passed\\nthrough fully connected layers with ds units to create Qd, Qs\\nfor stance and sentiment tasks, while Ashared is passed\\nthrough dense layer to generate Kshared, and Vshared. The\\nequations are represented visually with green dotted arrows\\nin Fig. 3 (top) part.\\nSPIAd = softmax(QdKT\\nshared)Vshared,\\n(11)\\nSPIAs = softmax(QsKT\\nshared)Vshared\\n(12)\\nFusion The ﬁnal output of the shared layer is the fusion of\\nthe output of the gated cell and shared-private speciﬁc in-\\nter attention. Recently, fusion technique with absolute dif-\\nference and element-wise product is found to be effective in\\n(Mou et al. 2015).\\nC1 = [Gd; SPIAd; Gd −SPIAd; Gd ⊙SSIAd];\\n(13)\\nC2 = [Gs; SPIAs; Gs −SPIAs; Gs ⊙SSIAs];\\n(14)\\nFdshared = tanh(Wfd.C1 + bfd),\\n(15)\\nFsshared = tanh(Wfs.C2 + bfs)\\n(16)\\nClassiﬁcation Layer\\nThe ﬁnal representation of the tweet\\nobtained(Ad,As) is passed through separate outputs for\\nstance and sentiment tasks (for SO-MT model (Fig. 1) ),\\nhowever individual task speciﬁc tweet representations along\\nwith the shared layer representations are passed through two\\noutput channels, subjected to polarisation (Ad,Fdshared) and\\nsentiment (As,Fsshared) tasks for SP-MT Model (Fig. 2).\\nThe task speciﬁc and shared loss are used as\\nLtotal = Ltask + λLshared\\n(17)\\nwhere λ = 0.5 is a hyper-parameter (Liu, Qiu, and Huang\\n2017).\\n5\\nExperiment\\n5.1\\nDatasets\\nWe evaluate our model performance and compare with the\\nother baselines on the two datasets:\\nClimate Change Data The details of the data collection and\\nstatistics are covered in section 3 and table 2.\\nSemEval is provided in the SemEval-2016 shared task 6.A\\non tweet stance detection (Mohammad et al. 2016). Each\\ntweet is in favor, against or neutral corresponding to one of\\nthe ﬁve targets: Atheism, Climate Change is a Real Con-\\ncern, Feminist Movement, Hillary Clinton, and Legalization\\nof Abortion(Abortion). There has been several works that use\\nthis benchmark dataset for stance classiﬁcation.\\n5.2\\nSet-up\\nWe use the python-based library Keras7 at various stages of\\nour implementations. For the experiments, we perform strat-\\niﬁed k-fold cross-validation on our dataset, oversample the\\nminority class (deniers) in the k-1 training data using the\\nsklearn resampling technique, and report the averaged scores\\nand standard deviation (over 5 folds) for the accuracy and F1\\nscores. We select m = 5 and p = 10, which ﬁts our dataset\\nwell, where m denotes the number of most similar topics\\nand each topic contains p number of words for the topic fea-\\nture of each tweet. In the feature extraction sub-module, Bi-\\nLSTM ( dl ) with 100 memory cells is used. The dimensions\\nda and ds of the fully connected layers used in the attention\\nframework to extract queries, keys and values for feature-\\nspeciﬁc and shared-speciﬁc attention (refer section 4.2) are\\nused with 100 units each. The stance and sentiment output\\nchannels contain 2 and 3 output neurons, respectively. The\\nloss functions binary cross-entropy and categorical cross-\\nentropy are used for the stance and sentiment output chan-\\nnels, respectively. The experiments are run on an NVIDIA\\nGeForce GTX 1080Ti GPU and the models are optimized\\nusing Adam optimizer with a learning rate of 0.0001. All\\nthese values are selected using TPE in the hyperopt python\\nlibrary (Bergstra et al. 2013) and after a thorough sensitivity\\nanalysis of the parameters that minimise the loss functions.\\n5.3\\nBaseline Techniques\\nWe compare our proposed approach with the following base-\\nlines on our climate change dataset :\\nLogistic regression (Argyris et al. 2021): This study use\\nlogistic regression with Count Vectorizer feature extraction\\nmethod to classify vaccine-related tweets into provaccine,\\nantivaccine, and neutral stances.\\nESD (Vychegzhanin and Kotelnikov 2021): The authors\\nform a relevant feature set using an ensemble of feature se-\\nlection methods and propose the model ESD by selecting\\nan optimal ensemble of classiﬁers. They evaluate the per-\\nformance of the model using the UKP Sentential Argument\\nMining Corpus and the SemEval-2016 dataset.\\nHAN (Wang et al. 2020): In this article, researchers pro-\\nposed a hierarchical attention neural model, focusing on dif-\\nferent features such as document, sentiment, dependency,\\n7https://keras.io/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 7}, page_content='Model\\nSingle-Task Stance Detection\\nSingle-Task Sentiment\\nText\\nText+Topic\\nText\\nText+Topic\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nShared-only (SO)\\n76.73±2.48\\n70.75±2.11\\n79.56±1.10\\n75.72 ±1.08\\n71.61±0.45\\n70.11±0.79\\n75.82±2.03\\n74.63±2.41\\nSO + Self Attn. (SA)\\n80.29±0.55\\n74.24±0.31\\n82.88±0.78\\n77.56 ±0.87\\n73.49±0.92\\n72.57±1.27\\n77.68±1.27\\n76.59±1.66\\nSO + Inter Attn. (IA)\\n81.33±1.04\\n73.35±1.41\\n83.13±1.42\\n76.04 ±1.11\\n73.68±3.20\\n72.23±3.14\\n77.93±2.01\\n74.11±1.98\\nSO + SA + IA\\n(Feature-Speciﬁc Attn.)\\n82.15±0.05\\n76.78±0.81\\n85.01±1.05\\n80.03 ±2.42\\n76.21±0.84\\n74.95±1.02\\n80.81±1.29\\n79.28±1.44\\nTable 5: Results of the Single-Task Stance Detection and Sentiment tasks for various combinations\\nModel\\nMulti-Task Stance Detection + Sentiment\\nStance Detection\\nSentiment\\nText\\nText+Topic\\nText\\nText+Topic\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nAcc.\\nF1\\nShared-only (SO)\\n81.16±1.89\\n77.01±2.61\\n84.86±1.13\\n81.66 ±3.28\\n75.44±1.23\\n74.40±1.75\\n80.49±2.05\\n79.19±2.8\\nShared-Private (SP)\\n84.64±1.95\\n81.53±2.55\\n87.47±2.01\\n84.24 ±2.04\\n77.93±2.09\\n74.11±2.19\\n85.28±0.91\\n84.58±1.02\\nShared-Private (SP) +\\nFeature-Speciﬁc Attn.\\n86.49±2.29\\n81.67±3.21\\n91.31±1.06\\n85.93 ±1.22\\n81.46±0.71\\n80.71±0.39\\n85.46±0.9\\n84.73±1.10\\nShared-Private (SP) +\\nShared-Speciﬁc Attn.\\n88.10±1.04\\n84.09±1.39\\n92.29±1.32\\n86.12 ±1.4\\n81.67±0.61\\n80.89±0.32\\n87.59±0.45\\n87.07±0.39\\nShared-Private +\\nFeature-Sp. Attn. +\\nShared-Sp. Attn.\\n(SP-MT)\\n89.99±2.67\\n86±2.02\\n93.95±1.27\\n90.24 ±1.16\\n84.60±0.44\\n83.98±0.81\\n89.08±1.01\\n88.48±1.60\\nTable 6: Results of the Multi-Task Stance Detection and Sentiment tasks for various combinations\\nand argument representations. They evaluated the model\\nperformance on SemEval-2016 and the H&N14 dataset.\\nAT-JSS-LEX (Li and Caragea 2019): is a multi-task\\nframework for stance detection with sentiment analysis as\\nauxiliary task. The attention mechanism of the model is\\nguided by target-speciﬁc attention along with sentiment and\\nstance lexicons.\\nMNB (Kabaghe and Qin 2019): Multinomial naive bayes\\nperformed better with respect to other models proposed in\\nthe study, to classify tweets into positive, negative or neutral\\nbeliefs towards climate change.\\nDNN (Chen, Zou, and Zhao 2019): Deep Neural Network\\n(DNN) is used as a classiﬁer to identify users who either be-\\nlieve or deny climate change based on the content of tweets.\\nThe model’s performance is assessed on the real-time col-\\nlection of climate change twitter data.\\nSVM-ngram (Sobhani, Mohammad, and Kiritchenko\\n2016): is trained on word and character n-grams features for\\nstance detection task on SemEval 2016 dataset. The model\\nsurpassed the best model in SemEval-2016 competition.\\nWe evaluate our model performance on SemEval 2016\\ndataset and contrast with the these state-of-the art models\\n: ESD (Vychegzhanin and Kotelnikov 2021), HAN (Wang\\net al. 2020), AT-JSS-LEX (Li and Caragea 2019), and\\nSVM-ngram (Sobhani, Mohammad, and Kiritchenko 2016)\\nas described above.\\n6\\nResult and Analysis\\nIn this section, we investigate the performance of the pro-\\nposed approach. We ﬁrst compare different single-task and\\nmulti-task variants and then compare them with the state-\\nof-the-art methods mentioned in section 5.3. We also ana-\\nlyze the importance of each feature and the different variants\\nof the attention framework. We report all the results of the\\nﬁve-fold cross-validation (mean and standard deviation of\\naccuracy and F1 score) for the different combinations of the\\nproposed system. The Tables 5 and 6 illustrate the results of\\nthe single-task and the various combinations of the proposed\\nmulti-task models for both the stance detection and senti-\\nment tasks. It is evident that the addition of topic words con-\\nsistently improves the performance of the models. This im-\\nprovement means that the proposed architecture makes very\\neffective use of the interaction between input features. This\\nshows the importance of incorporating multiple features for\\nvarious analysis tasks.\\n6.1\\nComparison amongst Single-Task and\\nMulti-task Framework\\nFrom the tables 5 and 6, the multi-task variants perform\\nbetter than the single-task variants by achieving an average\\nmacro F1 score of 90.24 and 88.48 for the stance detection\\n(primary) and sentiment analysis (auxiliary) tasks respec-\\ntively. The results show that the sentiment and stance tasks\\nimprove each other’s performance when learned together.\\nThe single stance detection task is able to correctly label\\nsome tweets from deniers and believer that contain predomi-\\nnantly negative and positive sentiments, respectively (exam-\\nples 2 and 6 from Table 7). However, examples 1, 4, and 5\\nfrom Table 7 clearly show that the stance task, together with\\nthe sentiment analysis task, is able to unambiguously iden-\\ntify denier and believers tweets with the corresponding less\\ndominant positive, neutral, and negative sentiment polari-\\nties. As stated earlier, we consider sentiment analysis as an\\nauxiliary task that supports the main task, i.e., stance detec-\\ntion. However, we report the performance of the sentiment\\ntask for the proposed model for both single-task and multi-\\ntask frameworks in the tables 5 and 6 to illustrate the impact\\nof the main task on the auxiliary task and to show that the\\nmultiple features in the form of tweet text and topic words,\\nas well as the attention framework, also beneﬁt the sentiment'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 8}, page_content='No.\\nTweet\\nSentiment\\nTrue\\nPredicted\\nStance\\nPredicted\\nStance +\\nSentiment\\n1.\\nMy family support Oil and Gas! ClimateHoax\\npositive\\ndenier\\nbeliever\\ndenier\\n2.\\nOnce again brainwashing kids to push the green tax agenda,\\nunder the globalwarminghoax umbrella! Stop\\nnegative\\ndenier\\ndenier\\ndenier\\n3.\\nHis Green BS policies will send us back to the Dark Ages. ClimateChangeHoax\\nnegative\\ndenier\\nbeliever\\ndenier\\n4.\\nClimateHoax The climate has ﬂuctuated since the time of creation,\\nand nothing those people will do can change that one way or the other\\nneutral\\ndenier\\nbeliever\\ndenier\\n5.\\nAnd yet, there are those who deny climate change?? Ice-shelves breaking off,\\nheat waves, etc. Sad ScienceIsReal\\nnegative\\nbeliever\\ndenier\\nbeliever\\n6.\\nHave you seen this? Its very moving. We deﬁnitely need more ClimateAction\\npositive\\nbeliever\\nbeliever\\nbeliever\\n7.\\nFor those adamant that global warming is real, THIS is Today in Alaska.\\nFour inches of snow overnight, and still coming down! ClimateChange\\nneutral\\ndenier\\nbeliever\\nbeliever\\n8.\\nI am glad you went by plane. Way better for our climate instead of zoommeetings...\\npositive\\nbeliever\\ndenier\\ndenier\\n9.\\nOver 60° today. Over 6” of snow tomorrow. But yeah, climatechange is total bullshit right?\\nnegative\\nbeliever\\ndenier\\ndenier\\nTable 7: Few example tweets with ground truth and predicted labels for single and multi-task models\\nModel\\nTraining Time (secs)\\nMean Accuracy\\nSingle Task Best\\n(SO + SA + IA) [Table 5]\\n870\\n85.01\\nMulti Task Variants [Refer Table 6]\\nShared-only (SO)\\n918\\n84.86\\nShared-Private (SP)\\n1218\\n87.47\\nShared-Private (SP) +\\nFeature-Speciﬁc Attn.\\n1419\\n91.31\\nShared-Private (SP) +\\nShared-Speciﬁc Attn.\\n1506\\n92.29\\nShared-Private +\\nFeature-Sp. Attn. +\\nShared-Sp. Attn. (SP-MT)\\n1791\\n93.95\\nTable 8: Training time of Different Text + Topic Models\\nclassiﬁcation task. However, we do not make explicit efforts\\nto improve the model performance on the auxiliary task.\\n6.2\\nComparison amongst Different Multi-task\\nFrameworks\\nTable 6 shows the improvement of the multi-task framework\\nfrom the shared-only variant (Fig. 1) to the shared-private\\nmulti-task model(SP-MT) (Fig. 2). The inclusion of feature-\\nspeciﬁc and shared-speciﬁc attention frameworks helped the\\nmulti-task models focus on the important parts of the fea-\\ntures and effectively discard the useless shared features, re-\\nsulting in a 7.40% increase in accuracy and a 12.46% in-\\ncrease in F1 score. Furthermore, in Table 8 we give the train-\\ning times of the best performing single-task model and dif-\\nferent variants of the multi-task model for 20 epochs to an-\\nalyze the additional time required by the best performing\\nmulti-task model with text and topic as input features (SP-\\nMT) compared to other variants. As can be seen from the\\ntable 8, SP-MT requires about 15 more minutes (approxi-\\nmately twice the time) to achieve a 10.51% improvement\\nin accuracy compared to the best performing single-task\\nmodel, while SP-MT requires 9.5 more minutes to achieve a\\n7.4% improvement in performance compared to the shared-\\nprivate multi-task variant.\\nAll results reported here are statistically signiﬁcant as we\\nperformed a t-test at the 5% signiﬁcance level (Welch 1947)\\nagainst the null hypothesis, which states that the mean ac-\\ncuracy/F1 score of all the multi-task variants is more when\\ncompared to the the best performing proposed model SP-\\nMT (Shared-Private + Feature-Speciﬁc Attention +Shared-\\nSpeciﬁc Attention) [refer table 6]. If the p-value is signif-\\nicant (p < 0.05), we reject the null hypothesis. Our best\\nperforming proposed model outperforms all the other multi-\\ntask variants while meeting statistical signiﬁcance under t-\\ntests (p < 0.05). For the conﬁdence analysis, we also report\\nthe p-values and t-test statistics of all the multi-task variant\\nmodels compared to the best performing model in tables 1\\nand 2 in Supplementary.\\n6.3\\nComparison with the Baseline Methods\\nIn Table 9, we report the results for the baseline methods by\\nre-implementing them on the Climate Change dataset (sec-\\ntion 3). It is observed that our proposed multi-task approach\\nSP-MT outperforms the SOTA approaches in terms of accu-\\nracy and F1 score. Our best performing model achieved bet-\\nter results compared to ESD (Vychegzhanin and Kotelnikov\\n2021) and HAN (Wang et al. 2020). This highlights that\\nthe shared-private multi-tasking approach takes advantage\\nof task-speciﬁc and invariant features to improve classiﬁca-\\ntion task performance. Although the AT-JSS-LEX (Li and\\nCaragea 2019) model was implemented with a multi-tasking\\napproach, our model performs better because it keeps the\\ntask-dependent and task-independent feature spaces sepa-\\nrate and removes the useless shared features that hinder\\ntask performance of the stance detection, demonstrating the\\nimportance of the shared-speciﬁc attention framework. It\\nis also observed that the methods that use sentiment fea-\\ntures (ESD, HAN, and AT-JSS-LEX) perform better than the\\nother baselines. This proves the importance of the proposed\\nsentiment analysis approach for climate change. Our best\\nperforming single-task polarization framework (Table 5)\\nalso outperforms MNB, DNN, and SVM-ngram approaches.\\nThis justiﬁes the beneﬁts of using topic words in addition to\\ntweet text and feature-speciﬁc attention framework to im-\\nprove the performance of the model.\\nConsequently, we also performed a comparative analysis of\\nthe proposed multi-tasking approach SP-MT with the state-\\nof-the-art models (SOTA) on the SemEval 2016 dataset.\\nThe model is trained with three polarized classes (Favour,\\nAgainst, None) and the metrics (Favg, MacFavg) are evalu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 9}, page_content='Model\\nAccuracy\\nF1-score\\nProposed SP-MT (Fig. 2)\\n93.95\\n90.24\\nLR (Argyris et al. 2021)\\n81.48\\n81.00\\nESD (Vychegzhanin and Kotelnikov 2021)\\n89.65\\n85.11\\nHAN (Wang et al. 2020)\\n89.47\\n86.00\\nAT-JSS-LEX (Li and Caragea 2019)\\n88.02\\n84.01\\nMNB (Kabaghe and Qin 2019)\\n85.44\\n78.08\\nDNN (Chen, Zou, and Zhao 2019)\\n84.61\\n76.23\\nSVM-ngram (Sobhani, Mohammad, and Kiritchenko 2016)\\n85.55\\n66.33\\nTable 9: Results of Proposed Framework SP-MT with base-\\nlines on our Climate Change Dataset\\nated according to the procedure deﬁned in (Li and Caragea\\n2019). Table 10 shows that our approach outperforms other\\nmethods with an overall MacFavg value of 66.84. Our pro-\\nposed framework performs better in the climate, feminism,\\nand abortion domains, while the Favg values are comparable\\nin the atheism and Hillary domains, showing that our frame-\\nwork generalises well in different domains.\\n6.4\\nError Analysis\\nWe perform an in-depth error analysis to understand where\\nthe proposed model has faltered. These are the following\\nscenarios: (i) The climate change dataset is an imbalanced\\ndataset with a high proportion of believer tweets, resulting in\\nlow F1 values compared to accuracy. Although we applied\\noversampling to partially counter this problem, even ﬁner\\ncategories of believers can be identiﬁed and labeled, which\\ncan be beneﬁcial for the model to learn different classes with\\na clear separation of distribution in tweets, such as “tweet\\nconveys causes of climate change” or “tweet believes in\\nhuman-caused climate change”. (ii) We determine the fre-\\nquency of unigrams and bigrams extracted using TF-IDF\\nand ﬁnd that some of the denier’s tweets containing either\\nrarely used keywords or keywords frequently used in be-\\nlievers’ tweets were misclassiﬁed. For example, the denier’s\\ntweet in example 7 (table 7) contains words such as real,\\nsnow overnight, which are most commonly found in believ-\\ners’ tweets and confuse the model and lead to an incorrect\\nprediction. (iii) We investigated that of the total misclassiﬁed\\ndenier tweets, 35.7% of the tweets contained sarcasm to ex-\\npress their denial. Of the sarcastic denier tweets, 50.16% of\\nthe tweets have positive sentiment, 31.70% have neutral sen-\\ntiment, and the rest have negative sentiment, while 25.78%\\nof the misclassiﬁed believer tweets have sarcastic labels (ex-\\namples 8 and 9 of table 7). The labeling of sarcasm is based\\non the majority vote of three trained annotators with an inter-\\nrater agreement of 0.78, calculated with the Fleiss-Kappa\\nmeasure. This motivated us to investigate the presence of\\nsarcasm in climate change tweets to further improve the per-\\nformance of the model as a part of our future work.\\n7\\nPrivacy and Ethics\\nAlthough social media offers innovative ways to raise aware-\\nness about climate change, phenomena such as climate de-\\nnial and climate delay have become a serious problem for\\nscientists and the government to convince people of the im-\\nportance of understanding the current climate crisis. Climate\\nchange deniers are not only skeptical about climate change\\nbut also emphasize the disadvantages of all measures pro-\\nposed to combat climate change and abandon the idea that\\nModel\\nAtheism\\nFavg\\nClimate\\nFavg\\nFeminism\\nFavg\\nHillary\\nFavg\\nAbortion\\nFavg\\nMac Favg\\nProposed SP-MT(Fig.2)\\n69.5\\n63.5\\n63.2\\n67.5\\n70.5\\n66.84\\nESD\\n(Vychegzhanin and\\nKotelnikov 2021)\\n66.64\\n43.82\\n62.85\\n67.79\\n64.94\\n61.20\\nHAN (Wang et al. 2020)\\n70.53\\n49.56\\n57.50\\n61.23\\n66.16\\n61.00\\nAT-JSS-LEX\\n(Li and Caragea 2019)\\n69.22\\n59.18\\n61.49\\n68.33\\n68.41\\n65.33\\nSVM-ngram\\n(Sobhani, Mohammad,\\nand Kiritchenko2016)\\n65.19\\n42.35\\n57.46\\n58.63\\n66.42\\n58.01\\nTable 10: Results of Proposed Framework SP-MT with base-\\nlines on SemEval 2016 Dataset\\nit is not possible to prevent climate change. This often leads\\nto the spread of misinformation, resulting in a delay in the\\nimplementation of effective climate change mitigation mea-\\nsures (Zhou and Shen 2021). Since our work is dedicated\\nto classifying Twitter content into climate change deniers or\\nbelievers, the proposed approach can be useful for govern-\\nment agencies, researchers, and tech companies that moni-\\ntor such content on social media to identify and intervene\\ntweets from climate change deniers. The proposed approach\\nis useful in combating climate misinformation by identify-\\ning posts by climate change deniers and reducing the spread\\nof such content that is deemed false or misleading.\\nThe input feature of the proposed model, such as the tweet\\ntext, is available as soon as the user posts something. How-\\never, the topic feature can be extracted by performing topic\\nmodeling for a collection of tweets after a ﬁxed interval, e.g.,\\nafter every 5, 10, 15, or t minutes of duration. Therefore, our\\nproposed approach can be used in a real-time environment\\nby interested agencies and authorities to classify social me-\\ndia content into one of the two polarized classes.\\nAlthough we conduct our work with public data from social\\nmedia, however, we are committed to protecting the privacy\\nof individuals and therefore avoid providing personally iden-\\ntiﬁable content. The dataset that is made publicly available\\nconsists only of tweet IDs and comments.\\n8\\nConclusion and Future Work\\nIn this paper, we focus on the importance of classifying\\nTwitter content into climate change deniers and believers,\\nbecause climate change deniers emphasize the downsides of\\nany action to address climate change, which often leads to\\nthe spread of misinformation and delays the implementation\\nof effective action to mitigate climate change. Our proposed\\napproach, implemented in real-time, can be useful for gov-\\nernment agencies, researchers, and tech companies in com-\\nbating climate misinformation by identifying content that\\ndenies climate change and reducing the spread of such posts\\nthat are considered misleading.\\nIn this work, we investigated the role of sentiment in classi-\\nfying stance of the tweets related to climate change. We cu-\\nrate a novel dataset that includes annotations for both stance\\ndetection and sentiment analysis tasks, which will be useful\\nto the research community in exploring other needed classi-\\nﬁcation tasks. We propose a shared-private multi-task frame-\\nwork for the optimization of stance detection task beneﬁt-\\ning from the sentiment analysis (auxiliary task). The pro-\\nposed module uses feature-speciﬁc and shared-speciﬁc at-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 10}, page_content='tention to fuse multiple features and learn useful and rele-\\nvant private and shared features for both tasks. The results\\nshow that multi-tasking increased the performance of the\\nstance detection task compared to its uni-modal and single-\\ntask variants. Although we examined the performance of the\\nproposed approach in detecting attitudes in the domain of\\nclimate change, the performance of the model on the Se-\\nmEval dataset shows that it is much more broadly applica-\\nble beyond the domain of climate change, suggesting that\\nour framework can generalize well in different domains. Fu-\\nture work will attempt to analyse what other aspects of nat-\\nural language, such as sarcasm, aspect-based sentiment, and\\nemotion recognition, might help to more accurately clas-\\nsify attitudes toward climate change. The inclusion of other\\nmodality encodings such as images, emoji, and advanced ar-\\nchitectures will also be the subject of our future work.\\nReferences\\nAnwar, A.; Ilyas, H.; Yaqub, U.; and Zaman, S. 2021. Ana-\\nlyzing QAnon on Twitter in Context of US Elections 2020:\\nAnalysis of User Messages and Proﬁles Using VADER and\\nBERT Topic modeling. In DG. O2021: The 22nd Annual\\nInternational Conference on Digital Government Research,\\n82–88.\\nArgyris, Y. A.; Monu, K.; Tan, P.-N.; Aarts, C.; Jiang, F.; and\\nWiseley, K. A. 2021. Using Machine Learning to Compare\\nProvaccine and Antivaccine Discourse Among the Public on\\nSocial Media: Algorithm Development Study. JMIR public\\nhealth and surveillance 7(6): e23105.\\nBahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural ma-\\nchine translation by jointly learning to align and translate.\\narXiv preprint arXiv:1409.0473 .\\nBaziotis, C.; Pelekis, N.; and Doulkeridis, C. 2017. Datas-\\ntories at semeval-2017 task 4: Deep lstm with attention for\\nmessage-level and topic-based sentiment analysis. In Pro-\\nceedings of the 11th international workshop on semantic\\nevaluation (SemEval-2017), 747–754.\\nBergstra, J.; Yamins, D.; Cox, D. D.; et al. 2013. Hyperopt:\\nA python library for optimizing the hyperparameters of ma-\\nchine learning algorithms. In Proceedings of the 12th Python\\nin science conference, volume 13, 20. Citeseer.\\nCann, T. J.; Weaver, I. S.; and Williams, H. T. 2021. Ideo-\\nlogical biases in social sharing of online information about\\nclimate change. Plos one 16(4): e0250656.\\nCaruana, R. 1997. Multitask learning. Machine learning\\n28(1): 41–75.\\nChauhan, D. S.; Kumar, R.; and Ekbal, A. 2019. Attention\\nbased shared representation for multi-task stance detection\\nand sentiment analysis. In International Conference on Neu-\\nral Information Processing, 661–669. Springer.\\nChen, X.; Zou, L.; and Zhao, B. 2019. Detecting climate\\nchange deniers on twitter using a deep neural network. In\\nProceedings of the 2019 11th International Conference on\\nMachine Learning and Computing, 204–210.\\nClayton, S. 2020. Climate anxiety: Psychological responses\\nto climate change. Journal of Anxiety Disorders 74: 102263.\\nCody, E. M.; Reagan, A. J.; Mitchell, L.; Dodds, P. S.; and\\nDanforth, C. M. 2015. Climate change sentiment on Twit-\\nter: An unsolicited public opinion poll.\\nPloS one 10(8):\\ne0136092.\\nDahal, B.; Kumar, S. A.; and Li, Z. 2019. Topic modeling\\nand sentiment analysis of global climate change tweets. So-\\ncial Network Analysis and Mining 9(1): 1–20.\\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\\nBERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technolo-\\ngies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\\n2019, Volume 1 (Long and Short Papers), 4171–4186.\\nEl Barachi, M.; AlKhatib, M.; Mathew, S.; and Oroumchian,\\nF. 2021. A novel sentiment analysis framework for monitor-\\ning the evolving public opinion in real-time: Case study on\\nclimate change. Journal of Cleaner Production 127820.\\nGarimella, K.; Morales, G. D. F.; Gionis, A.; and Math-\\nioudakis, M. 2018. Quantifying controversy on social me-\\ndia. ACM Transactions on Social Computing 1(1): 1–27.\\nGautam, A.; V, V.; and Masud, S. 2021. Fake News Detec-\\ntion System Using XLNet Model with Topic Distributions:\\nCONSTRAINT@AAAI2021 Shared Task. In Chakraborty,\\nT.; Shu, K.; Bernard, H. R.; Liu, H.; and Akhtar, M. S., eds.,\\nCombating Online Hostile Posts in Regional Languages dur-\\ning Emergency Situation - First International Workshop,\\nCONSTRAINT 2021, Collocated with AAAI 2021, volume\\n1402 of Communications in Computer and Information Sci-\\nence, 189–200. Springer.\\nGrootendorst, M. 2020.\\nBERTopic: Leveraging BERT\\nand c-TF-IDF to create easily interpretable topics.\\ndoi:\\n10.5281/zenodo.4381785.\\nURL https://doi.org/10.5281/\\nzenodo.4381785.\\nHutto, C.; and Gilbert, E. 2014. Vader: A parsimonious rule-\\nbased model for sentiment analysis of social media text. In\\nProceedings of the International AAAI Conference on Web\\nand Social Media, volume 8.\\nJang, S. M.; and Hart, P. S. 2015. Polarized frames on “cli-\\nmate change” and “global warming” across countries and\\nstates: Evidence from Twitter big data. Global Environmen-\\ntal Change 32: 11–17.\\nJiang, Y.; Song, X.; Harrison, J.; Quegan, S.; and Maynard,\\nD. 2017.\\nComparing Attitudes to Climate Change in the\\nMedia using sentiment analysis based on Latent Dirichlet\\nAllocation. In Proceedings of the 2017 EMNLP Workshop:\\nNatural Language Processing meets Journalism, 25–30.\\nKabaghe, C.; and Qin, J. 2019. Classifying Tweets Based on\\nClimate Change Stance. Training 66(60.9): 61–6.\\nLi, Y.; and Caragea, C. 2019. Multi-task stance detection\\nwith sentiment and stance lexicons. In Proceedings of the\\n2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Confer-\\nence on Natural Language Processing (EMNLP-IJCNLP),\\n6299–6305.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-11-08T01:54:42+00:00', 'source': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2211.03533v1.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-11-08T01:54:42+00:00', 'trapped': '', 'modDate': 'D:20221108015442Z', 'creationDate': 'D:20221108015442Z', 'page': 11}, page_content='Lineman, M.; Do, Y.; Kim, J. Y.; and Joo, G.-J. 2015. Talk-\\ning about climate change and global warming.\\nPloS one\\n10(9): e0138996.\\nLittman, J.; and Wrubel, L. 2019. Climate Change Tweets\\nIds. doi:10.7910/DVN/5QCCUU. URL https://doi.org/10.\\n7910/DVN/5QCCUU.\\nLiu, P.; Qiu, X.; and Huang, X. 2017. Adversarial Multi-\\ntask Learning for Text Classiﬁcation. In Barzilay, R.; and\\nKan, M., eds., Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics, ACL 2017,\\n1–10. URL https://doi.org/10.18653/v1/P17-1001.\\nLuo, Y.; Card, D.; and Jurafsky, D. 2020.\\nDetecting\\nStance in Media on Global Warming.\\narXiv preprint\\narXiv:2010.15149 .\\nMajumder, N.; Poria, S.; Peng, H.; Chhaya, N.; Cambria, E.;\\nand Gelbukh, A. 2019. Sentiment and sarcasm classiﬁcation\\nwith multitask learning. IEEE Intelligent Systems 34(3): 38–\\n43.\\nMisra, A.; Ecker, B.; Handleman, T.; Hahn, N.; and Walker,\\nM. 2016. NLDS-UCSC at SemEval-2016 Task 6: A Semi-\\nSupervised Approach to Detecting Stance in Tweets. In Pro-\\nceedings of the 10th International Workshop on Semantic\\nEvaluation (SemEval-2016). Association for Computational\\nLinguistics.\\nMohammad, S.; Kiritchenko, S.; Sobhani, P.; Zhu, X.; and\\nCherry, C. 2016. Semeval-2016 task 6: Detecting stance in\\ntweets. In Proceedings of the 10th international workshop\\non semantic evaluation (SemEval-2016), 31–41.\\nMou, L.; Men, R.; Li, G.; Xu, Y.; Zhang, L.; Yan, R.;\\nand Jin, Z. 2015.\\nNatural language inference by tree-\\nbased convolution and heuristic matching. arXiv preprint\\narXiv:1512.08422 .\\nMyhre, G.; Shindell, D.; Br´eon, F.-M.; Collins, W.; Fu-\\nglestvedt, J.; Huang, J.; Koch, D.; Lamarque, J.-F.; Lee,\\nD.; Mendoza, B.; et al. 2013.\\nClimate change 2013: the\\nphysical science basis. Contribution of Working Group I to\\nthe Fifth Assessment Report of the Intergovernmental Panel\\non Climate Change. K., Tignor, M., Allen, SK, Boschung,\\nJ., Nauels, A., Xia, Y., Bex, V., and Midgley, PM, Cam-\\nbridge University Press Cambridge, United Kingdom and\\nNew York, NY, USA .\\nSamantray, A.; and Pin, P. 2019.\\nCredibility of climate\\nchange denial in social media. Palgrave Communications\\n5(1): 1–8.\\nSen, I.; Fl¨ock, F.; and Wagner, C. 2020. On the reliabil-\\nity and validity of detecting approval of political actors in\\ntweets. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP), 1413–\\n1426.\\nShi, J.; Visschers, V. H.; and Siegrist, M. 2015. Public per-\\nception of climate change: The importance of knowledge\\nand cultural worldviews. Risk Analysis 35(12): 2183–2201.\\nSingh, A.; Saha, S.; Hasanuzzaman, M.; and Dey, K. 2021.\\nMultitask learning for complaint identiﬁcation and senti-\\nment analysis. Cognitive Computation 1–16.\\nSobhani, P.; Mohammad, S.; and Kiritchenko, S. 2016. De-\\ntecting stance in tweets and analyzing its interaction with\\nsentiment. In Proceedings of the ﬁfth joint conference on\\nlexical and computational semantics, 159–169.\\nSpitzer, R. L.; Cohen, J.; Fleiss, J. L.; and Endicott, J. 1967.\\nQuantiﬁcation of agreement in psychiatric diagnosis: A new\\napproach. Archives of General Psychiatry 17(1): 83–87.\\nTyagi, A.; Babcock, M.; Carley, K. M.; and Sicker, D. C.\\n2020a.\\nPolarizing tweets on climate change.\\nIn In-\\nternational Conference on Social Computing, Behavioral-\\nCultural Modeling and Prediction and Behavior Represen-\\ntation in Modeling and Simulation, 107–117. Springer.\\nTyagi, A.; Field, A.; Lathwal, P.; Tsvetkov, Y.; and Carley,\\nK. M. 2020b. A computational analysis of polarization on\\nIndian and Pakistani social media. In International Confer-\\nence on Social Informatics, 364–379. Springer.\\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\\ntention is all you need. In Advances in neural information\\nprocessing systems, 5998–6008.\\nVychegzhanin, S.; and Kotelnikov, E. 2021. A New Method\\nfor Stance Detection Based on Feature Selection Techniques\\nand Ensembles of Classiﬁers.\\nIEEE Access 9: 134899–\\n134915.\\nWang, Z.; Sun, Q.; Li, S.; Zhu, Q.; and Zhou, G. 2020. Neu-\\nral Stance Detection With Hierarchical Linguistic Represen-\\ntations.\\nIEEE/ACM Transactions on Audio, Speech, and\\nLanguage Processing 28: 635–645.\\nWeber, I.; Garimella, V. R. K.; and Batayneh, A. 2013. Sec-\\nular vs. islamist polarization in egypt on twitter. In Proceed-\\nings of the 2013 IEEE/ACM international conference on ad-\\nvances in social networks analysis and mining, 290–297.\\nWelch, B. L. 1947.\\nThe generalization of ‘STU-\\nDENT’S’problem when several different population var-\\nlances are involved. Biometrika 34(1-2): 28–35.\\nWu, L.; Rao, Y.; Jin, H.; Nazir, A.; and Sun, L. 2019.\\nDifferent absorption from the same sharing: Sifted multi-\\ntask learning for fake news detection. In Empirical Meth-\\nods in Natural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP), 4636–4645.\\nZhou, P.; Qi, Z.; Zheng, S.; Xu, J.; Bao, H.; and Xu, B.\\n2016. Text Classiﬁcation Improved by Integrating Bidirec-\\ntional LSTM with Two-dimensional Max Pooling. In Calzo-\\nlari, N.; Matsumoto, Y.; and Prasad, R., eds., COLING 2016,\\n26th International Conference on Computational Linguis-\\ntics, Proceedings of the Conference: Technical Papers, De-\\ncember 11-16, 2016, Osaka, Japan, 3485–3495. ACL. URL\\nhttps://aclanthology.org/C16-1329/.\\nZhou, Y.; and Shen, L. 2021. Conﬁrmation Bias and the\\nPersistence of Misinformation on Climate Change. Commu-\\nnication Research 00936502211028049.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 0}, page_content='Sentiment and Social Signals in the Climate Crisis: A Survey on\\nAnalyzing Social Media Responses to Extreme Weather Events\\nPouya Shaeri\\npshaeri@asu.edu\\nSchool of Computing and Augmented\\nIntelligence, Arizona State University\\nTempe, Arizona, USA\\nYasaman Mohammadpour\\nymoham15@asu.edu\\nNew College of Interdisciplinary Arts\\nand Sciences, Arizona State\\nUniversity\\nTempe, Arizona, USA\\nAlimohammad Beigi\\nabeigi@asu.edu\\nSchool of Computing and Augmented\\nIntelligence, Arizona State University\\nTempe, Arizona, USA\\nAriane Middel\\nariane.middel@asu.edu\\nSchool of Arts Media and\\nEngineering, Arizona State University\\nTempe, Arizona, USA\\nHuan Liu\\nhliu@asu.edu\\nSchool of Computing and Augmented\\nIntelligence, Arizona State University\\nTempe, Arizona, USA\\nAbstract\\nExtreme weather events driven by climate change, such as wildfires,\\nfloods, and heatwaves, prompt significant public reactions on social\\nmedia platforms. Analyzing the sentiment expressed in these online\\ndiscussions can offer valuable insights into public perception, in-\\nform policy decisions, and enhance emergency responses. Although\\nsentiment analysis has been widely studied in various fields, its\\nspecific application to climate-induced events, particularly in real-\\ntime, high-impact situations like the 2025 Los Angeles forest fires,\\nremains underexplored. In this survey, we thoroughly examine the\\nmethods, datasets, challenges, and ethical considerations related\\nto sentiment analysis of social media content concerning weather\\nand climate change events. We present a detailed taxonomy of ap-\\nproaches, ranging from lexicon-based and machine learning models\\nto the latest strategies driven by large language models (LLMs). Ad-\\nditionally, we discuss data collection and annotation techniques,\\nincluding weak supervision and real-time event tracking. Finally,\\nwe highlight several open problems, such as misinformation detec-\\ntion, multimodal sentiment extraction, and model alignment with\\nhuman values. Our goal is to guide researchers and practitioners in\\neffectively understanding sentiment during the climate crisis era.\\nKeywords\\nsentiment analysis, climate change, social media, misinformation,\\nheat crisis, extreme weather events\\n1\\nIntroduction\\nAs climate instability intensifies, societies worldwide are grappling\\nnot only with the physical consequences of extreme weather events\\nbut also with their psychological and sociocultural impacts [56, 130,\\n162]. Catastrophic events such as wildfires, hurricanes, floods, heat-\\nwaves, and droughts—driven by human-induced climate change are\\noccurring with increasing frequency, severity, and unpredictabil-\\nity [54, 151, 159]. These events do not occur in isolation; they are\\npart of a complex web of social narratives, political discourse, me-\\ndia framing, and public emotion [9, 32, 108]. Understanding how\\nindividuals and communities emotionally and behaviorally react to\\nsuch events is crucial for a wide range of stakeholders, including\\nclimate scientists, public health officials, policymakers, emergency\\nresponders, and communication strategists [25, 115].\\nOne of the most direct and unfiltered sources of public response\\nin the modern era is social media. Platforms like Twitter (now X),\\nReddit, Facebook, and TikTok have become essential spaces where\\npeople share their experiences, express their concerns, voice their\\nanger or solidarity, and seek information or emotional support\\nduring and after climate-related disasters [17, 51, 120, 189]. These\\ndigital platforms serve not only as outlets for individual sentiment\\nbut also as sociotechnical systems that influence collective under-\\nstanding and public opinion [116, 117, 140]. They offer a dynamic\\nand rich source for computational sentiment analysis, allowing\\nfor the study of emotional responses to climate events on a large\\nscale, in real-time, and across various demographic and geographic\\nsegments [165].\\nHowever, mining sentiment from social media during climate\\ncrises is a complex task. Unlike structured surveys or formal re-\\nports, social media data is unstructured, fleeting, and multimodal [8,\\n96, 153, 178]. Posts may include sarcasm, coded language, memes,\\nimages, and contextual references that require deep linguistic and\\ncultural understanding for accurate interpretation [117, 138]. Addi-\\ntionally, the emotional tone of climate discourse is often intertwined\\nwith political ideology, narratives of environmental justice, mis-\\ninformation, and conspiracy theories, which complicates efforts\\nfor objective analysis [64, 101, 181]. The rapid changes in trending\\nhashtags, new slang, and platform-specific conventions contribute\\nto the linguistic volatility of platforms, making it more challeng-\\ning [9, 137].\\nDespite these challenges, sentiment analysis in this field shows\\ngreat potential. By examining how individuals feel, what they fear,\\nwhom they blame, and which solutions they support or reject, re-\\nsearchers can gain a better understanding of the emotional context\\nsurrounding climate events [95, 108, 140]. This can lead to bet-\\nter empathetic disaster communication strategies, identify mental\\nhealth stress signals, reveal misinformation dynamics, and guide\\npolicy discussions [2, 28, 44, 173].\\nIn addition to sudden climate disasters like wildfires and hur-\\nricanes, chronic stressors such as extreme summer heat in desert\\ncities are becoming an increasing public health concern. Urban\\narXiv:2504.18837v3  [cs.SI]  7 May 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 1}, page_content='Shaeri and Mohammadpour, et al.\\nClimate Events\\n- Wildfires\\n- Heatwaves\\n- Hurricanes, floods\\nSocial\\nMedia\\nData Collection\\nand\\nAnnotation\\nSentiment\\nModeling\\nReal-World\\nImpact\\nand\\nPolicies\\nFigure 1: Overview of the climate sentiment pipeline linking extreme events to social media reactions, analyzed through data,\\nmodels, and their societal impact. Our survey follows this structure, reviewing existing literature across each stage of the\\npipeline.\\nareas in the U.S. Southwest, in particular, face heightened warm-\\ning due to the Urban Heat Island (UHI) effect, which can often\\nexceed tolerable limits during summer heatwaves [11, 124, 149].\\nRecent studies indicate that prolonged exposure to heat can re-\\nsult in heat stress, physiological strain, and emotional exhaustion,\\nparticularly among vulnerable groups such as unhoused individ-\\nuals [87, 155]. Discussions about these impacts are increasingly\\nprevalent on social media, where public sentiment reflects distress\\nand frustration regarding inadequate urban infrastructure. There\\nare also rising calls for cooling interventions, such as shade or\\nmisting systems [29, 76, 171]. By considering extreme heat as a\\nslow-onset climate disaster, we broaden the focus of this survey to\\nincorporate the emotional and communicative aspects of prolonged\\nthermal crises.\\nThis survey provides a thorough overview of sentiment analysis\\ntechniques used in social media related to weather and climate\\nchange events 1. While there are general reviews available, very\\nfew specifically address the unique challenges, methodologies, and\\ninsights that emerge at the intersection of climate disasters and\\npublic sentiment [6, 147, 178]. To frame our discussion, we will use\\nthe 2025 Los Angeles forest fires as a case study. This large-scale\\nwildfire sparked intense online discourse, characterized by grief,\\noutrage, political polarization, and systemic critique [49, 64].\\nWe start by discussing the fundamental concepts in sentiment\\nand emotion analysis. We categorize the approaches into several\\ntypes, including early lexicon-based methods, traditional machine\\nlearning models, and more recent deep learning architectures. Ad-\\nditionally, we highlight the emerging use of large language models\\n(LLMs) like GPT-4 and LLaMA [22, 100, 106].\\nWe examine methods for real-time data collection and annota-\\ntion, including hashtag tracking, social media scraping, and weak\\nsupervision, which are used to create labeled datasets during rapidly\\nevolving climate events [9, 51, 133]. Special attention is given to\\nthe challenges of annotating sarcasm, emotional nuance, and cross-\\ncultural expressions of emotion in high-stakes and politically sensi-\\ntive contexts [117, 138, 140]. We also examine current evaluation\\nstrategies and datasets, highlighting key research gaps: the absence\\n1To support transparency and reproducibility, an anonymous GitHub repository is\\nmaintained at https://github.com/whisperoghost/ClimateSentiment, where the the list\\nof referenced papers is available and open for community contributions.\\nof multilingual and multimodal corpora, insufficient real-time mod-\\neling of sentiment evolution, and the inadequate representation\\nof misinformation-aware sentiment systems [96, 102, 178, 186]. A\\nprimary motivation for this survey is the increasing prevalence of\\nAI-generated misinformation during climate emergencies. [64, 123,\\n186]. Events like the 2025 LA forest fires revealed how synthetic\\nimages and videos, such as fake depictions of burning Hollywood\\nlandmarks, can mislead the public. [4, 33, 65].\\nIn addition to methodological synthesis, we explore how senti-\\nment analysis is incorporated into climate adaptation and mitiga-\\ntion frameworks. This includes its use in crisis dashboards, policy\\nresponsiveness, and early warning systems for emotional well-\\nbeing [25, 51, 59, 68]. We also emphasize efforts that extend beyond\\nclassification to include psychological aspects of emotional trauma,\\ndistress, and collective feelings during environmental crises [103,\\n121, 172]. Finally, we consider the broader societal implications,\\nincluding concerns about surveillance, algorithmic bias, and the\\npotential misuse of affective computing, especially in ways that\\nmay reinforce inequality or marginalize vulnerable voices [11, 12,\\n101, 117, 181]. This paper outlines a roadmap for interdisciplinary\\ncollaboration among NLP, social computing, environmental com-\\nmunication, and digital humanities, demonstrating how emotional\\nsignals in digital discourse can enhance our understanding and\\nresponse to the climate crisis (Figure 1).\\n2\\nPreliminaries\\nThis section defines key terms, techniques, and modeling paradigms\\nused throughout the discussion. The preliminaries orient readers\\nfrom diverse disciplines—ranging from NLP and machine learning\\nto climate science and social media studies—by introducing both\\ndomain-specific terminology and the computational tools applied to\\nsentiment analysis in the context of climate-related social discourse.\\n2.1\\nKey Concepts\\nSentiment Analysis: Also known as opinion mining, sentiment\\nanalysis is the computational task of identifying and categorizing\\nthe emotional valence expressed in a given piece of text. Most\\ncommonly, this involves labeling user-generated content—such as\\ntweets, Reddit posts, or Facebook comments—as positive, negative,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 2}, page_content='Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events\\nor neutral. In the context of climate-related events, sentiment anal-\\nysis helps detect expressions of outrage, fear, solidarity, blame, or\\nhope in the aftermath or buildup of environmental crises [24, 110].\\nEmotion Detection: Emotion detection extends beyond binary or\\nternary sentiment classification by identifying nuanced emotional\\nstates such as anger, anxiety, empathy, sadness, joy, or frustration.\\nThese emotional signals provide a finer-grained lens into public\\nperception and can be particularly important during climate dis-\\nasters where emotional polarity alone may not be sufficient to\\ncapture the full spectrum of human reactions [59, 140]. Emotion\\ndetection models often rely on specialized emotion lexicons or are\\nfine-tuned using emotion-labeled corpora such as GoEmotions [41]\\nor EmpatheticDialogues [141].\\nHeat Stress: Heat stress refers to the physiological and psycho-\\nlogical strain that occurs when the human body is exposed to ex-\\ntreme heat, particularly when it cannot effectively regulate its in-\\nternal temperature. This condition is exacerbated in urban areas\\ndue to the Urban Heat Island (UHI) effect, where built environ-\\nments retain heat more than natural landscapes. In desert cities\\nlike Phoenix or Las Vegas, heat stress can reach crisis levels dur-\\ning summer, posing severe health risks to vulnerable populations\\nincluding outdoor workers, children, the elderly, and unhoused\\nindividuals [67, 68, 174]. In the context of climate communication,\\nheat stress is not only a public health issue but also a recurring\\nemotional theme in social media posts, where users express anxiety,\\nexhaustion, and anger about ongoing exposure and insufficient\\nurban adaptation [25, 125].\\nEvent-Centric Sentiment: Unlike general sentiment analysis,\\nevent-centric sentiment focuses on linking emotional or affective\\nexpressions to specific real-world events or triggers. For instance, a\\ntweet expressing outrage about delayed wildfire evacuation efforts\\nmay be tagged not only as negative but also temporally and con-\\ntextually tied to a specific climate event (e.g., the 2025 Los Angeles\\nforest burns) [95, 142]. This approach enables sentiment tracking\\nacross the timeline of an event and supports temporal and causal\\nmodeling of emotional trajectories.\\nMisinformation and Disinformation: In climate discourse, the\\nline between fact and fiction can often be blurred. Misinformation\\nrefers to false or misleading information spread without harmful\\nintent, whereas disinformation denotes the deliberate spread of such\\ncontent to manipulate or deceive [139, 178]. Both phenomena can\\nsignificantly distort sentiment analysis outcomes by influencing\\npublic perception and emotional response to climate events. For\\nexample, false narratives about the causes of a wildfire or conspiracy\\ntheories surrounding geoengineering may amplify anger or fear\\nwithin online communities [64].\\n2.2\\nModeling Paradigms\\nThe sentiment analysis pipeline generally involves preprocessing\\nsocial media data, extracting features, training models, and eval-\\nuating predictions. A wide range of computational models have\\nbeen employed over the years, and their capabilities have evolved\\nalongside advances in machine learning and NLP:\\nTraditional Machine Learning Models: Early approaches to sen-\\ntiment analysis often relied on hand-engineered features—such as\\nn-grams, TF-IDF scores, and part-of-speech tags—fed into classi-\\ncal classifiers including Support Vector Machines (SVMs), Logistic\\nRegression, Decision Trees, and Random Forests. Although these\\nmodels remain interpretable and computationally efficient, they\\nstruggle with informal language, sarcasm, and domain-specific ter-\\nminology prevalent in climate discourse on social media [51, 142].\\nNeural Models: With the rise of deep learning, neural architectures\\nlike Convolutional Neural Networks (CNNs) and Long Short-Term\\nMemory networks (LSTMs) [164] gained prominence for sentiment\\nanalysis. These models are able to capture sequential dependencies\\nand local context in text data, better handling noisy and informal\\nsocial media language. In climate applications, LSTM-based mod-\\nels have been applied to wildfire discourse to analyze temporal\\nemotional patterns [59], while CNNs were employed in visual-text\\nsentiment fusion models in disaster-related image captioning [123].\\nTransformer-Based Models: The introduction of transformer\\narchitectures, particularly BERT (Bidirectional Encoder Representa-\\ntions from Transformers) [43] and its variants (e.g., RoBERTa [113],\\nDistilBERT [144], ClimateBERT [182]), has revolutionized text clas-\\nsification tasks. Transformers are pre-trained on massive corpora\\nand fine-tuned for downstream tasks with relatively small labeled\\ndatasets. They excel at capturing long-range dependencies and\\ncontextual semantics [140, 178]. In climate sentiment research, fine-\\ntuned transformers have been shown to outperform traditional\\nmodels in handling domain shifts, contextual nuance, and multi-\\nlingual inputs [154, 186].\\nLarge Language Models (LLMs): The most recent advancement\\nin the field comes with the emergence of generative large language\\nmodels like GPT-3.5, GPT-4, Claude, and LLaMA. These models,\\ntrained on trillions of tokens, possess strong capabilities in zero-\\nshot and few-shot learning through prompting. LLMs can be used\\nnot only for direct sentiment classification but also for generating\\nrationales, explanations, and even synthetic labeled datasets [169].\\nIn the context of climate event sentiment analysis, LLMs offer the\\npotential for real-time deployment and on-demand adaptation to\\nnew events without extensive retraining [63, 106].\\nTogether, these concepts and modeling paradigms establish the\\ntechnical and theoretical basis for the rest of this survey. As we\\nexplore specific methodologies, datasets, and challenges in subse-\\nquent sections, we will refer back to these foundational ideas to\\nsituate each contribution within the broader landscape of sentiment\\nanalysis for climate communication.\\n3\\nData Collection and Annotation\\nThe effectiveness of sentiment analysis for climate events depends\\nlargely on the quality of the analyzed data. Building accurate sentiment-\\naware models requires collecting representative, relevant, and high-\\nfidelity social media data. However, this task demands careful strate-\\ngies that consider platform differences, event-driven dynamics, and\\nlinguistic diversity [70, 180]. Annotation quality is equally crucial,\\nas it directly impacts model training, evaluation, and benchmarking.\\nThis section reviews major data sources and annotation methods in\\nclimate sentiment studies. Several works have leveraged hashtags,\\ngeolocation, and event-based keywords for social media data collec-\\ntion [9, 133]. Other studies have addressed challenges in annotating\\nemotional nuances across cultures, especially during fast-evolving\\ndisaster contexts [108, 117].'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 3}, page_content='Shaeri and Mohammadpour, et al.\\n3.1\\nSources of Climate-Related Social Media\\nData\\nSocial media provides a massive, real-time stream of user-generated\\ncontent that can reflect the collective emotional landscape during\\nclimate events. Several platforms are particularly relevant for min-\\ning sentiment in this domain:\\nTwitter/X and Mastodon: Twitter remains the most widely used\\nsource for climate-related sentiment analysis due to its public na-\\nture, short-text format, and widespread use during emergencies.\\nResearchers typically collect tweets using event specific hashtags\\n(e.g., #LAfire, #ClimateCrisis, #HeatWave), location-based filters,\\nor keyword-based queries. Twitter’s real-time API or tools such as\\nTweepy and Hydrator are commonly used to scrape tweet content,\\nmetadata, timestamps, and geolocation (when available). Mastodon,\\na decentralized alternative to Twitter, offers similar microblogging\\nfunctionality within a federated network of independent servers.\\nAlthough less widely adopted, its public posts and chronological\\ntimelines can also serve as a source for event-based sentiment track-\\ning [9, 83, 95].\\nReddit: Reddit hosts long-form discussions in climate-focused\\ncommunities such as r/climate, r/environment, r/news, and\\nr/disasterpreppers. Unlike Twitter, Reddit supports in-depth,\\ncontext-rich exchanges through dialogue trees, debates, and nu-\\nanced emotional reflection. Data are commonly collected using\\nPushshift APIs or custom web scrapers to access posts and hierar-\\nchical comment threads for a sentiment analysis [117].\\nNews Article Comment Sections and Blogs: Climate sentiment\\nis also expressed in the comment sections of digital news articles,\\nespecially during high-visibility events. Publicly available data sets\\nor scraped content from platforms such as The Guardian, CNN, or\\nlocal newspapers provide complementary sources for sentiment\\nextraction that differ in tone and vocabulary from traditional social\\nmedia [133].\\nMultimodal Platforms (TikTok, Instagram, YouTube): Al-\\nthough this survey focuses on textual sentiment, it is important to\\nnote that platforms such as TikTok and Instagram contain rich emo-\\ntional narratives shared through images, videos, captions, and com-\\nments. Although harder to process, these platforms offer a growing\\nfrontier for multimodal climate sentiment analysis [66, 96].\\n3.2\\nAnnotation Techniques for Climate\\nSentiment\\nOnce raw data is collected, it must be annotated with appropriate\\nsentiment labels before it can be used for model training or evalua-\\ntion. Climate-related social media data often contains subjective,\\nambiguous, or emotionally complex expressions that make labeling\\ndifficult. The following are the most commonly used annotation\\nstrategies in this domain:\\nManual Annotation: Human annotators—either domain experts\\nor crowdworkers recruited via platforms such as Amazon Mechan-\\nical Turk or Prolific—label social media posts based on predefined\\nsentiment categories [16, 45]. These may include basic polarity\\nlabels (positive, negative, neutral) or fine-grained emotions (e.g.,\\nanger, fear, hope, sadness). Manual annotation typically yields high-\\nquality labels but is time-consuming, expensive, and often incon-\\nsistent across annotators, especially when the content is politically\\ncharged or emotionally subtle. Inter-annotator agreement (e.g., Co-\\nhen’s kappa) is used to evaluate label reliability [10, 108].\\nWeak Supervision and Lexicon-Based Labeling: Instead of rely-\\ning on expensive manual labeling, many studies employ weak super-\\nvision techniques, including lexicon-based methods using resources\\nsuch as VADER [78], NRC Emotion Lexicon [127], SentiWordNet[21],\\nor AFINN [42]. These tools score words or phrases based on their\\nemotional polarity or affective strength and aggregate sentiment\\nacross a post. While scalable, lexicon-based methods struggle with\\nslang, sarcasm, negation, and domain-specific language (e.g., cli-\\nmate jargon or activist rhetoric), limiting their precision in climate\\nsentiment applications [53, 165].\\nRule-Based and Heuristic Methods: In some cases, heuristic\\nrules are defined based on hashtags, emojis, or common phrases [78].\\nFor example, tweets containing #ClimateEmergency or fire-related\\nemojis may be assumed to convey negative sentiment. These tech-\\nniques offer lightweight labeling strategies but must be used cau-\\ntiously to avoid bias and overgeneralization [185, 191].\\nLLM-Assisted Annotation: The growing availability of powerful\\nLLMs such as GPT-4 and Claude has enabled a new form of semi-\\nautomated annotation. With appropriate prompting, LLMs can label\\nsentiment with context-awareness, explain their reasoning, and\\neven provide probabilistic uncertainty estimates. In some studies,\\nhuman annotators review or verify LLM-generated labels, creating\\nhybrid annotation pipelines that combine scale with interpretabil-\\nity [22, 169]. However, LLM-generated labels may still reflect the\\nmodel’s training bias or hallucinate interpretations, especially in\\nemotionally charged or culturally sensitive contexts [5, 22, 131].\\nDistant Supervision from Emojis or Reactions: Some researchers\\nuse emojis or platform-specific reactions (e.g., Facebook’s “angry”\\nor “sad” emojis) as distant proxies for emotional labels. While noisy,\\nthis method enables the creation of large-scale labeled corpora\\nwith minimal manual intervention and can be used for pre-training\\nsentiment models [8, 111, 160].\\nAnnotation for Complex Tasks: In addition to basic sentiment,\\nsome climate studies annotate related tasks such as stance detection\\n(support/opposition to climate action), misinformation classifica-\\ntion, emotional intensity, or cause attribution. These require task-\\nspecific guidelines and are often used to train multi-task models\\ncapable of deeper interpretation [48, 92, 102, 138].\\nThe annotation strategy selected influences not only model per-\\nformance but also the type of insights that can be derived. Given\\nthe subjective and politicized nature of climate discourse, ensuring\\nannotation consistency, transparency, and ethical oversight is criti-\\ncal for producing trustworthy and socially responsible sentiment\\nanalysis systems.\\n4\\nMethodology Taxonomy\\nIn this section, we present a structured taxonomy of methodologies\\nused for sentiment analysis in the context of social media and\\nclimate-related events. These methods can be broadly grouped into\\nfive categories, reflecting both the historical evolution and current\\nstate-of-the-art in NLP: lexicon-based methods, traditional machine\\nlearning, deep learning architectures, transformer-based models,\\nand LLMs. Each category brings unique strengths and limitations in\\nhandling the linguistic complexity, emotional nuance, and domain-\\nspecificity of climate discourse.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 4}, page_content='Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events\\n4.1\\nLexicon-Based Methods\\nLexicon-based methods are among the earliest and most inter-\\npretable approaches to sentiment analysis. These rule-based mod-\\nels rely on predefined sentiment dictionaries—also known as lexi-\\ncons—that associate individual words or phrases with sentiment\\nscores. Prominent lexicons include:\\nSentiWordNet: An extension of WordNet that assigns polarity\\nscores (positive, negative, objective) to synsets.\\nVADER (Valence Aware Dictionary and sEntiment Reasoner):\\nA lexicon and rule-based sentiment analysis tool designed specifi-\\ncally for social media text, accounting for capitalization, intensifiers,\\nnegations, and emoticons.\\nNRC Emotion Lexicon: Associates words with multiple emotional\\ncategories such as anger, anticipation, and joy.\\nAFINN: Assigns integer-based sentiment scores to words based on\\ntheir emotional tone.\\nThese methods are fast, transparent, and easy to implement, mak-\\ning them useful for exploratory analyses or as weak supervision\\nsignals in downstream tasks. However, they struggle with the in-\\nformal, ambiguous, and context-dependent nature of social media\\nlanguage—particularly in climate-related discussions where sar-\\ncasm, irony, activism rhetoric, or technical terminology may not\\nbe represented in the lexicon. They are also limited in capturing\\nphrase-level or sentence-level sentiment composition, where word-\\nlevel polarity may be insufficient [95, 142, 188].\\n4.2\\nTraditional Machine Learning Approaches\\nTraditional supervised machine learning methods form the back-\\nbone of many early sentiment classifiers. These approaches typi-\\ncally involve transforming raw text into numerical feature represen-\\ntations—such as: Bag-of-Words (BoW) (A simple frequency count\\nof words in a document), Term Frequency-Inverse Document\\nFrequency (TF-IDF) (A weighting scheme that balances word\\nfrequency with distinctiveness across documents), and N-grams\\nand POS tags (Features capturing short sequences or grammatical\\nstructures).\\nOnce features are extracted, they are fed into classifiers such\\nas Support Vector Machines (SVMs), Naive Bayes, Logistic\\nRegression, and Random Forests.\\nWhile these models are relatively lightweight and interpretable,\\nthey rely heavily on feature engineering and cannot inherently\\nmodel word order or semantics. This limits their understanding\\nof nuanced sentiment or evolving slang in climate conversations.\\nNonetheless, they are still widely used as baselines or for ensemble\\nmethods when computational efficiency is prioritized [13, 18, 118].\\n4.3\\nDeep Learning Approaches\\nDeep learning revolutionized sentiment analysis by enabling mod-\\nels to learn complex patterns in raw text data without requiring\\nextensive manual feature engineering. The most common deep\\nlearning architectures include:\\nConvolutional Neural Networks (CNNs): Originally designed\\nfor image processing, CNNs have been adapted for text classification\\nby treating sentences as matrices of word embeddings. They are\\nparticularly good at capturing local n-gram patterns [50, 88, 90].\\nRecurrent Neural Networks (RNNs): Including LSTM and\\nGated Recurrent Unit (GRU) variants, RNNs are designed to model\\nsequential text dependencies and can capture the flow of sentiment\\nin longer texts or threads [47, 50, 58].\\nAttention Mechanisms: These allow models to dynamically\\nfocus on relevant parts of the input text when making predictions,\\nimproving performance in emotionally complex or multi-topic in-\\nputs [107, 119, 156].\\nDeep learning models outperform traditional approaches in cap-\\nturing context, emotion, and non-linear relationships. However,\\nthey typically require larger labeled datasets and are less inter-\\npretable. Their performance may degrade when applied to domains\\n(like climate discourse) with rare or emerging terms not well repre-\\nsented in pretraining data [20, 59, 64].\\n4.4\\nTransformer-Based Models\\nTransformer architectures, particularly since the introduction of\\nBERT, have become the dominant paradigm in NLP. Transformers\\nuse self-attention mechanisms to model relationships between all\\ntokens in a sentence simultaneously, allowing for deep contextual\\nunderstanding recently in climate research.\\nKey transformer models used in sentiment analysis include:\\nBERT (Bidirectional Encoder Representations from Trans-\\nformers): A deeply bidirectional model pre-trained on masked\\nlanguage modeling and next sentence prediction [39, 97, 175].\\nRoBERTa: A robustly optimized BERT variant trained with more\\ndata and dynamic masking [15, 128].\\nDistilBERT and ALBERT: Lightweight alternatives [61, 176].\\nClimateBERT: A domain-adapted BERT model fine-tuned on cli-\\nmate change-related texts, improving relevance for environmental\\ndiscourse [60, 148, 182].\\nThese models can be fine-tuned on small labeled datasets for sen-\\ntiment classification. They excel at handling context, sarcasm, nega-\\ntion, and multi-lingual sentiment, making them highly effective in\\nclimate-related social media analysis. However, their interpretabil-\\nity and computational cost remain areas of concern, especially for\\nreal-time deployment during rapidly unfolding events [19, 94].\\n4.5\\nLLMs for Sentiment Analysis\\nThe emergence of generative LLMs such as GPT-3.5, GPT-4, Claude,\\nand LLaMA represents a new frontier in sentiment analysis. Unlike\\nfixed classifiers, LLMs can be prompted dynamically to perform\\nzero-shot or few-shot sentiment classification using natural lan-\\nguage instructions [71, 98]. For example, a prompt such as:\\n\"Determine the sentiment of the following tweet: ‘Another day of\\norange skies and toxic air. When will the fires end?’\" →\"Sentiment:\\nNegative. Reason: The speaker expresses frustration and despair about\\nongoing wildfires.\"\\nLarge Language Models (LLMs) offer flexible sentiment analysis\\nthrough prompt-based inference, enabling them to interpret sar-\\ncasm, moral nuance, and emotional intensity without task-specific\\nfine-tuning [22, 26, 170]. They can also generate synthetic labeled\\ndata and rationales, supporting downstream models with improved\\ntraining samples and richer emotional reasoning [77, 167].\\nHowever, LLMs also have challenges: they may hallucinate labels\\nor explanations, exhibit bias, or lack transparency in their reasoning.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 5}, page_content='Shaeri and Mohammadpour, et al.\\nTheir outputs often require human oversight, especially in sensitive\\nor high-stakes domains such as climate misinformation or disaster\\nresponse [23, 179].\\nThe methodological landscape for sentiment analysis in climate\\ndiscourse has evolved significantly over the last decade. The choice\\nof method depends on the nature of the task, the quality and size\\nof available data, the urgency of analysis, and the interpretability\\nor ethical constraints of the application. In the following sections,\\nwe delve into the practical deployment of these models, supported\\nby case studies and performance benchmarks.\\n5\\nEvent-Centric Case Studies\\nEvent-centric analysis is a powerful lens through which we can\\nunderstand how sentiment evolves in real time and across different\\nphases of a climate-related disaster [80, 114]. By focusing on specific\\nhigh-impact events, researchers can study the temporal dynamics\\nof emotional expression, the emergence of misinformation, the role\\nof political and geographic factors, and the broader socio-cultural\\nimplications of public discourse. Especially, while earlier climate\\ndisasters were shaped by organic information flows and traditional\\nmedia narratives, the 2025 Los Angeles (LA) forest fires unfolded in\\nan era where both AI-generated text and synthetic images played\\nan active role in manipulating public perception [64, 184]. This\\ndual use of AI—both for analyzing sentiment and for fabricating\\nemotionally charged misinformation—marks a turning point in\\nclimate discourse [33]. This section presents a detailed case study\\nof the 2025 LA forest fires, followed by comparative insights drawn\\nfrom prior global climate events [37, 104].\\n5.1\\nLA Forest Fires (2025)\\nThe 2025 Los Angeles wildfires, driven by severe drought, high\\nwinds, and extreme heat, became one of the most destructive in\\nU.S. history. The fires led to mass evacuations, extensive property\\ndamage, fatalities, and widespread smoke pollution. Public senti-\\nment surged across social media in a highly networked environ-\\nment [86, 105]. AI-generated misinformation, such as fake images\\nand fabricated narratives, further complicated emotional reactions\\nand public understanding [64, 186]. These emotional responses re-\\nvealed deep psychological impacts, including trauma, helplessness,\\nand environmental grief.\\nSentiment Trajectory: Using temporal sentiment analysis on\\nmillions of geo-tagged and hashtag-filtered tweets (e.g., #LAFires,\\n#LAWildfire, #SummerHeat, #ClimateCrisis), researchers observed\\npronounced shifts in public emotion as the fires progressed. The\\nearly phase of the disaster was marked by fear and anxiety, often\\nexpressed through exclamatory, urgent language (e.g., “The skies\\nare blood orange — this is terrifying!”) [65, 146]. As the fires wors-\\nened, negative sentiment peaked, with sharp increases in anger,\\nhelplessness, and blame attribution [38, 183].\\nBlame and Political Polarization: A recurring pattern in the\\ndata was the attribution of responsibility. Users expressed frus-\\ntration at perceived government inaction, utility companies (e.g.,\\nPG&E), and climate inaction by political leaders. Sentiment analysis\\nclustered by political hashtags (#GreenNewDeal, #ClimateHoax)\\nrevealed stark polarization: some users framed the fires as evidence\\nof anthropogenic climate change, while others rejected such narra-\\ntives, attributing the crisis to poor forest management or “natural\\ncycles” [57, 112, 132].\\nEmergent Themes: Social media platforms also served as digi-\\ntal outlets for psychological coping and community support, where\\nexpressions of fear, confusion, and solidarity offered insight into\\nthe population’s collective emotional burden. Four thematic clus-\\nters emerged in the online discourse: (1) Public Health: Concerns\\nabout air quality, asthma, and mask usage are often tied to on-\\ngoing COVID-19 sensitivities [72, 184]. (2) Solidarity and Sup-\\nport: Tweets offering shelter, fundraising links, or expressions of\\nempathy (e.g., “Praying for SoCal”) [14, 30, 146, 157]. (3) Misin-\\nformation and Rumors: Unverified reports about evacuation\\nzones, fire origins, or conspiracy theories (e.g., “directed energy\\nweapons”) also spread rapidly, confounding sentiment classification\\nefforts [33, 145]. (4) Psychological Distress: Many users openly\\nshared feelings of anxiety, burnout, and emotional exhaustion, with\\nposts reflecting signs of trauma, climate grief, and mental health\\nstrain—underscoring the need for integrating psychological moni-\\ntoring into crisis response systems [14, 187].\\nTemporal Sentiment Modeling: Models such as Dynamic\\nBERT and LSTM-based sentiment trackers revealed that sentiment\\nfluctuations were tightly correlated with official press releases,\\nvisual media coverage, and viral content [69, 143]. These results\\nunderscore the need for temporally-aware sentiment systems that\\nadapt to evolving discourse and event phases (e.g., onset, peak,\\ncontainment, aftermath) [168].\\n5.2\\nComparison with Past Events\\nWhile the 2025 LA forest fires represent a recent and highly media-\\ntized case—marked by both AI-driven sentiment analysis and the\\nspread of synthetic media—sentiment patterns can be compared\\nwith earlier climate-related disasters, which unfolded without the\\ninfluence of generative AI, to identify commonalities and diver-\\ngences.\\nAustralian Bushfires (2019–2020): The “Black Summer” fires\\nin Australia generated a global outpouring of emotion on platforms\\nlike Twitter, with widespread sympathy, international solidarity\\n(e.g., donation campaigns), and ecological grief over wildlife loss.\\nSentiment was predominantly negative, but less politically polar-\\nized than in the U.S. context. Emotion detection highlighted deep\\nsadness and despair, amplified by distressing images of burned\\nkoalas and scorched landscapes [1, 126].\\nHurricane Ida (2021): Social media sentiment during Hurricane\\nIda, which struck the Gulf Coast and moved into the Northeastern\\nU.S., was shaped by resilience, resource disparity, and infrastructure\\nfailure themes. Tweets from affected areas conveyed helplessness\\nand frustration about power outages, while national discourse fo-\\ncused on emergency response. Compared to wildfire discourse, the\\nsentiment here was more logistical and survival-oriented, with\\nfewer ideological divides [85, 89].\\nEuropean Heatwaves (2022 and 2023): In Western Europe,\\nrepeated heatwaves fueled negative sentiment about heat stress,\\ntransport failures, and public health, while irony, memes, and dark\\nhumor complicated emotion classification. Political discussions on\\nenergy dependency, infrastructure, and climate denialism were'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 6}, page_content='Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events\\nprominent in French, German, and Spanish tweets, highlighting the\\nneed for multilingual sentiment analysis tools. [52, 79, 129, 166].\\nComparative Observations:\\nUniversal Emotions: Fear, anxiety, and anger were common across\\nall events, though expressed in culturally specific ways [25].\\nEvent Type Matters: Fires prompted more vivid imagery and per-\\nsonal storytelling; storms induced logistical and survival-focused\\nsentiments; heatwaves often sparked systemic critiques and satire [126].\\nTemporal Rhythm: Sentiment typically follows a surge - plateau\\n- decline pattern aligned with event progression and media cover-\\nage [74].\\nPlatform Differences: Twitter fosters real-time reaction and am-\\nplification; Reddit enables reflective dialogue; Facebook emphasizes\\nlocal community sentiment [133].\\nThese case studies collectively illustrate the value of event-\\ncentric sentiment analysis in climate communication research. They\\nalso highlight the need for models that are context-aware, culturally\\nsensitive, and capable of tracking emotional shifts over time and\\nspace.\\n6\\nEvaluation Metrics\\nEvaluating sentiment analysis models in climate discourse requires\\ninterpretable metrics that reflect both technical performance and\\nsocietal impact. These metrics have been widely used in climate-\\nrelated sentiment studies [3, 74, 81, 158].\\nAccuracy, Precision, Recall, F1-Score: Common classification\\nmetrics that assess correctness, balance between false positives\\nand negatives, and overall model reliability—especially under class\\nimbalance during crisis events [3, 35, 158].\\nMacro vs. Micro Averaging: Macro-averaged metrics highlight\\nminority emotion categories, supporting fairness, while micro-\\naveraged metrics reflect overall performance and scalability [31, 81,\\n192].\\nConfusion Matrix Analysis: Helps uncover systematic errors,\\nsuch as misinterpreting sarcasm or misclassifying anger as fear [34,\\n161].\\nEmotion Intensity Correlation: Pearson and Spearman correla-\\ntions assess agreement between model-predicted and human-rated\\nemotion strength [75, 109].\\nTemporal Stability: Evaluates how well models maintain accuracy\\nacross the timeline of a climate event—from onset to recovery [40,\\n73, 150].\\nHuman Judgment and LLM Output Evaluation: For LLMs, hu-\\nman evaluation is essential to assess explanation quality, emotional\\ncoherence, and alignment with human reasoning [27, 36, 46, 55].\\nFairness and Bias Audits: Tools and approaches like Equity Eval-\\nuation Corpus (EEC) [93] and disparity metrics detect performance\\ngaps across dialects or communities—critical for ensuring equitable\\nclimate sentiment analysis [62].\\n7\\nSocietal and Ethical Implications\\nThe use of sentiment analysis on climate-related social media con-\\ntent introduces several pressing societal and ethical considerations.\\nThese go beyond technical challenges and encompass questions of\\npower, representation, accountability, and harm mitigation. In this\\nsection, we examine three core dimensions of ethical concern: bias,\\nmisinformation, and the responsible use of user data.\\n7.1\\nBias and Representation\\nSentiment analysis tools, especially those trained on general-purpose\\nor Western-centric corpora, often fail to accurately capture the\\nvoices of marginalized groups or speakers using non-standard di-\\nalects, regional slang, or culturally specific expressions. For instance,\\nAfrican American Vernacular English (AAVE), Indigenous dialects,\\nor immigrant language mixing may be misinterpreted as negative or\\nneutral due to underrepresentation in training data. Similarly, hash-\\ntags or emojis that signal community identity or resilience may be\\nincorrectly labeled as emotionally neutral or ambiguous [103, 134].\\nMoreover, climate discourse itself is shaped by inequalities in\\nwho has access to digital platforms, whose experiences get ampli-\\nfied, and whose narratives are algorithmically suppressed. These\\ndisparities can lead to sentiment analyses that overrepresent afflu-\\nent, English-speaking, urban populations while underrepresenting\\nfrontline communities who experience the worst effects of climate\\nchange [91, 152, 177].\\nMitigating these biases requires inclusive data curation, diverse\\nannotator pools, dialect-aware modeling, and explicit fairness bench-\\nmarks.\\n7.2\\nMisinformation and Its Emotional Impact\\nClimate discourse is frequently entangled with misinformation,\\nconspiracy theories, and politically motivated disinformation cam-\\npaigns. False claims about the causes of wildfires, denial of anthro-\\npogenic climate change, or rumors about evacuation orders can\\ntrigger strong emotional reactions, which in turn skew sentiment\\ndistributions [136].\\nMisinformation can also be strategic: actors may intentionally\\nmanipulate sentiment through coordinated campaigns to induce\\npanic, deflect blame, or suppress climate activism. Models that\\nfail to detect or account for such manipulation may inadvertently\\nreinforce misleading narratives or amplify polarization [82, 136].\\nFuture sentiment systems expected to incorporate misinforma-\\ntion detection modules or co-training with fact-checking datasets.\\nAdditionally, researchers must reflect on whether to include or ex-\\nclude misinformation-driven posts in sentiment modeling pipelines,\\nand how to annotate them appropriately.\\n7.3\\nEthical Use and Privacy Concerns\\nAnalyzing public sentiment during climate disasters, particularly in\\nreal-time, raises critical questions about surveillance, consent, and\\ndata ethics. While much of social media content is technically public,\\nusers often do not expect their posts to be harvested, classified, or\\nscrutinized by algorithms—especially during traumatic events such\\nas home loss, injury, or community displacement [84].\\nReal-time sentiment tracking can be beneficial for disaster re-\\nsponse agencies, mental health interventions, or policy decision-\\nmaking, but it also risks infringing on personal privacy, stigmatiz-\\ning certain communities, or being repurposed for commercial or\\npolitical exploitation.\\nKey ethical guidelines include:'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 7}, page_content='Shaeri and Mohammadpour, et al.\\nInformed Consent: Where possible, respect user expectations\\nand include ethical disclosures when deploying sentiment systems\\nat scale [99].\\nAnonymization and Aggregation: Always remove personally\\nidentifiable information (PII) and present findings at the aggregate\\nlevel to protect user identity [24, 177].\\nImpact Audits: Regularly assess how sentiment outputs may\\naffect public narratives, media reporting, or institutional decisions,\\nespecially when tied to resource allocation or public safety [24, 136].\\nCommunity Involvement: Collaborate with affected commu-\\nnities to co-interpret sentiment results and ensure outputs are mean-\\ningful, respectful, and contextually accurate [7, 24].\\nWhile sentiment analysis offers powerful tools for understanding\\npublic response to climate change, its application must be guided\\nby a principled commitment to fairness, accountability, and respect\\nfor human dignity—especially as we face increasingly severe and\\nemotionally charged climate disruptions.\\n8\\nChallenges and Future Work\\nWhile significant progress has been made in developing sentiment\\nanalysis systems for social media and climate discourse, numer-\\nous open challenges remain. These span technical limitations, data\\navailability, linguistic diversity, ethical boundaries, and model inter-\\npretability. Addressing these issues will be crucial for scaling sen-\\ntiment systems that are accurate, equitable, and practically useful\\nduring climate crises. Below, we outline five prominent directions\\nfor future research.\\nMultimodal Sentiment Analysis (Text + Images/Videos): Cli-\\nmate events are often captured and shared not just through text,\\nbut via images, videos, emojis, and other non-verbal modalities.\\nAerial photos of wildfire damage, videos of flooded neighborhoods,\\nor infographics on heatwaves all carry strong emotional signals.\\nHowever, most sentiment analysis models remain text-centric. Re-\\ncent work has explored multimodal sentiment analysis in other\\ndomains—such as general cross-lingual or affective computing\\ntasks [122, 135, 163]—but not in the context of climate research.\\nFuture work should explore fusion models that integrate vision\\nand language (e.g., CLIP, LRQ-Fact, or image-caption sentiment\\nalignment) to capture the full affective scope of climate discourse.\\nThis also includes analyzing visual misinformation or emotionally\\nmanipulative content in climate activism or denial.\\nReal-Time Emotion Dynamics During Evolving Events: Cli-\\nmate events unfold over time, often with unpredictable trajectories.\\nThe public’s emotional response can shift rapidly—from fear and\\nconfusion during the early stages, to anger and blame as impacts\\nescalate, to grief and fatigue in the aftermath. Capturing these dy-\\nnamics requires temporally-aware models that update continuously\\nas new data flows in. Future research should focus on streaming\\nsentiment systems, adaptive classifiers, and emotion state model-\\ning (e.g., hidden Markov models or dynamic embeddings) to track\\nemotional evolution during disaster timelines.\\nFine-Grained Emotion Classification: Current sentiment sys-\\ntems often operate at coarse granularity (positive/negative/neutral)\\nor with a limited emotion palette (anger, fear, joy, sadness). How-\\never, climate communication involves complex emotional states\\nsuch as helplessness, betrayal, anxiety, urgency, climate grief, and\\neco-hope. These nuanced emotions are particularly important for\\ninforming targeted interventions (e.g., mental health support, mo-\\nbilization campaigns). Developing new taxonomies, datasets, and\\nannotation guidelines for fine-grained emotional categorization\\nremains an open and pressing challenge.\\nCross-Lingual and Cross-Cultural Sentiment Tracking: Cli-\\nmate change is a global phenomenon, yet most sentiment models\\nare trained on English-language data. This excludes vast popula-\\ntions whose emotional narratives unfold in Spanish, Hindi, Arabic,\\nSwahili, Mandarin, and other global and Indigenous languages [190].\\nWhile many platforms now offer automatic translation features,\\nthese often fail to preserve nuanced emotional tone, cultural idioms,\\nor context-specific sarcasm—leading to sentiment distortion or loss.\\nTherefore, effective cross-lingual sentiment analysis—utilizing mul-\\ntilingual transformers (such as XLM-R and mBERT), translation-\\nenhanced frameworks, or culturally sensitive large language mod-\\nels—will be essential for global-scale monitoring and new directives\\nin the investigation of [122]. Furthermore, cultural factors shape\\nhow emotions are expressed and interpreted, demanding models\\nthat avoid Western-centric assumptions and account for regional\\naffective norms.\\nIntegration with LLM Reasoning and Explanation Frame-\\nworks: LLMs offer unprecedented capabilities in contextual under-\\nstanding, emotional reasoning, and explanation generation. How-\\never, their integration into sentiment analysis pipelines remains\\nunderexplored. Future systems could prompt LLMs not only for\\nclassification, but also for rationale generation (“Why is this tweet\\nangry?”), counterfactual reasoning (“How would sentiment change\\nif the message came from a public official?”), and cross-checking\\nfor hallucinations. Additionally, combining LLM sentiment outputs\\nwith symbolic reasoning or graph-based models may yield more\\ninterpretable and controllable systems.\\nBeyond these technical directions, future work should also prior-\\nitize interdisciplinary collaboration with social scientists, linguists,\\nclimate communication scholars, and community stakeholders to\\nensure that sentiment models are not only computationally robust\\nbut also socially grounded and actionable.\\n9\\nConclusion\\nSentiment analysis of social media in the context of climate change\\nand extreme weather events represents both a compelling technical\\nchallenge and a profound psychological and societal necessity. As\\nclimate disruptions become more frequent, severe, and emotionally\\ncharged, the ability to understand and respond to public sentiment\\nin real time becomes increasingly important for researchers, re-\\nsponders, policymakers, and citizens alike.\\nIn this survey, we have provided a comprehensive taxonomy of\\nsentiment analysis methodologies, reviewed a diverse set of anno-\\ntation strategies, and examined how these tools have been applied\\nto real-world climate events such as the 2025 Los Angeles forest\\nfires. We analyzed the strengths and limitations of various model-\\ning approaches—from lexicon-based systems and classical machine\\nlearning to deep learning, transformer architectures, and the latest\\nLLMs. We also highlighted the societal and ethical implications\\nof applying sentiment technologies in high-stakes environments,\\nincluding issues of bias, misinformation, and privacy.\\nBy identifying emerging trends, methodological challenges, and\\nfuture research opportunities, we hope this survey serves as both a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 8}, page_content='Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events\\nreference and a roadmap for the community. Our goal is to foster\\nresponsible, inclusive, and scientifically rigorous sentiment analy-\\nsis research that enhances public understanding, informs climate\\naction, and supports more empathetic and equitable responses to a\\nwarming world.\\nAcknowledgments\\nThe author would like to thank the faculty and colleagues at the\\nSchool of Computing and Augmented Intelligence, Arizona State\\nUniversity, for their valuable feedback and support during the devel-\\nopment of this work. Special thanks to the SHaDE Lab for ongoing\\ninspiration in interdisciplinary climate research, and to the wider\\nNLP and computational social science communities for their contri-\\nbutions to open tools, datasets, and DMML lab for discussions that\\nshaped this survey. Automated tools, including GPT-based models,\\nwere used to assist with grammar refinement and minor language\\nediting for clarity and fluency.\\nReferences\\n[1] Carole Adam, Charles Bailly, and Julie Dugdale. 2023. Communication during\\nbushfires, towards a serious game for a serious matter: communication during\\nbushfires. In Research Anthology on Managing Crisis and Risk Communications.\\nIGI Global, 100–130.\\n[2] Ioannis Adamopoulos, Aikaterini Frantzana, and Niki Syrou. 2024. Climate\\ncrises associated with epidemiological, environmental, and ecosystem effects\\nof a storm: Flooding, landslides, and damage to urban and rural areas (Extreme\\nweather events of Storm Daniel in Thessaly, Greece). In Medical Sciences Forum,\\nVol. 25. MDPI, 7.\\n[3] Ioannis Adamopoulos, Antonios Valamontes, and George Dounias. 2025. Pre-\\ndictive Analysis Research for Workplace Job Risks and Burnout of Public Health\\nand Safety Inspectors Amid the Global Climate Crisis. (2025).\\n[4] Melissa Fleur Afshar. 2025. How AI Convinced World Hollywood Sign Was\\nBurning Down in LA Fire. https://www.newsweek.com/how-ai-convinced-\\nhollywood-sign-burning-down-los-angeles-sunset-fire-2013048\\nAccessed:\\n2025-01-24.\\n[5] Pegah Ahadian and Qiang Guan. 2025. A Survey on Hallucination in Large\\nLanguage and Foundation Models. (2025).\\n[6] Fridoon Jawad Ahmad. 2022. Climate change: A Global Crisis. Pakistan Journal\\nof Health Sciences (2022), 01–01.\\n[7] Shahriar Akter and Samuel Fosso Wamba. 2019. Big data and disaster manage-\\nment: a systematic review and agenda for future research. Annals of Operations\\nResearch 283 (2019), 939–959.\\n[8] Ahmed Al-Rawi, Breanna Blackwell, Oumar Kane, Derrick O’Keefe, and Aimé-\\nJules Bizimana. 2022. COVID-19 in the time of climate change: Memetic dis-\\ncourses on social media. Environmental Communication 16, 7 (2022), 864–882.\\n[9] Walid Al-Saqaf and Peter Berglez. 2019. How do social media users link different\\ntypes of extreme events to climate change? A study of Twitter during 2008–2017.\\nJournal of Extreme Events 6, 02 (2019), 1950002.\\n[10] Rahma Alahmary and Hmood Al-Dossari. 2023. A semiautomatic annotation\\napproach for sentiment analysis. Journal of Information Science 49, 2 (2023),\\n398–410.\\n[11] Saud R AlKhaled, Ariane Middel, Pouya Shaeri, Isaac Buo, and Florian A Schnei-\\nder. 2024. WebMRT: An online tool to predict summertime mean radiant\\ntemperature using machine learning. Sustainable Cities and Society 115 (2024),\\n105861.\\n[12] Shayan Meshkat Alsadat, Nasim Baharisangari, Yash Paliwal, and Zhe Xu. 2024.\\nDistributed reinforcement learning for swarm systems with reward machines.\\nIn 2024 American Control Conference (ACC). IEEE, 33–38.\\n[13] Shayan Meshkat Alsadat and Zhe Xu. 2024. Multi-Agent Reinforcement Learn-\\ning in Non-Cooperative Stochastic Games Using Large Language Models. IEEE\\nControl Systems Letters (2024).\\n[14] Francesca Ancarani, Pedro Garijo Añaños, Bain Gutiérrez, Juan Pérez-Nievas,\\nGermán Vicente-Rodríguez, and Fernando Gimeno Marco. 2025. The Effective-\\nness of Debriefing on the Mental Health of Rescue Teams: A Systematic Review.\\nInternational Journal of Environmental Research and Public Health 22, 4 (2025),\\n590.\\n[15] Merih Angin, Beyza Taşdemir, Cenk Arda Yılmaz, Gökcan Demiralp, Mert Atay,\\nPelin Angin, and Gökhan Dikmener. 2022. A roberta approach for automated\\nprocessing of sustainability reports. Sustainability 14, 23 (2022), 16139.\\n[16] Sinem Aslan, Sinem Emine Mete, Eda Okur, Ece Oktay, Nese Alyuz, Utku Ergin\\nGenc, David Stanhill, and Asli Arslan Esme. 2017. Human expert labeling\\nprocess (HELP): towards a reliable higher-order user state labeling process and\\ntool to assess student engagement. Educational Technology (2017), 53–59.\\n[17] Reza Rahimi Azghan, Nicholas C Glodosky, Ramesh Kumar Sah, Carrie Cuttler,\\nRyan McLaughlin, Michael J Cleveland, and Hassan Ghasemzadeh. 2023. Per-\\nsonalized modeling and detection of moments of cannabis use in free-living\\nenvironments. In 2023 IEEE 19th International Conference on Body Sensor Net-\\nworks (BSN). IEEE, 1–4.\\n[18] Reza Rahimi Azghan, Nicholas C Glodosky, Ramesh Kumar Sah, Carrie Cuttler,\\nRyan McLaughlin, Michael J Cleveland, and Hassan Ghasemzadeh. 2024. CU-\\nDLE: Learning Under Label Scarcity to Detect Cannabis Use in Uncontrolled\\nEnvironments. arXiv preprint arXiv:2410.03211 (2024).\\n[19] Reza Rahimi Azghan, Nicholas C Glodosky, Ramesh Kumar Sah, Carrie Cuttler,\\nRyan McLaughlin, Michael J Cleveland, and Hassan Ghasemzadeh. 2025. CAN-\\nSTRESS: A Real-World Multimodal Dataset for Understanding Cannabis Use,\\nStress, and Physiological Responses. arXiv preprint arXiv:2503.19935 (2025).\\n[20] Reza Rahimi Azghan, Nicholas C Glodosky, Ramesh Kumar Sah, Carrie Cuttler,\\nRyan McLaughlin, Michael J Cleveland, and Hassan Ghasemzadeh. 2025. CU-\\nDLE: Learning Under Label Scarcity to Detect Cannabis Use in Uncontrolled\\nEnvironments using Wearables. IEEE Sensors Journal (2025).\\n[21] Stefano Baccianella, Andrea Esuli, Fabrizio Sebastiani, et al. 2010. Sentiwordnet\\n3.0: an enhanced lexical resource for sentiment analysis and opinion mining..\\nIn Lrec, Vol. 10. Valletta, 2200–2204.\\n[22] Alimohammad Beigi, Bohan Jiang, Dawei Li, Tharindu Kumarage, Zhen Tan,\\nPouya Shaeri, and Huan Liu. 2024. Lrq-fact: Llm-generated relevant questions\\nfor multimodal fact-checking. arXiv preprint arXiv:2410.04616 (2024).\\n[23] Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu, and\\nHuan Liu. 2024. Model attribution in llm-generated disinformation: A domain\\ngeneralization approach with supervised contrastive learning. In 2024 IEEE 11th\\nInternational Conference on Data Science and Advanced Analytics (DSAA). IEEE,\\n1–10.\\n[24] Ghazaleh Beigi, Xia Hu, Ross Maciejewski, and Huan Liu. 2016. An Overview of\\nSentiment Analysis in Social Media and Its Applications in Disaster Relief. Springer\\nInternational Publishing, Cham, 313–340. doi:10.1007/978-3-319-30319-2_13\\n[25] Magnus Bergquist, Andreas Nilsson, and P Wesley Schultz. 2019. Experiencing\\na severe weather event increases concern about climate change. Frontiers in\\npsychology 10 (2019), 220.\\n[26] Naman Bhargava, Mohammed I Radaideh, O Hwang Kwon, Aditi Verma, and\\nMajdi I Radaideh. 2025. On the Impact of Language Nuances on Sentiment\\nAnalysis with Large Language Models: Paraphrasing, Sarcasm, and Emojis.\\narXiv preprint arXiv:2504.05603 (2025).\\n[27] Rachel Bina, Kha Luong, Shrey Mehta, Daphne Pang, Mingjun Xie, Christine\\nChou, and Steven O Kimbrough. 2025. On Large Language Models as Data\\nSources for Policy Deliberation on Climate Change and Sustainability. arXiv\\npreprint arXiv:2503.05708 (2025).\\n[28] Luca Bracchetti, Martina Capriotti, Massimiliano Fazzini, Paolo Cocci, and\\nFrancesco Alessandro Palermo. 2024. Mass mortality event of Mediterranean\\nmussels (Mytilus galloprovincialis) in the middle Adriatic: potential implications\\nof the climate crisis for marine ecosystems. Diversity 16, 3 (2024), 130.\\n[29] Matthew Browning, Kathleen Wolf, Pamela Murray-Tuite, Lara Browning,\\nMashrur Chowdhury, Chien-Fei Chen, David Coyle, Angel M Dzhambov, Chao\\nFan, Richard Hauer, et al. 2024. The Value and Urgency of Transportation\\nForestry. (2024).\\n[30] California Governor’s Office of Emergency Services. 2025.\\nShelter Avail-\\nable for Communities Impacted by the Wildfires in Southern Califor-\\nnia. https://news.caloes.ca.gov/shelters-available-for-communities-impacted-\\nby-wildfires-in-southern-california-5/ Accessed: 2025-04-24.\\n[31] Erik Cambria, Yang Li, Frank Z Xing, Soujanya Poria, and Kenneth Kwok. 2020.\\nSenticNet 6: Ensemble application of symbolic and subsymbolic AI for sentiment\\nanalysis. In Proceedings of the 29th ACM international conference on information\\n& knowledge management. 105–114.\\n[32] Anabela Carvalho. 2009. Reporting the climate change crisis. In The Routledge\\nCompanion to News and Journalism. Routledge, 485–495.\\n[33] Bill Chappell. 2025. LA’s wildfires prompted a rash of fake images. Here’s why.\\nhttps://www.npr.org/2025/01/16/nx-s1-5259629/la-wildfires-fake-images Ac-\\ncessed: 2025-04-24.\\n[34] Priyavrat Chauhan, Nonita Sharma, and Geeta Sikka. 2021. The emergence\\nof social media data and sentiment analysis in election prediction. Journal of\\nAmbient Intelligence and Humanized Computing 12 (2021), 2601–2627.\\n[35] Davide Chicco and Giuseppe Jurman. 2020. The advantages of the Matthews\\ncorrelation coefficient (MCC) over F1 score and accuracy in binary classification\\nevaluation. BMC genomics 21 (2020), 1–13.\\n[36] A. Chiorrini, C. Diamantini, A. Mircoli, et al. [n. d.]. Emotion and Sentiment\\nAnalysis of Tweets Using BERT. ([n. d.]).\\n[37] Akshat Chulahwat and Hussam Mahmoud. 2024. The impact of wind charac-\\nteristics on the spatial distribution of damage to the built environment during\\nwildfire events: the 2022 Marshall Fire. Natural Hazards Review 25, 1 (2024),'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 9}, page_content='Shaeri and Mohammadpour, et al.\\n06023003.\\n[38] Shelby N Corning, Esther Boere, Andrey Krasovskiy, Andrey Lessa Derci Au-\\ngustynczik, Ted Shepherd, Rohit Ghosh, Florian Kraxner, and Peter Havlík.\\n2024. Flammable futures—storylines of climatic impacts on wildfire events and\\npalm oil plantations in Indonesia. Environmental Research Letters 19, 11 (2024),\\n114039.\\n[39] Tom Corringham, Daniel Spokoyny, Eric Xiao, Christopher Cha, Colin Lemarc-\\nhand, Mandeep Syal, Ethan Olson, and Alexander Gershunov. 2021. Bert clas-\\nsification of paris agreement climate action plans. In ICML 2021 Workshop on\\nTackling Climate Change with Machine Learning, Vol. 45.\\n[40] Biraj Dahal, Sathish AP Kumar, and Zhenlong Li. 2019. Topic modeling and\\nsentiment analysis of global climate change tweets. Social network analysis and\\nmining 9 (2019), 1–20.\\n[41] Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gau-\\nrav Nemade, and Sujith Ravi. 2020. GoEmotions: A dataset of fine-grained\\nemotions. arXiv preprint arXiv:2005.00547 (2020).\\n[42] Radhi D Desai. 2018. Sentiment analysis of Twitter data. In 2018 Second Interna-\\ntional Conference on Intelligent Computing and Control Systems (ICICCS). IEEE,\\n114–117.\\n[43] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert:\\nPre-training of deep bidirectional transformers for language understanding. In\\nProceedings of the 2019 conference of the North American chapter of the association\\nfor computational linguistics: human language technologies, volume 1 (long and\\nshort papers). 4171–4186.\\n[44] Raktima Dey and Sophie C Lewis. 2021. Natural disasters linked to climate\\nchange. In The Impacts of Climate Change. Elsevier, 177–193.\\n[45] Marco D’Orazio, Elisa Di Giuseppe, and Gabriele Bernardini. 2022. Automatic\\ndetection of maintenance requests: Comparison of Human Manual Annotation\\nand Sentiment Analysis techniques. Automation in Construction 134 (2022),\\n104068.\\n[46] James N Druckman and Mary C McGrath. 2019. The evidence for motivated\\nreasoning in climate change preference formation. Nature Climate Change 9, 2\\n(2019), 111–119.\\n[47] Mohcen El Makkaoui, Anouar Dalli, and Khalid Elbaamrani. 2024. A Com-\\nparative Study of RNN and DNN for Climate Prediction. In 2024 International\\nConference on Global Aeronautical Engineering and Satellite Technology (GAST).\\nIEEE, 1–6.\\n[48] Al Nahian Bin Emran, Amrita Ganguly, Sadiya Sayara Chowdhury Puspo,\\nDhiman Goswami, and Md Nishat Raihan. 2024. Masonperplexity at climate-\\nactivism 2024: Integrating advanced ensemble techniques and data augmenta-\\ntion for climate activism stance and hate event identification. arXiv preprint\\narXiv:2402.01976 (2024).\\n[49] Ebrahim Farahmand, Reza Rahimi Azghan, Nooshin Taheri Chatrudi, Eric Kim,\\nGautham Krishna Gudur, Edison Thomaz, Giulia Pedrielli, Pavan Turaga, and\\nHassan Ghasemzadeh. 2025. Attengluco: Multimodal transformer-based blood\\nglucose forecasting on ai-readi dataset. arXiv preprint arXiv:2502.09919 (2025).\\n[50] Vahid Farhangmehr, Hanifeh Imanian, Abdolmajid Mohammadian, Juan Hiedra\\nCobo, Hamidreza Shirkhani, and Pierre Payeur. 2025. A spatiotemporal CNN-\\nLSTM deep learning model for predicting soil temperature in diverse large-scale\\nregional climates. Science of The Total Environment 968 (2025), 178901.\\n[51] Miriam Fernandez, Lara SG Piccolo, Diana Maynard, Meia Wippoo, Christoph\\nMeili, and Harith Alani. 2016. Talking climate change via social media: commu-\\nnication, engagement and behaviour. In Proceedings of the 8th ACM Conference\\non Web Science. 85–94.\\n[52] Frauke Feser, Linda van Garderen, and Felicitas Hansen. 2024. The Summer\\nHeatwave 2022 over Western Europe: An Attribution to Anthropogenic Climate\\nChange. Bulletin of the American Meteorological Society 105, 11 (2024), E2175–\\nE2179.\\n[53] Julia Coombs Fine. 2022. Language and social justice in US climate movements:\\nBarriers and ways forward. Frontiers in Communication 7 (2022), 920568.\\n[54] Jaymund M Floranza and John Mark R Asio. 2019. The Impact of Disasters and\\nClimate Change on Migration and Displacement. (2019).\\n[55] Nina L Frings, Jessica F Helm, and Ulf JJ Hahnel. 2024. The energy crisis\\ndifferentially impacted Swiss and German citizens’ energy literacy and efficiency\\npreferences but not their support for climate policies. Communications Earth &\\nEnvironment 5, 1 (2024), 544.\\n[56] Albert J Gabric. 2023. The climate change crisis: a review of its causes and\\npossible responses. Atmosphere 14, 7 (2023), 1081.\\n[57] Colin S Gannon and Nik C Steinberg. 2021. A global assessment of wildfire\\npotential under climate change utilizing Keetch-Byram drought index and\\nland cover classifications. Environmental Research Communications 3, 3 (2021),\\n035002.\\n[58] Wenliang Gao, Jingxiang Gao, Liu Yang, Mingjun Wang, and Wenhao Yao. 2021.\\nA novel modeling strategy of weighted mean temperature in China using RNN\\nand LSTM. Remote Sensing 13, 15 (2021), 3004.\\n[59] Yury E García, Miryam Elizabeth Villa-Pérez, Kuang Li, Xiao Hui Tai, Luis A\\nTrejo, Maria L Daza-Torres, J Cricelio Montesinos-López, and Miriam Nuño.\\n2024. Wildfires and social media discourse: exploring mental health and emo-\\ntional wellbeing through Twitter. Frontiers in public health 12 (2024), 1349609.\\n[60] Eduardo C Garrido-Merchán, Cristina González-Barthe, and María Coronado\\nVaca. 2023. Fine-tuning ClimateBert transformer with ClimaText for the disclo-\\nsure analysis of climate-related financial risks. arXiv preprint arXiv:2303.13373\\n(2023).\\n[61] Elizabeth Leah George and Subashini Parthasarathy. 2024. Decoding Bias in#\\nClimatechange Tweets: A Neural Language Style Transfer Approach. In 2024\\nInternational Conference on Artificial Intelligence and Quantum Computation-\\nBased Sensor Application (ICAIQSA). IEEE, 1–7.\\n[62] Caroline M Gevaert, Thomas Buunk, and Marc JC Van Den Homberg. 2024. Au-\\nditing geospatial datasets for biases: Using global building datasets for disaster\\nrisk management. IEEE Journal of Selected Topics in Applied Earth Observations\\nand Remote Sensing (2024).\\n[63] Pawanjit Singh Ghatora, Seyed Ebrahim Hosseini, Shahbaz Pervez, Muham-\\nmad Javed Iqbal, and Nabil Shaukat. 2024. Sentiment Analysis of Product\\nReviews Using Machine Learning and Pre-Trained LLM. Big Data and Cognitive\\nComputing 8, 12 (2024), 199.\\n[64] Frédéric Gimello. 2025. From Embers to Rumors: Decoding the Societal Impact\\nof the January 2025 Los Angeles Wildfires on Misinformation. (2025).\\n[65] Frédéric Gimello-Mesplomb. [n. d.]. Decoding January 2025 Los Angeles Wild-\\nfires: How (and Why) the Emotional Power of Iconic Fires Revives Ancestral\\nFears and Fuels Misinformation. ([n. d.]).\\n[66] Nicholas C Glodosky, Michael J Cleveland, Reza Rahimi Azghan, Hassan\\nGhasemzadeh, Ryan J McLaughlin, and Carrie Cuttler. 2024. Multimodal exami-\\nnation of daily stress rhythms in chronic Cannabis users. Psychopharmacology\\n(2024), 1–24.\\n[67] Gisel Guzman-Echavarria, Ariane Middel, Daniel J Vecellio, and Jennifer Vanos.\\n2024. The development of an adaptive heat stress compensability classification\\napplied to the United States. International journal of biometeorology (2024),\\n1–15.\\n[68] P Sol Hart, Lauren Feldman, Soobin Choi, Sedona Chinn, and Dan Hiaeshutter-\\nRice. 2024. Climate Change Advocacy and Engagement on Social Media. Science\\nCommunication (2024), 10755470241284934.\\n[69] Aixiang He and Mideth Abisado. 2024. Text sentiment analysis of Douban film\\nshort comments based on BERT-CNN-BiLSTM-Att model. Ieee Access (2024).\\n[70] SungKu Heo, Pouya Ifaei, Mohammad Moosazadeh, and ChangKyoo Yoo. 2022.\\nPublic perception assessment on climate change and natural disaster influence\\nusing social media big-data: A case study of USA. In EGU General Assembly\\nConference Abstracts. EGU22–3482.\\n[71] David Herrera-Poyatos, Carlos Peláez-González, Cristina Zuheros, Andrés\\nHerrera-Poyatos, Virilo Tejedor, Francisco Herrera, and Rosana Montes. 2025.\\nAn overview of model uncertainty and variability in LLM-based sentiment\\nanalysis. Challenges, mitigation strategies and the role of explainability. arXiv\\npreprint arXiv:2504.04462 (2025).\\n[72] Attila J Hertelendy, Jeremy Maggin, and Gregory R Ciottone. 2025. Health-care\\nsystem adaptability during wildfire disasters: crucial insights from LA County.\\nThe Lancet (2025).\\n[73] Solomon M Hsiang and Marshall Burke. 2014. Climate, conflict, and social\\nstability: what does the evidence say? Climatic change 123 (2014), 39–55.\\n[74] Qian Hu, Seongho An, Naim Kapucu, Timothy Sellnow, Murat Yuksel, Rebecca\\nFreihaut, and Prasun Kanti Dey. 2024. Emergency communication networks\\non Twitter during Hurricane Irma: information flow, influential actors, and top\\nmessages. Disasters 48, 4 (2024), e12628.\\n[75] Huanchun Huang, Yang Li, Yimin Zhao, and Wei Zhai. 2022. Analysis of the\\nimpact of urban summer high temperatures and outdoor activity duration on\\nresidents’ emotional health: Taking hostility as an example. Frontiers in Public\\nHealth 10 (2022), 955077.\\n[76] Xinjie Huang, Elie Bou-Zeid, Jennifer Vanos, Ariane Middel, and Prathap Ra-\\nmamurthy. 2025. Outdoor Misting Is an Effective Blue Infrastructure Solution\\nfor Urban Heat Mitigation. In 105th AMS Annual Meeting. AMS.\\n[77] Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, and\\nYuzhong Qu. 2024. TARGA: Targeted Synthetic Data Generation for Practical\\nReasoning over Structured Data. arXiv preprint arXiv:2412.19544 (2024).\\n[78] Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model\\nfor sentiment analysis of social media text. In Proceedings of the international\\nAAAI conference on web and social media, Vol. 8. 216–225.\\n[79] Chibuike Chiedozie Ibebuchi and Itohan-Osa Abu. 2023. Characterization of\\ntemperature regimes in Western Europe, as regards the summer 2022 Western\\nEuropean heat wave. Climate Dynamics 61, 7 (2023), 3707–3720.\\n[80] Umar Ishfaq, Hikmat Ullah Khan, and Danial Shabbir. 2024. Exploring the role\\nof sentiment analysis with network and temporal features for finding influential\\nusers in social media platforms. Social Network Analysis and Mining 14, 1 (2024),\\n241.\\n[81] Tunazzina Islam, Ruqi Zhang, and Dan Goldwasser. 2023. Analysis of climate\\ncampaigns on social media using bayesian model averaging. In Proceedings of\\nthe 2023 AAAI/ACM Conference on AI, Ethics, and Society. 15–25.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 10}, page_content='Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events\\n[82] S Mo Jang and P Sol Hart. 2015. Polarized frames on “climate change” and\\n“global warming” across countries and states: Evidence from Twitter big data.\\nGlobal environmental change 32 (2015), 11–17.\\n[83] Ujun Jeong, Alimohammad Beigi, Anique Tahir, Susan Xu Tang, H Russell\\nBernard, and Huan Liu. 2025. FediverseSharing: A Novel Dataset on Cross-\\nPlatform Interaction Dynamics between Threads and Mastodon Users. arXiv\\npreprint arXiv:2502.17926 (2025).\\n[84] Yuqin Jiang, Zhenlong Li, and Xinyue Ye. 2019. Understanding demographic\\nand socioeconomic biases of geotagged Twitter users at the county level. Car-\\ntography and geographic information science 46, 3 (2019), 228–242.\\n[85] Xianlin Jin and Patric R Spence. 2023. Check crisis information on Twitter:\\nInformation flow and crisis communication patterns of hurricane ida. Commu-\\nnication Studies 74, 4 (2023), 337–355.\\n[86] W Matt Jolly, Mark A Cochrane, Patrick H Freeborn, Zachary A Holden, Tim-\\nothy J Brown, Grant J Williamson, and David MJS Bowman. 2015. Climate-\\ninduced variations in global wildfire danger from 1979 to 2013. Nature commu-\\nnications 6, 1 (2015), 7537.\\n[87] Joseph Karanja, Jennifer Vanos, Ankit Joshi, Scott Penner, Gisel E Guzman,\\nDylan S Connor, and Konrad Rykaczewski. 2024. Impact of tent shade on heat\\nexposures and simulated heat strain for people experiencing homelessness.\\nInternational journal of biometeorology (2024), 1–14.\\n[88] Shahab Kareem, Zhala Jameel Hamad, and Shavan Askar. 2021. An evaluation\\nof CNN and ANN in prediction weather forecasting: A review. Sustainable\\nEngineering and Innovation 3, 2 (2021), 148–159.\\n[89] Wael Khallouli. 2024. Harnessing Social Media for Disaster Response: Intelligent\\nIdentification of Reliable Rescue Requests During Hurricanes. Ph. D. Dissertation.\\nOld Dominion University.\\n[90] Jongsung Kim, Myungjin Lee, Heechan Han, Donghyun Kim, Yunghye Bae, and\\nHung Soo Kim. 2022. Case study: Development of the CNN model considering\\nteleconnection for spatial downscaling of precipitation in a climate change\\nscenario. Sustainability 14, 8 (2022), 4719.\\n[91] Mirae Kim. 2024. Evaluating English Fluency Effect on the Delivery of Water\\nCampaign Messages to Residents in Metro Vancouver. Master’s thesis. Royal\\nRoads University (Canada).\\n[92] Soo Yun Kim. 2022. Understanding Attitude Extremity in Climate Change: Public\\nPerceptions of Climate Change Deniers and Climate Change Doomsday Believers.\\nThe University of Wisconsin-Madison.\\n[93] Svetlana Kiritchenko and Saif M Mohammad. 2018. Examining gender and race\\nbias in two hundred sentiment analysis systems. arXiv preprint arXiv:1805.04508\\n(2018).\\n[94] Maria Klariza Madrazo, Huikyo Lee, Arezoo Khodayari, Weile Wang, Taejin\\nPark, and Colin Raymond. 2023. The impact of climate change on fire dan-\\nger over the contiguous United States. In EGU General Assembly Conference\\nAbstracts. EGU–16986.\\n[95] Jessie WY Ko, Shengquan Ni, Alexander Taylor, Xiusi Chen, Yicong Huang,\\nAvinash Kumar, Sadeem Alsudais, Zuozhi Wang, Xiaozhen Liu, Wei Wang, et al.\\n2024. How the experience of California wildfires shape Twitter climate change\\nframings. Climatic Change 177, 1 (2024), 17.\\n[96] Vanessa Kokoschka, Cristian A Secco, and Kawa Nazemi. 2024. Visual Analytics-\\nClimate Change in Social Media. In 2024 28th International Conference Informa-\\ntion Visualisation (IV). IEEE, 167–173.\\n[97] Julian F Kölbel, Markus Leippold, Jordy Rillaerts, and Qian Wang. 2024. Ask\\nBERT: How regulatory disclosure of transition and physical climate risks affects\\nthe CDS term structure. Journal of Financial Econometrics 22, 1 (2024), 30–69.\\n[98] Jan Ole Krugmann and Jochen Hartmann. 2024. Sentiment analysis in the age\\nof generative AI. Customer Needs and Solutions 11, 1 (2024), 3.\\n[99] Yury Kryvasheyeu, Haohui Chen, Nick Obradovich, Esteban Moro, Pascal\\nVan Hentenryck, James Fowler, and Manuel Cebrian. 2016. Rapid assessment\\nof disaster damage using social media activity. Science advances 2, 3 (2016),\\ne1500779.\\n[100] Batool Lakzaei, Mostafa Haghir Chehreghani, and Alireza Bagheri. 2025. A\\nDecision-Based Heterogenous Graph Attention Network for Multi-Class Fake\\nNews Detection. arXiv preprint arXiv:2501.03290 (2025).\\n[101] Batool Lakzaei, Mostafa Haghir Chehreghani, and Alireza Bagheri. 2025.\\nNeighborhood-Order Learning Graph Attention Network for Fake News Detec-\\ntion. arXiv preprint arXiv:2502.06927 (2025).\\n[102] Batool Lakzaei, Mostafa Haghir Chehreghani, and Alireza Bagheri. 2024. Disin-\\nformation detection using graph neural networks: a survey. Artificial Intelligence\\nReview 57, 3 (2024), 52.\\n[103] Jess Lasoff-Santos. 2024. Supporting Community and Psychological Resilience to\\nthe Climate Crisis. Ph. D. Dissertation.\\n[104] Jake Lever, Sibo Cheng, and Rossella Arcucci. 2023. Human-sensors & physics\\naware machine learning for wildfire detection and nowcasting. In International\\nConference on Computational Science. Springer, 422–429.\\n[105] Jake Lever, Sibo Cheng, and Rossella Arcucci. 2023. Social & Physics Based Data\\nDriven Methods for Wildfire Prediction. In EGU General Assembly Conference\\nAbstracts. EGU–15645.\\n[106] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao,\\nZhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu,\\net al. 2024. From generation to judgment: Opportunities and challenges of\\nllm-as-a-judge. arXiv preprint arXiv:2411.16594 (2024).\\n[107] Haobo Li, Zhaowei Wang, Jiachen Wang, Alexis Kai Hon Lau, and Huamin Qu.\\n2024. Cllmate: A multimodal llm for weather and climate events forecasting.\\narXiv preprint arXiv:2409.19058 (2024).\\n[108] Qiuyan Liao, Yucan Xu, Sijia Li, et al. 2024. Decoding public’s real-time emo-\\ntional and cognitive responses to the changing climate on social media. (2024).\\n[109] Bing Liu. 2022. Sentiment analysis and opinion mining. Springer Nature.\\n[110] Bing Liu et al. 2010. Sentiment analysis and subjectivity. Handbook of natural\\nlanguage processing 2, 2010 (2010), 627–666.\\n[111] Chuchu Liu, Fan Fang, Xu Lin, Tie Cai, Xu Tan, Jianguo Liu, and Xin Lu. 2021.\\nImproving sentiment analysis accuracy with emoji embedding. Journal of Safety\\nScience and Resilience 2, 4 (2021), 246–252.\\n[112] Qiaoyi Liu, Yuheun Kim, and Jeff Hemsley. 2025. Scientists, but deny science?\\nClimate change sceptics networks on YouTube led by scientists. Information\\nResearch an international electronic journal 30, iConf (2025), 741–751.\\n[113] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,\\nOmer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta:\\nA robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\\n(2019).\\n[114] Yongcong Luo. 2024. Research on the Sentiment Analysis and Evolutionary\\nMechanism of Sudden Network Public Opinion Based on SNA-ARIMA Model\\nwith Text Mining. J. Electrical Systems 20, 2 (2024), 1961–1972.\\n[115] Ridha Mahmood, Philippa Clery, Justin Yang, Liying Cao, Jennifer Dykxhoorn,\\nand Jennifer Dykxhoorn. [n. d.]. The impact of climate change on mental health\\nin vulnerable groups: a systematic review. ([n. d.]).\\n[116] Judith Mair. 2011. Events and climate change: an Australian perspective. Inter-\\nnational Journal of Event and Festival Management 2, 3 (2011), 245–253.\\n[117] Maria Mäkelä. 2024. Climate uncertainty, social media certainty: A story-critical\\napproach to climate change storytelling on social media. Frontiers of Narrative\\nStudies 9, 2 (2024), 232–253.\\n[118] Sunil Malviya, Arvind Kumar Tiwari, Rajeev Srivastava, and Vipin Tiwari. 2020.\\nMachine learning techniques for sentiment analysis: A review. SAMRIDDHI: A\\nJournal of Physical Sciences, Engineering and Technology 12, 02 (2020), 72–78.\\n[119] Francisco S Marcondes, Adelino Gala, Renata Magalhães, Fernando Perez de\\nBritto, Dalila Durães, and Paulo Novais. 2025. Case Study: LLM-Based Anxiety\\nClimate Index. In Natural Language Analytics with Generative Large-Language\\nModels: A Practical Approach with Ollama and Open-Source LLMs. Springer,\\n53–73.\\n[120] Aleksandrina V Mavrodieva, Okky K Rachman, Vito B Harahap, and Rajib Shaw.\\n2019. Role of social media as a soft power tool in raising public awareness and\\nengagement in addressing climate change. Climate 7, 10 (2019), 122.\\n[121] Dorothea Metzen and Sebastian Ocklenburg. 2024. The Psychology and Neuro-\\nscience of the Climate Crisis. Springer.\\n[122] Md Saef Ullah Miah, Md Mohsin Kabir, Talha Bin Sarwar, Mejdl Safran, Sultan\\nAlfarhood, and MF Mridha. 2024. A multimodal approach to cross-lingual\\nsentiment analysis with ensemble of transformer and LLM. Scientific Reports\\n14, 1 (2024), 9603.\\n[123] Emmanouil Michail, Aristeidis Bozas, Dimitrios Stefanopoulos, Stavros Pas-\\npalakis, Georgios Orfanidis, Anastasia Moumtzidou, Ilias Gialampoukidis, Kon-\\nstantinos Ioannidis, Stefanos Vrochidis, and Ioannis Kompatsiaris. 2024. Incor-\\nporating Social Media Sensing and Computer Vision Technologies to Support\\nWildfire Monitoring. In IGARSS 2024-2024 IEEE International Geoscience and\\nRemote Sensing Symposium. IEEE, 2082–2085.\\n[124] Ariane Middel, Benjamin Bechtel, Matthias Demuzere, and Negin Nazarian.\\n2023. Urban climate informatics. Frontiers Media SA.\\n[125] Ariane Middel and E Scott Krayenhoff. 2019. Micrometeorological determinants\\nof pedestrian thermal exposure during record-breaking heat in Tempe, Arizona:\\nIntroducing the MaRTy observational platform. Science of the total environment\\n687 (2019), 137–151.\\n[126] Marie-Gabrielle Mocatta and Erin Hawley. 2020. Uncovering a climate catastro-\\nphe? Media coverage of Australia’s Black Summer bushfires and the revelatory\\nextent of the climate blame frame. (2020).\\n[127] Saif M Mohammad and Peter D Turney. 2013. Nrc emotion lexicon. National\\nResearch Council, Canada 2 (2013), 234.\\n[128] Mohasina Mohasina. 2023. RoBERTa: a machine reading comprehension for\\nclimate change question answering in natural language processing. Ph. D. Disser-\\ntation.\\n[129] MO Molina, C Gutiérrez, M Ortega, and E Sánchez. 2023. Summer heatwaves,\\nwind production and electricity demand in Southern Europe: climatic conditions\\nand impacts. Environmental Research Communications 5, 8 (2023), 085005.\\n[130] Abdel Rauf Mostafa, Allaa M Owes, and Shimaa Ghoniem. 2025. Interconnected\\nImpacts of Climate Change on Biodiversity, Agriculture, and Human Health.\\nAdvances in Basic and Applied Sciences (2025).\\n[131] Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, and Dongwon Lee.\\n2024. Fakes of varying shades: How warning affects human perception and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 11}, page_content='Shaeri and Mohammadpour, et al.\\nengagement regarding LLM hallucinations. arXiv preprint arXiv:2404.03745\\n(2024).\\n[132] Samuel Chukwujindu Nwokolo. 2025. Climate Hoax: The Shift from Scientific\\nDiscourse to Speculative Rhetoric in Climate Change Conversations. Next\\nResearch (2025), 100322.\\n[133] Alexandra Olteanu, Carlos Castillo, Nicholas Diakopoulos, and Karl Aberer.\\n2015. Comparing events coverage in online news and social media: The case of\\nclimate change. In Proceedings of the international AAAI conference on web and\\nsocial media, Vol. 9. 288–297.\\n[134] Nathaniel O’Grady. 2025. (Re) framing the politics of climate change: resilience,\\nreparative critique and affective life in situations of extreme heat. Critical\\nStudies on Security 13, 1 (2025), 70–86.\\n[135] Ananya Pandey and Dinesh Kumar Vishwakarma. 2024. Progress, achievements,\\nand challenges in multimodal sentiment analysis using deep learning: A survey.\\nApplied Soft Computing 152 (2024), 111206.\\n[136] Warren Pearce, Kim Holmberg, Iina Hellsten, and Brigitte Nerlich. 2014. Climate\\nchange on Twitter: Topics, communities and conversations about the 2013 IPCC\\nWorking Group 1 report. PloS one 9, 4 (2014), e94785.\\n[137] Edouard Pignot, Hajer Kéfi, and Mark Thompson. 2023. Affective Circulation via\\nSocial Media: Examining Climate Change as an Example. In IFIP Joint Working\\nConference on the Future of Digital Work: The Challenge of Inequality. Springer,\\n21–27.\\n[138] Protik Bose Pranto. 2024. Satire or Fake News? Machine Learning-Based Ap-\\nproaches to Resolve the Dilemma. In 2024 4th International Conference on Elec-\\ntrical, Computer, Communications and Mechatronics Engineering (ICECCME).\\nIEEE, 1–6.\\n[139] Protik Bose Pranto, Waqar Hassan Khan, Sahar Abdelnabi, Rebecca Weil, Mario\\nFritz, and Rakibul Hasan. 2023. From Bad to Worse: Using Private Data to\\nPropagate Disinformation on Online Platforms with a Greater Efficiency. arXiv\\npreprint arXiv:2306.04883 (2023).\\n[140] Yashaswi Pupneja, Joseph Zou, Sacha Lévy, and Shenyang Huang. 2023. Under-\\nstanding Opinions Towards Climate Change on Social Media. arXiv preprint\\narXiv:2312.01217 (2023).\\n[141] Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2018.\\nTowards empathetic open-domain conversation models: A new benchmark and\\ndataset. arXiv preprint arXiv:1811.00207 (2018).\\n[142] Suny Sadik, Jacob Benedetti, and Swapna S Gokhale. 2022. Analyzing Climate\\nChange Dialogue During California Wildfires. In 2022 2nd Asian Conference on\\nInnovation in Technology (ASIANCON). IEEE, 1–8.\\n[143] Hajar Sakai, Sarah S Lam, Mohammadsadegh Mikaeili, and Joshua Bosire. 2024.\\nPatient Experience Feedback Sentiment Analysis: Combining BERT and LSTM\\nwith Genetic Algorithm Optimization. In IISE Annual Conference. Proceedings.\\nInstitute of Industrial and Systems Engineers (IISE), 1–6.\\n[144] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distil-\\nBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv\\npreprint arXiv:1910.01108 (2019).\\n[145] Srikanta Sannigrahi, Francesco Pilla, Arabinda Maiti, Somnath Bar, Sandeep\\nBhatt, Qi Zhang, Saskia Keesstra, Artemi Cerda, et al. 2022. Examining the\\nstatus of forest fire emission in 2020 and its connection to COVID-19 incidents\\nin West Coast regions of the United States. Environmental Research 210 (2022),\\n112818.\\n[146] Septien Dwi Savandha, Amelia Amelia, and Ginna Novarianti Dwi Putri Pra-\\nmesti. 2025. From Headlines to Public Awareness: A Media Discourse Analysis\\nof The Los Angeles 2025 Wildfire. WINTER JOURNAL: IMWI STUDENT RE-\\nSEARCH JOURNAL 6, 1 (2025), 43–54.\\n[147] Ali Sayigh. 2023. Global Catastrophe: Climate Change Is Happening Now,\\nRenewable Energy Can Reduce Its Impact. In Mediterranean Architecture and\\nthe Green-Digital Transition: Selected Papers from the World Renewable Energy\\nCongress Med Green Forum 2022. Springer, 291–312.\\n[148] Tobias Schimanski, Julia Bingler, Camilla Hyslop, Mathias Kraus, and Markus\\nLeippold. 2023. ClimateBERT-NetZero: Detecting and assessing net zero and\\nreduction targets. arXiv preprint arXiv:2310.08096 (2023).\\n[149] Florian A Schneider, Erin Epel, and Ariane Middel. 2024. A disconnect in science\\nand practitioner perspectives on heat mitigation. npj Urban Sustainability 4, 1\\n(2024), 17.\\n[150] Samuel Sellers, Kristie L Ebi, and Jeremy Hess. 2019. Climate change, human\\nhealth, and social stability: addressing interlinkages. Environmental health\\nperspectives 127, 04 (2019), 045002.\\n[151] Sonia Seneviratne, Michael Windisch, Bianca Biess, Felix Jaeger, Lukas Gud-\\nmundsson, Mathias Hauser, Laibao Liu, Quilcaille Yann, Schwaab Jonas, and\\nSieber Petra. 2024. Extreme events and land use changes in the climate crisis.\\nTechnical Report. Copernicus Meetings.\\n[152] Quinta Seon, Natalie Greaves, Michael Campbell, Simon Anderson, Paula Henry,\\nEden Augustus, Emanuel Cummings, Leann Kendall, Erica Wheeler, Ans Ver-\\ncammen, et al. 2024.\\nExploratory empirical model of combined effects of\\nCOVID-19 and climate change on youth mental health. Nature Mental Health 2,\\n2 (2024), 218–227.\\n[153] Pouya Shaeri, Saud AlKhaled, and Ariane Middel. 2025. A Multimodal Physics-\\nInformed Neural Network Approach for Mean Radiant Temperature Modeling.\\narXiv:2503.08482 [cs.CV] https://arxiv.org/abs/2503.08482\\n[154] Pouya Shaeri and Ali Katanforoush. 2023. A semi-supervised fake news de-\\ntection using sentiment encoding and lstm with self-attention. In 2023 13th\\nInternational Conference on Computer and Knowledge Engineering (ICCKE). IEEE,\\n590–595.\\n[155] Maryam Shafiee Shakib, Patricia Solís, and Kate Varfalameyeva. 2024. Mapswipe\\nfor SDGs 3 & 13: take urgent cartographic action to combat heat vulnerabil-\\nity of manufactured and mobile home communities. International Journal of\\nCartography (2024), 1–23.\\n[156] Shan Shan. 2024.\\nFrom Correlation to Causation: Understanding Climate\\nChange through Causal Analysis and LLM Interpretations. arXiv preprint\\narXiv:2412.16691 (2024).\\n[157] ShelterBox USA. 2025. Los Angeles Wildfires: In Solidarity with Displaced Families.\\nhttps://www.shelterboxusa.org/los-angeles-wildfires-update/ Accessed: 2025-\\n04-24.\\n[158] Valeriy Shevchenko, Aleksandr Lukashevich, Daria Taniushkina, Alexander\\nBulkin, Roland Grinis, Kirill Kovalev, Veronika Narozhnaia, Nazar Sotiriadi,\\nAlexander Krenke, and Yury Maximov. 2024. Climate change impact on agri-\\ncultural land suitability: An interpretable machine learning-based Eurasia case\\nstudy. IEEE Access 12 (2024), 15748–15763.\\n[159] Zhiying Shi. 2018. Impact of climate change on the global environment and\\nassociated human health. Open Access Library Journal 5, 10 (2018), 1–6.\\n[160] Mohammed Shiha and Serkan Ayvaz. 2017. The effects of emoji in sentiment\\nanalysis. Int. J. Comput. Electr. Eng.(IJCEE.) 9, 1 (2017), 360–369.\\n[161] Yerik Afrianto Singgalen. 2024. Sentiment Classification of Climate Change and\\nTourism Content Using Support Vector Machine. J. Comput. Syst. Informatics 5,\\n2 (2024), 357–367.\\n[162] EA Singh, MR Shindikar, et al. 2023. A comprehensive review on climate change\\nand its effects. (2023).\\n[163] Upendra Singh, Kumar Abhishek, and Hiteshwar Kumar Azad. 2024. A survey\\nof cutting-edge multimodal sentiment analysis. Comput. Surveys 56, 9 (2024),\\n1–38.\\n[164] Ralf C Staudemeyer and Eric Rothstein Morris. 2019. Understanding LSTM–a\\ntutorial into long short-term memory recurrent neural networks. arXiv preprint\\narXiv:1909.09586 (2019).\\n[165] Bebe Chand Sultana, Md Tabiur Rahman Prodhan, Edris Alam, Md Salman\\nSohel, ABM Mainul Bari, Subodh Chandra Pal, Md Kamrul Islam, and Abu Reza\\nMd Towfiqul Islam. 2024. A systematic review of the nexus between climate\\nchange and social media: present status, trends, and future challenges. Frontiers\\nin Communication 9 (2024), 1301400.\\n[166] Yuqing Sun, Gensuo Jia, and Xiyan Xu. 2025. Extreme high temperatures and\\nheatwave events across Europe in 2023. Environmental Research Communica-\\ntions (2025).\\n[167] Pushpika Sundarreson and Sapna Kumarapathirage. 2024. SentiGEN: Synthetic\\nData Generator for Sentiment Analysis. Journal of Computing Theories and\\nApplications 1, 4 (2024), 461–477.\\n[168] Dimitris Sykas, Dimitrios Zografakis, Konstantinos Demestichas, Constantina\\nCostopoulou, and Pavlos Kosmidis. 2023. EO4WildFires: An Earth observation\\nmulti-sensor, time-series machine-learning-ready benchmark dataset for wild-\\nfire impact prediction. In Ninth International Conference on Remote Sensing and\\nGeoinformation of the Environment (RSCy2023), Vol. 12786. SPIE, 11–20.\\n[169] Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita\\nBhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024.\\nLarge language models for data annotation and synthesis: A survey. arXiv\\npreprint arXiv:2402.13446 (2024).\\n[170] Horus Cleopatra Tano Kania. [n. d.]. Enhancing E-commerce Customer Insights:\\nSentiment Analysis, Causal Reasoning, and LLM Benchmarking. ([n. d.]).\\n[171] Kush Thakar, Rajesh Patel, and Gaurav Patel. 2021. Techno-economic analysis\\nof district cooling system: A case study. Journal of Cleaner Production 313 (2021),\\n127812.\\n[172] Myriam V Thoma, Nicolas Rohleder, and Shauna L Rohner. 2021. Clinical\\necopsychology: The mental health impacts and underlying pathways of the\\nclimate and environmental crisis. Frontiers in psychiatry 12 (2021), 675936.\\n[173] Adelle Thomas and William WL Cheung. [n. d.]. IMPACTS OF CLIMATE\\nCHANGE. ([n. d.]).\\n[174] V Kelly Turner, Emma M French, John Dialesandro, Ariane Middel, David\\nM Hondula, George Ban Weiss, and Hana Abdellati. 2022. How are cities\\nplanning for heat? Analysis of United States municipal plans. Environmental\\nresearch letters 17, 6 (2022), 064054.\\n[175] Samson Ebenezar Uthirapathy and Domnic Sandanam. 2023. Topic Modelling\\nand Opinion Analysis On Climate Change Twitter Data Using LDA And BERT\\nModel. Procedia Computer Science 218 (2023), 908–917.\\n[176] Roopal Vaid, Kartikey Pant, and Manish Shrivastava. 2022. Towards fine-grained\\nclassification of climate change related social media text. In Proceedings of the\\n60th annual meeting of the association for computational linguistics: Student\\nresearch workshop. 434–443.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-05-08T00:30:21+00:00', 'source': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2504.18837v3.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-08T00:30:21+00:00', 'trapped': '', 'modDate': 'D:20250508003021Z', 'creationDate': 'D:20250508003021Z', 'page': 12}, page_content='Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events\\n[177] A. Valdivia, M.V. Luzon, E. Cambria, et al. 2018. Consensus vote models for\\ndetecting and filtering neutrality in sentiment analysis. Information Fusion 44\\n(2018), 126–135.\\n[178] Maryline Vivion, Valérie Trottier, Ève Bouhêlier, Isabelle Goupil-Sormany,\\nThierno Diallo, et al. 2024. Misinformation About Climate Change and Re-\\nlated Environmental Events on Social Media: Protocol for a Scoping Review.\\nJMIR Research Protocols 13, 1 (2024), e59345.\\n[179] Jim Waldo and Soline Boussard. 2024. GPTs and Hallucination: Why do large\\nlanguage models hallucinate? Queue 22, 4 (2024), 19–33.\\n[180] Yan Wang and Caiyang Peng. 2024. Public perceptions of climate change\\nduring the COVID-19 crisis: Evidence from social media data in China. Current\\nSociology (2024), 00113921241248448.\\n[181] Helena Webb, Marina Jirotka, Bernd Carsten Stahl, William Housley, Adam\\nEdwards, Matthew Williams, Rob Procter, Omer Rana, and Pete Burnap. 2016.\\nDigital wildfires: hyper-connectivity, havoc and a global ethos to govern social\\nmedia. ACM SIGCAS Computers and Society 45, 3 (2016), 193–201.\\n[182] Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, and Markus Leippold.\\n2021. Climatebert: A pretrained language model for climate-related text. arXiv\\npreprint arXiv:2110.12010 (2021).\\n[183] Raquel Winker, Alexis Payton, Eric Brown, Elena McDermott, Jonathan H Freed-\\nman, Chris Lenhardt, Lauren A Eaves, Rebecca C Fry, and Julia E Rager. 2024.\\nWildfires and climate justice: future wildfire events predicted to dispropor-\\ntionally impact socioeconomically vulnerable communities in North Carolina.\\nFrontiers in public health 12 (2024), 1339700.\\n[184] Orison O Woolcott. 2025. Los Angeles County in flames: responsibilities on fire.\\nThe Lancet Regional Health–Americas 42 (2025).\\n[185] Selim F Yilmaz, E Batuhan Kaynak, Aykut Koç, Hamdi Dibeklioğlu, and Suley-\\nman Serdar Kozat. 2021. Multi-label sentiment analysis on 100 languages with\\ndynamic weighting for label imbalance. IEEE Transactions on Neural Networks\\nand Learning Systems (2021).\\n[186] Abraham Yosipof, Or Elroy, and Nadejda Komendantova. 2024. Cyber-Echoes\\nof Climate Crisis: Unraveling Anthropogenic Climate Change Narratives on\\nSocial Media. In EGU General Assembly Conference Abstracts. 16779.\\n[187] Douglas C Youvan. 2025. Facing the Flame: The Hidden Vulnerabilities of Los\\nAngeles in the Era of Wildfire and Global Conflict. (2025).\\n[188] Gorka Zamarreño-Aramendia, FJ Cristòfol, Jordi de San-Eugenio-Vela, and\\nXavier Ginesta. 2020. Social-media analysis for disaster prevention: Forest\\nfire in Artenara and Valleseco, Canary Islands. Journal of Open Innovation:\\nTechnology, Market, and Complexity 6, 4 (2020), 169.\\n[189] Muhammad Rifki Adinur Zein, Kurnia Lucky Fadillah, Nadia Febriani, Riki\\nNasrullah, and Nguyen Tan Khang. 2024. Social media use for climate change\\ncampaign among Indonesian millennials. PRofesi Humas 8, 2 (2024), 168–194.\\n[190] Chuanjun Zhao, Meiling Wu, Xinyi Yang, Wenyue Zhang, Shaoxia Zhang, Suge\\nWang, and Deyu Li. 2024. A systematic review of cross-lingual sentiment\\nanalysis: Tasks, strategies, and prospects. Comput. Surveys 56, 7 (2024), 1–37.\\n[191] Sicheng Zhao, Xiaopeng Hong, Jufeng Yang, Yanyan Zhao, and Guiguang Ding.\\n2023. Toward label-efficient emotion and sentiment analysis. Proc. IEEE 111, 10\\n(2023), 1159–1197.\\n[192] Yiyin Zhou, Yanrong Hu, Hongjiu Liu, Ping Huang, and Dan Dai. 2024. Linking\\nnegative herding effect to online public opinion: A new BiGADG approach for\\nsentiment classification. Expert Systems 41, 8 (2024), e13587.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 0}, page_content='Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n1 \\nAnalyzing Public opinion on Climate Change via Twitter: A \\nMachine Learning Approach Using Historical Data \\nMahmoud Mahdi a, Ahmad Salahb, Mohamed Omara   \\naDepartment of Computer Science, Faculty of Computers and Informatics, Zagazig University, Zagazig, Egypt \\n bCollege of Computing and Information Sciences, University of Technology and Applied Science, Ibri, Oman \\n \\n \\n*Corresponding Author: Ahmad Salah  [ahmad.salah@utas.edu.om] \\n \\n \\n \\n \\nARTICLE DATA \\nABSTRACT \\nArticle history: \\nReceived 26 April  2025 \\nRevised 17  May 2025 \\nAccepted 18  May 2025 \\nAvailable online 30 July 2025 \\nExamining public perspective regarding critical issues such as climate change in the context of \\na vast social media stream necessitates a computational approach. This study provides an initial \\nbenchmark of five classical machine learning classifiers (Multinomial Naive Bayes, Logistic \\nRegression, Linear Support Vector classifier (SVM), Random Forest, Gradient Boosting) on \\nmulti-class categorization (Anti, Neutral, Pro, News) using the openly available Twitter Climate \\nChange Sentiment dataset (2015-2018). Models were built using Term Frequency-Inverse\\nDocument Frequency (TF-IDF) for feature representation and performance was assessed using \\nstandard metrics (Accuracy, F1-score), as well as modeling generalizability by comparing \\ntraining versus testing performance to identify overfitting. Experimental results showed that \\nLinear SVC achieved the highest test F1-score (~70.7) but exhibited significant overfitting\\n(≈28%), while Logistic Regression provided the best compromise producing a competitive F1-\\nscore (~67.8) and a notably higher degree of generalizability (≈11.5% drop in F1). Gradient \\nBoosting showed remarkable robustness with minimal overfitting (≈1.4% drop in F1) but had \\nless absolute performance (~58.3% F1). This study provides an important baseline for this \\nclassification task and highlights the importance of generalization in evaluating model \\nperformance in addition to predictions for reliable stance analysis, particularly in computational \\nsocial science. \\nKeywords: \\nClimate Change \\nTweets \\nMachine Learning \\nClassification  \\nSentiment Analysis \\n1. Introduction \\nClimate change represents one of the most fundamental and multifaceted challenges humanity faces \\ntoday in the 21st century - a challenge that will require global action powered by science, understanding by \\npolicymakers, and engagement of people from all walks of life [1]. Online social media, particularly \\nmicroblogging services such as Twitter (recently referred to as X), have emerged as powerful channels of \\npublic discourse pertaining to climate change, shaping public opinion, disseminating information (and \\nmisinformation), and galvanizing action [2, 3, 4]. The volume and velocity of user-generated content \\nflowing through these platforms provides a significant opportunity, while also presenting challenges to \\nunderstanding the dynamics relating to public sentiment, opinions, and narratives surrounding this \\nimportant issue [5]. Therefore, the development and use of robust computational methods based mainly in \\nNatural Language Processing (NLP) and Machine Learning (ML), to automate the analysis of public \\ndiscourse at scale, is important [5].  \\nThe recent innovations in NLP have shaped our ability to capture nuanced semantic meaning, sentiment \\npolarity, and argumentative opinion in short, informal texts that dominate social media platforms, largely, \\nbut not solely, featured in larger pre-trained transformer models and Large Language Models (LLMs) [7]. \\nAt the same time, user classification on a multilayered topic such as climate change is not simply a set of \\npositive-negative sentiment classes; a consideration of the user relationship with the scientific consensus \\n(pro-belief, anti-belief, or neutral to climate science, as an example), or type of post (factual news reporting) \\nis necessary [8]. Accurately detecting opinions in a multi-class context remains an ongoing area of study, \\nespecially with multiple sociolinguistic phenomena, such as sarcasm, evolving language, and echo \\nchambers [9,10].'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 1}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n101 \\n \\nCurated datasets are essential for advancing research in this area. In this study, we employ the publicly \\navailable \"Twitter Climate Change Sentiment Dataset\" [11], which consists of 43,943 tweets collected \\nbetween April 27, 2015, and February 21, 2018; each tweet examined in this dataset was related to climate \\nchange and each tweet was coded with high reliability as determined by consensus of three independent \\nreviewers. Tweets are categorized into four levels of support, labeling tweets as: News (factual news post), \\nPro (support of belief in manmade climate change), Neutral (neither supporting nor denying), or Anti (do \\nnot believe in manmade climate change). This dataset acts as a valuable historical snapshot of public \\ndiscourse in a pivotal time, and a benchmark for building and evaluating computational models to help \\nresearchers know what or how the public does or does not believe around climate change [12, 13, 14,15]. \\n \\nThis work will utilize this dataset to assess the state-of-the-art machine learning models performance for \\nclimate opinion classification as the problem will be classified as a classification problem. This research \\nutilizes the Twitter Climate Change Sentiment Dataset to benchmark several traditional classifier models, \\nincluding Naive Bayes [16], Logistic Regression [17], SVM [18], Random Forest [19], and Gradient \\nBoosting [20]. In order to conduct feature extraction, we utilize TF-IDF vectorization and evaluate \\nperformance using a well-defined evaluation framework, where all classifiers have the same standard of \\nevaluation. By implementing this procedural framework for evaluation of effectiveness of traditional \\nclassifiers, we intend to illustrate the performance of various classifiers in sentiment analysis, while \\nillustrating strengths and limitations. We also consider challenges presented in sentiment analysis tasks, \\nwhich include overfitting, class imbalance, and feature sparsity. \\n \\nThis paper adds to the growing climate change communication literature by: \\n \\n1. We report on the performance of classical machine learning models for sentiment classification \\ntasks on the Twitter Climate Change Sentiment Dataset. \\n \\n2. The results may be serving as a baseline for work to be compared, such as pre-trained models, \\ntransformers, and hybrid models. \\n \\nThe rest of the paper is organized as follows. Section 2 discusses the related work. Section 3 exposes \\nthe utilized methodology. Section 4 lists the results and discusses the findings of this work. Section 5 \\nconcludes the paper. \\n \\n2. Related Work \\nTechniques for sentiment analysis are extending beyond conventional product evaluations or social media, \\nto explore more nuanced dimensions of perspectives within particular contexts, including within \\nenvironmental policy. For example, in [21], the authors undertook a sentiment analysis on the word \\n\"change\" within qualitative interviews of local government officials in Canada discussing climate change \\naction plans and governance. The authors manually coded the data since they prioritized an accurate analysis \\nwithin the context of the qualitative data rather than rely on computational methods (even on their \\nqualitative data). They discovered \"overwhelmingly positive emotional\" as relating to institutional and/or \\nstrategic change, related to many factors such as leadership and planning features, and overwhelmingly \\nnegative emotional with respect to the slow or nonexistent pace of change, behavioral hurdles, and \\ncommunicative framing issues etc. In addition to providing a working example of sentiment analysis being \\napplied to gain insights from domain-specific qualitative data/attitudes, this also provides an example where \\nthe authors assert that sentiment analysis is important and be aware of the context (which is often a challenge \\nfor fully automated NLP systems with specialized corpora and nuanced concepts). Thus, it is a working use \\ncase of sentiment analysis as a tool for gaining insight from policy-related research to uncover drivers and \\nbarriers closely associated with specific keywords.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 2}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n102 \\n \\n \\nElaborating upon applications of sentiment analysis in the field of climate change, the authors in [22] \\nexamined perceptions held within Malaysia by carrying out an analysis of editorial articles from The Sun \\nDaily newspaper. The authors constructed a domain-specific corpus called the Malaysian Daily Climate \\nChange Corpus (MyDCCC) as part of a mixed-method approach which included utilizing Azure Machine \\nLearning that was completed as a preliminary first step of polarization by sentiment. A corpus-driven \\napproach using AntConc was then used to identify salient terms of sentiment lexicon. The words were cross-\\nreferenced against the MPQA Subjectivity Lexicon to confirm significant findings. The authors indicated \\nthat there was a considerable prevalence of negative sentiment (90%) established based on sentiments \\nrelated to climate change, identified via the editorials. The authors identified key negative salient terms that \\nincluded long, critical, and serious, whereas the positive terms were not the most prevalent and included \\nbetter, best, and hope. In [22], the author’s discourse analysis demonstrated that this situated negative \\nsentiment was fundamentally rooted in public frustration with governmental responses rather than denying \\nclimate change, thus the author suggested caution in interpreting sentiment beyond a binary value and in \\nisolation of a context that is greater than simple sentiment scores. This study combines automated sentiment \\nanalytic resources with the analysis of corpus linguistics and discourse with the goal of identifying particular \\nthoughts of public importance in a particular source of media. \\n \\nIn [23], the authors undertook a comparative analysis of lexicon-based, machine learning and hybrid \\nmethods of sentiment analysis of climate change Twitter data, utilizing seven popular sentiment lexicons \\n(e.g., VADER, TextBlob, MPQA) and three machine learning classifiers (Logistic Regression, SVM, Naïve \\nBayes) in combination with Bag-of-Words and TF-IDF feature extractions. They observed that hybrid \\nmethods performed significantly outperformed the lexical and machine learning methods independently. \\nThe best performing approach combined the TextBlob lexicon with Logistic Regression classifier using TF-\\nIDF features to provide the highest F1-score. They also noted that lemmatization was important because it \\ngenerally provided better performance for machine learning and hybrid methods. \\n \\nThe authors in [24] investigated public sentiment on Twitter concerning climate change issues related to \\nspecific Sustainable Development Goals (SDGs). They applied and compared several NLP techniques, \\nincluding the rule-based methods VADER and TextBlob, and a transfer-learning approach using BERT \\nembeddings with a logistic regression classifier. Their results demonstrated that the BERT-based model \\nachieved superior performance (69% accuracy) compared to the lexicon-based approaches for classifying \\nsentiment in the collected climate-related tweets. While the overall sentiment detected was positive, the \\nstudy highlighted challenges with data noise due to broad keyword selection and suggested domain-specific \\nmodels like BERTweet could offer further improvements. \\n \\nIn [25], the authors explored climate change and energy discourse on Twitter in the UK and Spain using \\nNLP methods in early-2019. Using the NRC Emotion Lexicon (EmoLex), they analyzed the data for \\npositive/negative sentiment as well as the presence of eight discrete emotions (i.e., fear, trust, anticipation) \\nwithin the tweets. The authors found the UK discourse to be less negative and to include more anticipation \\nthan the Spanish discourse, which was less negative overall and more dominated by fear. The authors used \\nsentiment towards specific energy generators (e.g., positive for renewables, negative for coal) in a \\ncorrelation with energy supply and public acceptance of energy sources — demonstrating the timeliness of \\ndiscourse/sentiment analysis using social media for considering emotion and sentiment, complementing \\ntraditional surveys.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 3}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n103 \\n \\n3. Proposed Methodology \\nThis section explains the methodology employed in this study for the sentiment classification of climate \\nchange-related tweets derived from the Twitter Climate Change Sentiment Dataset. Figure 1 illustrates the \\ncomplete workflow of the proposed methodology for processing text data, feature representation, and model \\nevaluation. The workflow begins with the Raw Dataset, which goes through Data Pre-processing such as \\ntext cleaning, tokenization, stopword removal, and lemmatization or stemming. The raw text is converted to \\nnumerical features in the Feature Extraction phase, which consists of a feature vectorization step and a \\nformalization of the vectorized into a Term Frequency-Inverse Document Frequency (TF-IDF) generated \\nfeature matrix. At this point, the pre-processed data is split between training and testing in the Train-Test \\nSplit phase, allowing model training on one subprocess, and model evaluation on another. The Model \\nTraining step refers to the machine learning model fit to the processed training data. Then the Model \\nEvaluation step refers to applying various performance metrics such as accuracy, precision, recall, and F1-\\nscore to confirm the capability/modeling of the classifiers summarized as the final output, performance \\nmetrics. The workflow depicted in this pipeline is structured, systematic, and replicable for the evaluation \\nof text data and classifier models.  \\nFIGURE 1. Workflow for Machine Learning-Based Text Classification.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 4}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n104 \\n \\n \\n \\n3.1 The utilized dataset \\nThe Twitter Climate Change Sentiment Dataset is a dataset of 43,943 annotated tweets that capture public \\nsentiment about climate change. Tweets were collected between April 27, 2015 and February 21, 2018, and \\nwere assigned sentiment categories based on the consensus of three independent annotators. All tweets that \\nreceived sentiment agreement from all three analysts were retained to ensure a high-quality dataset. The \\ndataset organizes tweets into four sentiment classes: (2) News, for tweets linking to factual news articles; \\n(1) Pro, for tweets representing support for the belief that climate change is man-made; (0) Neutral, for \\ntweets that express no explicit sentiment about climate change; and (-1) Anti, for tweets denying that climate \\nchange was man-made. The class distribution is imbalanced, with fewer tweets in the Neutral and Anti \\nclasses— models trained on imbalanced datasets have the potential to incorporate that imbalance into its \\npredictions when later evaluating or testing the models. Thus, we must deal with this class imbalance level \\nfairly during the model training. \\n \\n3.2. Data Preprocessing \\n      To prepare the data for machine learning, a few preprocessing steps were performed. This includes \\nmissing value treatment, duplicate removal, and mapping sentiment labels to more descriptive names. To \\nstart, rows containing missing text values in the message column were deleted due to the detrimental impact \\nof incomplete text data on the training of a model. Furthermore, duplicate tweets were removed from the \\ndataset so it would not bias the results based solely on duplicate tweet counts. After this cleaning step, \\nsentiment labels, which were originally denoted by numerical values (-1, 0, 1, 2), were mapped to more \\ndescriptive names (Anti, Neutral, Pro, and News) to further assist in data interpretation, analysis, and better \\nunderstanding of the data and sentiments expressed within the data itself.  \\n \\nBy this stage, the processed dataset was ready for a training and testing split, which was an 80-20 split. The \\ntraining set was then used to train the machine learning models and the testing set was then used to test \\ngeneralization capability of the models. In the interest of reproducibility, a random seed was assigned at the \\npoint of the training and testing split procedure. Figure 2 lists the main steps of the data proprocessing. \\n \\n3.3. Feature extraction \\n      The TF-IDF (Term Frequency-Inverse Document Frequency) approach was employed to convert textual \\ndata into a numerical representation that is amenable to machine learning. The TF-IDF approach is \\nappropriate for representing text data because it models the importance of terms in a document relative to \\nthe corpus.  With the TF-IDF approach, distinct terms, or terms that appear with low frequency across the \\ncorpus, are emphasized while common terms are down-rated. Distinct terms are generally more suited for \\nthe models to discover meaningful patterns in the text. \\n \\nThe TF-IDF Vectorizer was set to ignore, or remove, common English \"stop words,\" which do not provide \\ncontext to sentiment like \"the\" and \"and.\" In addition, any term, or word, that appeared in more than 70% of \\ntweets would also be removed (𝑚𝑎𝑥_𝑑𝑓= 0.7). Common words that are so frequent, or prevalent, among \\nall tweets aren\\'t particularly helpful for distinguishing the classes from one another. Overall, the output of \\nthe TF-IDF matrix would be a sparse representation of the original text data and focused on the importance \\nof terms (or words) found in each tweet. \\n \\nTF-IDF is a statistical measure used to evaluate the importance of a term in a document relative to a \\ncollection of documents (corpus). It is calculated as the product of two components: TF and IDF. The formula \\nfor TF-IDF is expressed in Equation 1 as follows:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 5}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n105 \\n \\n \\n𝑇𝐹−𝐼𝐷𝐹(𝑡, 𝑑, 𝐷) =  𝑇𝐹(𝑡, 𝑑) ×  𝐼𝐷𝐹(𝑡, 𝐷)             (1) \\n \\nwhere TF is calculated as 𝑇𝐹(𝑡, 𝑑) =  𝑓(𝑡, 𝑑) / 𝛴 𝑓(𝑡′, 𝑑), f(t,d) is the frequency of term 𝑡 in document 𝑑, \\nΣf(t′,d) is the total number of terms in document 𝑑 and 𝑡′ is any term in the document d. Also, IDF is \\ncalculated as  𝐼𝐷𝐹(𝑡, 𝐷) =  𝑙𝑜𝑔(|𝐷| / (1 + |{𝑑 ∈ 𝐷∶ 𝑡 ∈ 𝑑}|)) where ∣D∣ represents the total number \\nof documents in the corpus and ∣𝑑∈𝐷∶ 𝑡∈𝑑∣ is the number of documents containing term 𝑡. \\n \\n \\n \\nFIGURE 2. Pipeline for the data preprocessing. \\n \\n \\n3.4. Machine Learning Models \\nIn this study, the sentiment of tweets was classified through five machine learning models namely \\nMultinomial Naive Bayes, Logistic Regression, Linear Support Vector Machines (SVM), Random Forest, \\nand Gradient Boosting Classifier. Collectively, these models span traditional supervised learning options \\nand offer unique advantages to be deployed within the context of text classification.  \\n \\n1. The Multinomial Naive Bayes model assumes features are independent, resulting in a particularly \\nvaluable model for high-dimensional, sparse datasets as those generated through TF-IDF. The model \\nis also computationally efficient and frequently has good predictive performance for text \\nclassification use cases.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 6}, page_content=\"Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n106 \\n \\n2. As a linear model, Logistic Regression estimates class probabilities. The model is resistant to class \\nimbalance and outputs are interpretable, providing a viable classification model for binary or multi-\\nclass problems. \\n \\n3. In attempting to maximize the margin between classes, Linear Support Vector Machines (SVM) can \\nbe deployed as the model is known to perform well on high dimensional feature spaces, as those \\nproduced from TF-IDF. The SVM models are also known for their robustness and high predictive \\nperformance in text classification. \\n \\n4. Random Forest is an ensemble model which generates multiple decision trees either through \\nsampling dimensions or utilizing randomized decision-thresholds and then aggregates the \\npredictions. Though Random Forest is often useful in describing non-linear relationships in data, its \\nuse for high-dimensional datasets may leave much to be desired. \\n \\n5. Finally, Gradient Boosting Classifier is also an ensemble method. Like Random Forest, Gradient \\nBoosting builds sequential decision-trees, to optimize across misclassified samples at each tree-\\ngenerated stage. The model is computationally expensive; however, it is known to be an accurate \\npredictive model. \\n \\n3.5. Evaluation Metrics \\n \\nThe training and evaluation of the machine learning models was executed in a way that purposefully \\ncompared the models’ performance on the sentiment classification. Each model was trained on the TF-IDF-\\ntransformed training data which led to a set of predictions for the test set to understand how well each model \\ngeneralized. We evaluated each model by computing key performance metrics, including accuracy (Equation \\n2), precision (Equation 3), recall (Equation 4), and F1-score (Equation 5), all weighted to address the class \\nimbalance in the data. The accuracy, precision, recall and F1-scores provided meaningful means for \\ncapturing the capability of each model to correctly classify tweets across the four sentiment classes (Anti, \\nNeutral, Pro, and News). Additionally, confusion matrices were generated to examine common \\nmisclassifications for each model, providing information about class specific classification issues. We also \\ncompared training and testing F1 scores and flagged potential overfitting if there was a difference in the two \\nmeans of at least 0.10. Each model's evaluation results were reported and visualized for the evaluation \\ncomparison, systematically achieving the goal of comparing the strengths and weaknesses of the various \\nmodels. This comprehensive evaluation process assisted in the thorough examination of each models' \\nperformance, thereby providing useful information for their potential use to code and classify sentiment in \\ntweets about climate change prior discourse. \\n \\n𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦=\\n(𝑇𝑃+ 𝑇𝑁)\\n(𝑇𝑃+ 𝑇𝑁+ 𝐹𝑃+ 𝐹𝑁)                  (2)    \\nwhere TP (True Positives) is the number of instances where the model correctly predicts the positive class, \\nTN (True Negatives) is the number of instances where the model correctly predicts the negative class, FP \\n(False Positives) is the number of instances where the model incorrectly predicts the positive class for a \\nnegative instance, and FN (False Negatives) is the number of instances where the model incorrectly predicts \\nthe negative class for a positive instance. \\n \\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=\\n(𝑇𝑃)\\n(𝑇𝑃+ 𝐹𝑃)                  (3)\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 7}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n107 \\n \\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=\\n(𝑇𝑃)\\n(𝑇𝑃+ 𝐹𝑁)                  (4)    \\n \\n𝐹1 −𝑠𝑐𝑜𝑟𝑒= (𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛⋅𝑅𝑒𝑐𝑎𝑙𝑙)\\n(𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+ 𝑅𝑒𝑐𝑎𝑙𝑙)                  (5)    \\n \\n 4. Experimental Results  \\n4.1. Setup \\nThe proposed work was written in the Python programming script. The Twitter sentiment dataset1 was \\nintroduced and preprocessed utilizing pandas, including missing value removal and duplicate removal, \\nalongside sentiment label mapping. Feature extraction was performed using Term Frequency-Inverse \\nDocument Frequency (TF-IDF), implemented via the TfidfVectorizer from sklearn.feature_extraction.text, \\nconfigured to remove English stop words along with terms occurring in greater than 70% of tweets, after \\ndividing the data into an 80% training and 20% testing set formed via sklearn.model_selection. Five \\ndifferent classifiers included as part of sklearn (MultinomialNB, LogisticRegression, LinearSVC, \\nRandomForestClassifier, GradientBoostingClassifier) were trained and assessed via classification statistics \\n(accuracy, precision, recall, F1-score) from sklearn.metrics on the TF-IDF features, including visualization \\nin the form of a confusion matrix via matplotlib and seaborn. \\n \\n4.2. Results \\n \\nThe performance measures for the five evaluated machine learning models using both the training and \\ntesting datasets are presented in tabular form in Table 1, and a graphical representation through confusion \\nmatrix in Figures 3-7 for convenience and summarizing purposes. These activities provide a measure of not \\nonly how well each model learned patterns in the training data but more importantly, how well it generalized \\nto the unseen testing data, providing a measure of robustness. A meaningful difference in performance \\nbetween training and testing performance, including the measure of F1-score (defined here as 0.10), can \\nindicate overfitting, when the model has forgotten learning to simply state characteristics of the training \\ndataset. \\nThe training results for both Random Forest (99.99% F1-score) and Linear SVC (98.60% F1-score) \\nproduce superior performance levels indicating that they both learned patterns present within the training \\nset. However, their measure of generalization, based upon performance on unseen test data, gives a very \\ndifferent indication of performance learning, which is evident in the confusion matrix for the respective \\nmodels (Figure 4: Random Forest, Figure 5: Linear SVC). For Random Forest, the measure of F1-score \\ndropped sharply to 65.27% testing data (almost a 34.7% difference), which can be considered as a classical \\nand severe example of overfitting. For Linear SVC, it achieved the highest test F1-score of testing among \\nthe other models, which also measured an overfitting response, as the F1-score dropped almost 27.9% from \\ntraining performance, as seen in Table 1; performance, and compare with knowledge of misclassification \\nby testing in Figure 5: Linear SVC. Thus, while Linear SVC had the highest score for this actuality of the \\ntest dataset, the model demonstrates some reliability, albeit yielded no guarantees in generalization training \\ndata. \\n \\n \\n \\n \\n \\n                                                          \\n1 https://www.kaggle.com/datasets/edqian/twitter-climate-change-sentiment-dataset/data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 8}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n108 \\n \\nTABLE 1: Comparison of model performance metrics on training and test datasets. \\n \\nModel \\nAccuracy \\nPrecision \\nRecall \\nF1-score \\nTrain \\nTest \\nTrain \\nTest \\nTrain \\nTest \\nTrain \\nTest \\nNaive \\nBayes \\n64.75% \\n57.48% \\n78.25% \\n73.23% \\n64.75% \\n57.48% \\n58.07% \\n47.46% \\nLogistic \\nRegression \\n80.41% \\n69.89% \\n81.84% \\n70.00% \\n80.41% \\n69.89% \\n79.32% \\n67.82% \\nLinear \\nSVC \\n98.61% \\n71.61% \\n98.61% \\n70.77% \\n98.61% \\n71.61% \\n98.60% \\n70.67% \\nRandom \\nForest \\n99.99% \\n67.41% \\n99.99% \\n68.20% \\n99.99% \\n67.41% \\n99.99% \\n65.27% \\nGradient \\nBoosting \\n63.54% \\n61.73% \\n67.03% \\n63.79% \\n63.54% \\n61.73% \\n59.72% \\n58.31% \\nBaseline \\n[26] \\n- \\n- \\n- \\n83.00% \\n- \\n48.00% \\n- \\n50.00% \\nLogistic \\nRegression \\n[26] \\n- \\n- \\n- \\n67.00%  \\n- \\n74.00% \\n- \\n69.00% \\n \\n \\n \\nFIGURE 3. CM for the logistic regression model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 9}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n109 \\n \\n \\nFIGURE 4. CM for the random forest model. \\n \\n \\nFIGURE 5. CM for the linear SVC model. \\n \\n \\n \\n \\nFIGURE 6. CM for the Gradient Boosting model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 10}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n110 \\n \\n \\n \\nFIGURE 7. CM for the Naïve Bayes model. \\n4.3. Discussion \\n \\nIn contrast to the previously mentioned higher-performing models, both Naive Bayes and Gradient \\nBoosting exhibited lower performance levels in their respective model evaluations from the test set. As \\nindicated in Table 1 and observed in the confusion matrices presented in Figures 6 and 7, Naive Bayes had \\na test accuracy of 57.48% and an F1-score of 47.46%. The previous confusion checking discussion \\nbehaviorally does not have direct relevance to the study, but it is worth noting that the somewhat lower \\nperformance may be due to the core assumption of feature independence in class conditional probabilities \\nthat is often violated in more complex but real text data. Regarding Gradient Boosting model performance, \\na test accuracy of 61.73% and F1-score of 58.31% were found. While these values were lower compared to \\nLinear SVC and the Logistic Regression model, Gradient Boosting provides more than just the absolute \\nnumber presented in training performance; the model demonstrated excellent generalization, with training \\nF1-score of 59.72% and testing F1-score of approximately 58.31%, indicating only a 1.4% performance \\nscore drop from training to testing data. In other words, the Gradient Boosting model performed best among \\nall the models in relation to avoiding overfitting training data, and to provide some contextual, while the \\nabsolute predictive performance is limited on these performance figures and visualizations, as demonstrated \\nin Figure 6. The Absolute testing performance metric scores for the Gradient Boosting and Naive Bayes \\nmodels represented with Figures 6 and 7 both indicate that models applied with these configurations were \\nperforming \"lower,\" indicating that both models could have benefited from hyperparameter tuning or that, \\nsimply, the models may function alternatively weaker on the complexity of the data represented.  \\nBringing the data evidence together in one condensation that considers the differences in the varied \\nevaluation criteria is helpful in the context of future and current modelling. Strictly on understanding from \\nabsolute test performance, Linear SVC (Figure 5) was the highest performing classifying model. If the \\nmeasure of evaluation is more of an indicator of generalization (avoiding overfitting training data), Linear \\nSVC did poorly corresponding to a model published with equal paragraph efficiency or seeking \\ngeneralization and dropped even significantly lower than the model in the logistic regression with \\nsubstantial overfitting (See Figure 4). In practice, in this visualization construct that specifies combined \\nreliance on absolute test performance and generalization parameters of Logistic Regression (Figure 3) was \\nthe second-best performing modelling choice that included dependency on each test measure ultimately \\nproviding an acceptable balance between two evaluation criteria overall. The Gradient Boosting model \\n(Figure 6) demonstrates both favorable generalizations while did not have significant absolute test \\nperformance, while Naive Bayes showed little more than some worst absolute test performance while it'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 11}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n111 \\n \\nstarted to show overlap of slight overfitting (although still weakest test performance). So, while any of the \\nevaluations could be different relative to the perceived model \"best,\" if the evaluation is solely about \\nperceived maximum \"score\" on undiscovered unseen data, one could select Linear SVC (despite being \\nadvised not to consider overfitting). However, if absolute test performance is to be considered in addition \\nto maximum model robustness, either of the earlier mentioned selections are a balance, or the even eventual \\nsuperior generalizing power of data was Gradient Boosting classifier. \\nLinear models, like those used in the Linear SVC and the Logistic Regression model can be relatively \\neffective with high dimensional, sparse datasets generated from TF-IDF, as evidenced in Figures 3 and 5. \\nAlthough in this case, like most of the previous discussion, substantial use of overfitting at time was seen \\nin the Random Forest model especially with the last variable into functionality of machine learning. The \\nsubstantial overfitting of data seen especially from Random Forest (Figure 4) and equally concerning (if \\nnot concerning) in the Linear SVC (Figure 5) indicates how difficult it is to find the ultimate balance \\nbetween model complexity and overfitting. All machine learning training frameworks need to consider \\nregularization, or even careful hyperparameter tuning according to function. Thus, in this study either \\nimpression of previous combinations provides equal the most ideal and highest testing measures low scores \\nfor Linear SVC and Logistic Regression in combination on both measures such evaluation, while Logistic \\nRegression demonstrated greater reliability in practice due to the checks for overfitting. \\n \\n4.4. Limitations \\nWhile the proposed methodology performs favorably, it does exhibit minor weaknesses. First, as it relies \\non TF-IDF, it may not measure comprehensive semantic relationships or context of the text or text elements; \\nutilizing some advanced techniques, e.g. word embeddings (BERT), may help to improve the results of \\nmodel performance. Second, there was limited hyperparameter tuning, particularly for the models like \\nGradient Boosting, which could have improved model performance. Finally, stem, lemmatization and \\nhandling class imbalance during the preprocessing steps may have improved model performance as well. \\nIntegrating into these aspects would improve the robustness of the reports further. \\n \\n5. Conclusion \\nThe study explored the effectiveness of five established machine learning methods for categorizing \\npublic views on climate change as expressed in a historical Twitter dataset, using features extracted from \\nthe TF-IDF technique. The goal of the experiment was to produce performance baselines and assess the \\ninherent trade–off of accuracy vs. generalizing to testing data on a multi-class classification task. There was \\na large variation in the obtained performance scores. The Linear SVC classifier obtained the highest F1–\\nscore (≈70.7%) on the held-out test set indicating strong overall prediction performance on similar training \\ndata distributions. However, this also came with a great deal of overfitting (≈28% drop in F1 performance \\nfrom training to testing) raising questions about the model’s performance for prediction on truly new/unseen \\ndata. Gradient Boosting achieved very good generalization with a limited performance drop in the held-out \\ntest data (≈1.4% drop in F1-score) but also had the much lower absolute F1-score of (≈58.3%). Logistic \\nRegression presented as the most balanced mechanism providing the second highest F1–score performance \\n(≈67.8%), while also presenting with much more controlled overfit (≈11.5%) over the Linear SVC model. \\nrandom forest, like the SVC model, also severely overfit and naive bayes performed quite poorly across the \\nboard. The major conclusion drawn is that using the highest test score on its own as the criteria for selecting \\nany of the models for use in practice can lead to poor practical implementation in noisy social media text. \\nThe amount of overfitting seen in the highest models shows that the readability of the models is highly \\ndependent on generalization performance. Especially for work such as stance detection where robustness \\nand reliability are vital elements, the commitment to models with better perceived balance sacrifices some \\ntest performance for improved generalization - as seen for the Logistic Regression model for this analysis. \\nThis work established a necessary baseline performance benchmark for classical methodologies with \\ninformed research and use for the various classical approaches for future and technique advancements in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-08-10T11:57:37+03:00', 'source': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'file_path': '..\\\\data\\\\pdf\\\\JOCC-Volume 4-Issue 2- Page 100-112.pdf', 'total_pages': 13, 'format': 'PDF 1.5', 'title': 'gcc_breast_cancer.pdf', 'author': 'Dr. Ammar Mohammed', 'subject': '', 'keywords': '', 'moddate': '2025-08-10T11:57:37+03:00', 'trapped': '', 'modDate': \"D:20250810115737+03'00'\", 'creationDate': \"D:20250810115737+03'00'\", 'page': 12}, page_content='Mohammed Omar et al.                                       \\n          Journal of Computing and Communication  Vol.4  , No.2 , PP. 100-112  , 2025 \\n \\n112 \\n \\nareas of deep learning technique or contextual embeddings (formatter embeddings - BERT or others). There \\nis an opportunity for extended hyperparameter tuning to inform performance improvements and additional \\nclasses to be balanced/or equal representation levels to improve performance and reliability. Besides, the \\nFriedman test will be applied to the performance differences to test if the reported results are significant. \\n \\n \\nReferences \\n \\n[1] \\nChatterjee, Moumita, Piyush Kumar, and Dhrubasish Sarkar. \"A novel framework for analyzing climate change tweets from online social media \\nusing supervised and unsupervised algorithms.\" In 2024 IEEE Calcutta Conference (CALCON), pp. 1-5. IEEE, 2024. \\n[2] \\nMaynard D, Bontcheva K. Understanding climate change tweets: an open source toolkit for social media analysis. InEnviroInfo and ICT for \\nSustainability 2015 2015 Sep (pp. 242-250). Atlantis Press. \\n[3] \\nUpadhyaya, Apoorva, Marco Fisichella, and Wolfgang Nejdl. \"A multi-task model for sentiment aided stance detection of climate change tweets.\" \\nIn Proceedings of the international AAAI conference on web and social media, vol. 17, pp. 854-865. 2023. \\n[4] \\nToupin, Rémi, Florence Millerand, and Vincent Larivière. \"Who tweets climate change papers? Investigating publics of research through users’ \\ndescriptions.\" Plos one 17, no. 6 (2022): e0268999. \\n[5] \\nMumenthaler, Christian, O. Renaud, R. Gava, and Tobias Brosch. \"The impact of local temperature volatility on attention to climate change: \\nEvidence from Spanish tweets.\" Global environmental change 69 (2021): 102286. \\n[6] \\nFownes, Jennifer R., Chao Yu, and Drew B. Margolin. \"Twitter and climate change.\" Sociology Compass 12, no. 6 (2018): e12587. \\n[7] \\nMarcondes, Francisco S., Adelino Gala, Renata Magalhães, Fernando Perez de Britto, Dalila Durães, and Paulo Novais. \"Case Study: LLM-Based \\nAnxiety Climate Index.\" In Natural Language Analytics with Generative Large-Language Models: A Practical Approach with Ollama and Open-\\nSource LLMs, pp. 53-73. Cham: Springer Nature Switzerland, 2025. \\n[8] \\nDahal, Biraj, Sathish AP Kumar, and Zhenlong Li. \"Topic modeling and sentiment analysis of global climate change tweets.\" Social network \\nanalysis and mining 9 (2019): 1-20. \\n[9] \\nMaynard, Diana G., and Mark A. Greenwood. \"Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis.\" \\nIn Lrec 2014 proceedings. ELRA, 2014. \\n[10] Amendola, Miriam, Danilo Cavaliere, Carmen De Maio, Giuseppe Fenza, and Vincenzo Loia. \"Towards echo chamber assessment by employing \\naspect-based sentiment analysis and gdm consensus metrics.\" Online Social Networks and Media 39 (2024): 100276. \\n[11] Qian, Edward. 2015. “Twitter Climate Change Sentiment Dataset.” Kaggle.com. 2015. https://www.kaggle.com/datasets/edqian/twitter-climate-\\nchange-sentiment-dataset?resource=download. \\n[12] Mi, Zhewei, and Hongwei Zhan. \"Text mining attitudes toward climate change: Emotion and sentiment analysis of the twitter corpus.\" Weather, \\nClimate, and Society 15, no. 2 (2023): 277-287. \\n[13] Shyrokykh, Karina, Max Girnyk, and Lisa Dellmuth. \"Short text classification with machine learning in the social sciences: The case of climate \\nchange on Twitter.\" Plos one 18, no. 9 (2023): e0290762. \\n[14] Uthirapathy, Samson Ebenezar, and Domnic Sandanam. \"Topic Modelling and Opinion Analysis On Climate Change Twitter Data Using LDA \\nAnd BERT Model.\" Procedia Computer Science 218 (2023): 908-917. \\n[15] Kvasničková Stanislavská, Lucie, Ladislav Pilař, Xhesilda Vogli, Tomas Hlavsa, Kateřina Kuralová, Abby Feenstra, Lucie Pilařová, Richard \\nHartman, and Joanna Rosak-Szyrocka. \"Global analysis of Twitter communication in corporate social responsibility area: sustainability, climate \\nchange, and waste management.\" PeerJ Computer Science 9 (2023): e1390. \\n[16] Webb, Geoffrey I., Eamonn Keogh, and Risto Miikkulainen. \"Naïve Bayes.\" Encyclopedia of machine learning 15, no. 1 (2010): 713-714. \\n[17] LaValley, Michael P. \"Logistic regression.\" Circulation 117, no. 18 (2008): 2395-2399. \\n[18] Hearst, Marti A., Susan T. Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. \"Support vector machines.\" IEEE Intelligent Systems and \\ntheir applications 13, no. 4 (1998): 18-28. \\n[19] Rigatti, S.J., 2017. Random forest. Journal of Insurance Medicine, 47(1), pp.31-39. \\n[20] Natekin, Alexey, and Alois Knoll. \"Gradient boosting machines, a tutorial.\" Frontiers in neurorobotics 7 (2013): 21. \\n[21] Jost, François, Ann Dale, and Shoshana Schwebel. \"How positive is “change” in climate change? A sentiment analysis.\" Environmental Science \\n& Policy 96 (2019): 27-36. \\n[22] Taufek, Tasha Erina, Nor Fariza Mohd Nor, Azhar Jaludin, and Sabrina Tiun. \"Public Perceptions on Climate Change: A Sentiment Analysis \\nApproach.\" GEMA Online Journal of Language Studies 21, no. 4 (2021). \\n[23] Mohamad Sham, Nabila, and Azlinah Mohamed. \"Climate change sentiment analysis using lexicon, machine learning and hybrid \\napproaches.\" Sustainability 14, no. 8 (2022): 4723. \\n[24] Rosenberg, Emelie, Carlota Tarazona, Fermín Mallor, Hamidreza Eivazi, David Pastor-Escuredo, Francesco Fuso-Nerini, and Ricardo Vinuesa. \\n\"Sentiment analysis on Twitter data towards climate action.\" Results in Engineering 19 (2023): 101287. \\n[25] Loureiro, Maria L., and Maria Alló. \"Sensing climate change and energy issues: Sentiment and emotion analysis with social media in the \\nUK and Spain.\" Energy Policy 143 (2020): 111490. \\n[26] Klein, G., Kim, Y., Deng, Y., Senellart, J., & Rush, A. M. (2017). Opennmt: Open-source toolkit for neural machine translation. arXiv preprint \\narXiv:1701.02810.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 0}, page_content='DOI: 10.4018/JOEUC.294901\\nJournal of Organizational and End User Computing\\nVolume 34 • Issue 3 \\nThis article published as an Open Access article distributed under the terms of the Creative Commons Attribution License\\n(http://creativecommons.org/licenses/by/4.0/) which permits unrestricted use, distribution, and production in any medium,\\nprovided the author of the original work and original publication source are properly credited.\\n*Corresponding Author\\n1\\nTopic Modelling and Sentiment \\nAnalysis of Global Warming Tweets:\\nEvidence From Big Data Analysis\\nFang Qiao, Xi’an International Studies University, China*\\n https://orcid.org/0000-0002-1859-4308\\nJago Williams, Bangor University, UK\\nABSTRACT\\nWith the increasing extreme weather events and various disasters, people are paying more attention to \\nenvironmental issues than ever, particularly global warming. Public debate on it has grown on various \\nplatforms, including newspapers and social media. This paper examines the topics and sentiments of \\nthe discussion of global warming on Twitter over a span of 18 months using two big data analytics \\ntechniques: topic modelling and sentiment analysis. There are seven main topics concerning global \\nwarming frequently debated on Twitter: factors causing global warming, consequences of global \\nwarming, actions necessary to stop global warming, relations between global warming and COVID-19, \\nglobal warming’s relation with politics, global warming as a hoax, and global warming as a reality. The \\nsentiment analysis shows that most people express positive emotions about global warming, though \\nthe most evoked emotion found across the data is fear, followed by trust. The study provides a general \\nand critical view of the public’s principal concerns and their feelings about global warming on Twitter.\\nKeywords\\nBig Data, Global Warming, Sentiment Analysis, Topic Modelling, Twitter\\nINTRODUCTION\\nRecent events such as the hottest recorded temperature at the Antarctic pole, Australian wildfires, and \\neven the Covid 19 pandemic have often drawn people’s attention to environmental issues, especially \\nglobal warming. Voluminous research on global warming shows that extreme weather events and \\ndisasters, be they natural or anthropogenic, tend to spark discussion of global warming (Cody et al., \\n2015; Kirilenko et al., 2014; Molodtsova et al., 2013; Yeo et al., 2017). Acknowledging the magnitude \\nof global warming, many studies have examined factors contributing to it (Gunnemyr, 2019), its \\nimpacts (Brown et al., 2011), its connection with other environmental problems (Le Duff et al., \\n2020), and attention to it in newspapers (Schmidt et al., 2013). With the arrival of digital era, social \\nmedia such as Facebook, Instagram and Twitter have become essential platforms for information \\ndissemination and public debate (Segerberg & Bennett, 2011). Twitter, in particular, is seen as the \\nmost popular platform to share breaking news, individual experience and personal opinions about \\ncurrent events (Hermida, 2013; Mustaqim et al., 2020; Zhao et al., 2011). Academic attention has'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 1}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n2\\nturned to Twitter data, including through analysing sentiment differences between the UK and Spain \\nconcerning global warming (Loureiro & Alló, 2020), media frames of global warming (Jang, 2013), \\ncomparing climate change and nuclear weapons (Allen & McAleer, 2018), and the assessment of \\ndisaster damage (Kryvasheyeu et al., 2016).\\nHowever, the present literature shows a lack of a comprehensive research on people’s specific \\nconcerns, how they (emotionally) perceive global warming, and how their concerns and emotions are \\nlinked. To fill this gap, the current study aims to explore people’s main loci of attention and emotions \\nregarding global warming. This study analysed tweets containing the keywords “global warming” \\nfrom January 2020 to July 2021 using topic modelling, specifically latent Dirichlet allocation (LDA) \\n(Blei et al., 2003) and sentiment analysis based on Plutchik’s eight basic human emotions. LDA offers \\nstatistical access to the latent topics across the unistructural data, which provides the information \\nabout what people are mainly debating. In order to know the attitude of the public towards global \\nwarming, sentiment analysis was performed to gain the polarity of people’s general opinions and the \\nspecific emotions people convey via language.\\nThe paper is structured into two parts: the first part focuses on the topics most popularly discussed \\nduring the period under study through LDA; the second part concerns the polarity of emotions that \\npeople expressed toward global warming.\\nMETHODOLOGY\\nThe current study examines the global warming discussion on Twitter from the perspective of topics \\nand emotions by employing latent Dirichlet allocation (LDA), a topic modelling technique, and \\nsentiment analysis.\\nLatent Dirichlet Allocation (LDA) Model\\nTopic model, also referred as probabilistic topic model, is a statistical model for unearthing the \\nlatent semantic structures in a corpus, providing observable topics hidden in the corpus. In the age of \\ninformation explosion, the information from all sources such as newspapers, web pages, books, and \\nsocial media is beyond human processing capacity, and people find it prohibitively difficult to find the \\nintended messages. Computational techniques such as topic modelling extract thematic information \\n(topics) to help people find, organize and understand the substantial quantity of unstructured texts \\n(Blei, 2012). LDA is the most popular topic modelling algorithm in the application of topic extraction \\nfrom a collection of text bodies (Albalawi et al., 2020; Gerlach et al., 2018).\\nThe basic and main assumptions of the LDA model are that each document contains diverse \\ntopics, and each topic has a probability distribution over words (Blei et al., 2003). One of the most \\nprominent advantages of the LDA model is that topics can be extracted from collections of documents \\nwithout any prior knowledge input (Albalawi et al., 2020). The model operates on the basis of following \\nassumptions: there are k topics in the corpus D consisting of M documents, and each document is \\na sequence of N words w. Hyperparameters α and η denote Dirichlet priors for the document topic \\ndistribution θ and word distribution β, respectively. The specific generation process is as follows: (a) \\nβj is selected in each topic j; (b) θm is selected in each document m; (c) a topic z is selected from the \\ndistribution represented by θm in each word position n in document m; (d) a word is selected from \\ndistribution represented by βz. The plate graph (Figure 1) illustrates how the LDA works specifically. \\nThe circles represent variables, and the rectangles represent iteration processes among documents, \\nwords and topics. The coloured circle represents the results of words denotating topics, which is the \\nonly visible variable in the corpus, the other variables are latent in this model.\\nThe number of topics is s the most crucial parameter for the final results as k is pre-decided \\nbefore the operation of LDA model. Usually, the number of topics depends on research questions \\nand research aims (Boussalis & Coan, 2016; Quinn et al., 2010). If the k value is too high, the results \\ncould be incomplete in terms of information; while if the k value is too low, over-clustering could'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 2}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n3\\nhappen (Greene et al., 2014). In order to gain a proper k value, LDA model was performed with 7 \\ntopics, 10 topics, and 15 topics to compare the quality of the results. Finally, it was decided that the \\nresults are satisfactory and suitable for the present study when k = 7. LDA was conducted using \\nPython (Rehurek & Sojka, 2010) via the Gensim package.\\nSentiment Analysis\\nSentiment analysis, also known as opinion mining, is a natural language processing (NLP) technique \\nthat explores people’s opinions, attitudes, and feelings towards a specific topic, and whether they \\nare positive, neutral or negative (Asif et al., 2020; Stine, 2019). Beyond polarity, it also examines \\nemotional states such as joy, anger, or fear. Sentiment analysis can be traced back to the 1950s \\n(Puschmann & Powell, 2018) when written paper documents were the main research source. With \\nthe development of the Internet, information now has a variety of different sources, such as web \\npages, online news, blogs, comments, reviews and social media in particular. Recent decades have \\nseen a rapid growth in the use of sentiment analysis. It has been applied in different areas to detect \\npeople’s opinions, such as product reviews (Bhuskute, 2020), newspaper article reviews (Pandiaraj \\net al., 2021), opinions linked to tourism (Yan et al., 2020), wine reviews (Matheson et al., 2019) and \\ngender studies (Thelwall, 2018). Social media, particularly Twitter, offers an observable and accessible \\nsight into people’s views and feelings towards current issues. Many studies using Twitter data to mine \\nopinions to events such as the government’s response to wildfires (Mustaqim et al., 2020), happiness \\nlevel relative to geography (Mitchell et al., 2013), multimodal information (Kumar & Garg, 2019), \\nand extremist tendencies (Asif et al., 2020).\\nThe two most popular approaches to the process of sentiment analysis are sentiment analysis \\nbased on wordlists, which are weighted in the form of scores, and sentiment analysis based on \\nmachine learning (Stine, 2019). The present study employs the wordlist-based approach, specifically \\nthe NRC Word-Emotion Association Lexicon, to track the emotions of people and the proportion of \\nthe emotions distributed on Twitter in the discussion of global warming. The NRC Word-Emotion \\nAssociation Lexicon (Mohammad & Turney, 2013, 2010), which is often used to extract emotions \\nfrom texts, which is a list of 10,170 English words coded for Plutchik’s eight basic human emotions \\n(anger, fear, anticipation, surprise, trust, joy, sadness, disgust) and two polarities (positive and \\nnegative). It assigns scores to each word ranging from 0 to 1, representing the lowest and highest \\namount of emotion of a specific word, respectively. The percentage (p) of a specific emotion e is \\ncomputed by following equation:\\np\\nF\\nF\\nF\\ne\\nw\\np\\nn\\n=\\n+\\n\\t\\n(1)\\nFigure 1. The workflow of the LDA model.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 3}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n4\\nwhere Fw represents the frequency of words with a specific emotion such as joy in a tweet, Fp \\nmeans the frequency of positive words in a tweet, and Fn is the frequency of negative words in a tweet.\\nData Processing\\n(1) \\tData collection: The corpus consists of Twitter data collected by Python via the Twitter \\ngardenhose Application Program Interface (API). All tweets retrieved contain the words “global \\nwarming” during a span from January 1, 2020 to July 30, 2021. After carefully excluding \\nduplicated messages via a duplicate detection algorithm (Rajaraman & Ullman, 2011), the final \\ndata returned 538,478 tweets with nearly 22 million words.\\n(2) \\tData preprocessing: In order to improve validity and reliability of the data, this study followed \\nsuggestions from Maier et al. (2018), and performed a sequence of careful and strict preprocessing \\nsteps in the following order. The first step is tokenization, dividing documents into smaller units, \\nusually word units. After tokenization, all capital letters are transformed into lowercase for the \\nconvenience of term unification and all punctuation marks were removed including period (.), \\ncomma (,), question mark (?), exclamation point (!) and special characters such as ampersand \\n(&), slash (/), backslash (\\\\), and the tilde (~), which are uninformative in text-mining based on \\nbag-of-words (Kirelli & Arslankaya, 2020; Maier et al., 2018). Following that, the removal of \\nstop words (e.g., articles, prepositions) is necessary since stop words bear no specific meaning \\nthus have little contribution to the document content (Mustaqim et al., 2020). The next step is \\nunification including lemmatization and stemming. Lemmatization is performed in preference \\nto stemming because lemmatization is much more informative than stemming while stemming is \\nbelieved to be less precise and more difficult to interpret (Schütze et al., 2008). Lemmatization \\nis a process of transforming inflected forms of words to the lemma, such that “studies” and \\n“studying” become “study”. It examines the surrounding context of a word to identify the part of \\nspeech of a given word. Stemming is the process of generating word stem, base or root form by \\ncutting derivational and inflectional suffixes, such as “studies” become “studi”, and “studying” \\nbecome “study”. Thus, the stem may not be an actual word can be looked up in dictionary. \\nUnlike lemmatization, stemming operates on a single word without consideration of the context \\nof a word. The last procedure is relative pruning, deleting extremely infrequent and extremely \\nfrequent words in a corpus to improve the algorithm’s performance.\\nRESULTS AND DISCUSSION\\nThe following part begins with the results of topic modelling by LDA, specifically, the categories \\nof topics, overall changes of topics over time and how the topics are represented in language. Then \\nthe results of sentiment analysis are presented in terms of eight different emotions distributed across \\nTwitter data concerning global warming.\\nTopic Analysis\\nThe LDA model computed the seven most manageable and significant clusters of topics. They are \\nlisted in descending order from the highest proportion to the lowest: Factors causing global warming; \\nImpact of global warming; Actions to stop global warming; Relation between global warming and \\nCovid-19; Close relation between Global warming and politics; Global warming as a hoax and Global \\nwarming as a reality. Figure 3 shows the proportions of each topic and their developments over time \\nand Figure 4 shows the key topics with highly weighted keywords surrounding.\\n(1) \\tFactors causing global warming'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 4}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n5\\nThe most frequently debated topic with the highest proportion is the factors causing global \\nwarming, as its name shows, focusing on some specific factors contributing to it. The highly frequent \\nwords around this topic are “emission”, “carbon”, “pollution”, “human”, “anthropogenic”, “energy”, \\n“environment”, “CO2”, “gas”, “greenhouse”, “fossil”, and “plastic”. The data show that the topic \\nreaches the peaks when disasters or abnormal weather events happen, such as Hurricane Laura and \\nCalifornia wildfires. As previous studies have pointed out, abnormal weather and disasters usually \\nintensify the discussion over global warming on Twitter (Hamilton & Stampone, 2013; Molodtsova \\net al., 2013; Zaval et al., 2014). An example is listed here: “it’s a feedback loop: as peatlands release \\nmore carbon, global warming increases, which thaws more peat and causes more wildfires.”.\\nGenerally speaking, the data shows that most people believe it is anthropogenic factors that \\ncause global warming, which is in line with the results of the research by Leiserowitz et al. (2020). \\nThe high occurrence of the word “emission” shows that carbon emissions and the greenhouse \\neffect are considered by Twitter users as the most crucial factor leading to global warming. Here \\nare some examples showing this topic: “For 40 years scientists understood human-caused increase \\nconcentrations of greenhouse gases are driving global warming but efforts to curtail emissions have \\nnot been successful. It’s now 2020, and our inaction is leading many to consider geoengineering.”; \\n“Ocean 2020 Definition: A large body of water filled with oil, trash, micro plastics, acidification, \\ndying ecosystems, rising temperatures and human ignorance. Global warming.”.\\n(2) \\tImpact of global warming\\nJust following the topic of Factors, the topic of Impact of global warming comes second in the \\nproportional size of topics. It centres on the consequences, specifically disasters, of global warming. \\nFigure 2. Research procedure'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 5}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n6\\nGlobal warming is considered as a threat, a dangerous phenomenon. The keywords around this topic \\nare “impact”, “consequence”, “polar”, “disaster”, “effect”, “rise”, “threat”, “extreme”, “weather”, \\n“alarm”, “sea”, and “glacier”.\\nThe topic often involves natural disasters and extreme weather events, such as wildfires, hurricane, \\nsnow storms, and floods. In other words, most people think those disasters and extreme weather \\nare caused by global warming which results from human activity. Here are some typical examples: \\n“Terrifying consequence of global warming. My heart is with Australia and Australians and their \\nbeautiful country and wildlife.”; “Global warming pushes temps to near-record levels in 2020, in \\neffect trying 2016 as the hottest year on record, according to data released by US science agencies.”; \\n“We all say ‘That’s 2020’ about stuff like multiple consecutive hurricanes and wildfires. It just hit \\nme. It’s not ‘2020’. This is what life is from now on until we address global warming.”.\\n(3) \\tActions to stop global warming\\nWith so many adverse consequences brought by global warming, a large number of Twitter users \\nadvocate for actions to be undertaken immediately, contributing to the third topic—Actions to stop \\nglobal warming. The most frequently co-occurring words are “action”, “act”, “fight”, “stop”, “policy”, \\n“scientist”, “agreement”, “solve”, “sustain”, and “renew”. It is not surprising to find advocation \\nof actions to stop global warming knowing its damages and the factors causing it. The actions are \\ngrouped into the following: reduction of toxic emissions, policy-making related to the environment, \\nusing products from sustainable sources, zero pollution, etc.\\nSome examples of this topic: “Peeps are saying ‘2020 is horrible’ as though they expect next \\nyear will be better. But will it? Rolling disasters have been predicted by science as the consequence \\nof global warming. Governments must ACT NOW. Elect governments that will take action.”; “In a \\nbid to promote eco-friendly industries in the state, the Gujarat Industrial Policy 2020 incentivizes \\nsetting-up of green ventures and adoption of clean and green technologies. This new policy will help \\nfight global warming and will help making Gujarat cleaner and greener.”; “Set in summer 2020, \\nValio’s emission reduction targets to stop global warming at 1.5 degrees have been certified by the \\nScience-based Target initiative.”\\nFigure 3. Proportions of topics'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 6}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n7\\n(4) \\tGlobal warming and Covid-19\\nThe fourth topic extracted from data is the connection between global warming and Covid-19. \\nThe two seemingly unconnected issues are unexpectedly connected with each other. The analysis \\nbegins with the keywords of the topic, which are as follows: “Covid-19”, “virus”, “hoax”, “people”, \\n“emission”, “coronavirus”, “Trump”, “kill”, “worry”, “pandemic”, “carbon”, “news”, “lockdown”, \\n“life”, “emergency”, and “science”.\\nExamining the tweets more closely, two general trends were found. The first focuses on carbon \\nemission reduction influenced by Covid-19 during the lockdown. As one Twitter user put it: “Although \\nthe Covid-19 pandemic will cause a dip in 2020 emission, this will not bring the words closer to \\nthe Paris Agreement goal of limiting global warming this century to well below 2°C and pursuing \\n1.5°C.”. This phenomenon also attracted academic attention to the relations between toxic emissions \\nand the pandemic, and the findings showed a significant decrease in emissions in most countries in \\nthe world during the lockdown period (Evangeliou et al., 2021; Sarfraz et al., 2021).\\nThe second draws parallels between global warming and Covid-19, highlighting the importance \\nand severity of problems humans are confronting, such as “2021 will not be a new year because global \\nwarming, the extinction crisis, the Covid-19 global pandemic, along with unemployment, hunger and \\npoverty, will be part of that year as they are in 2020.”. Interestingly, it also usually involves blame \\nor sarcasm about Trump, for example, “Funny how Trump dismisses scientist’s warning regarding \\nglobal warming and Covid 19, but he is desperately hoping they will produce a vaccine before Nov 3.”; \\n“Trump doesn’t believe in science. He said Covid-19 was a ‘Democratic hoax’. He also claims global \\nwarming is a Democrat hoax.”. From the data, it is inferred that a sizeable number of Twitter users \\nwere not satisfied with statements by Trump that Covid-19 and global warming are both unimportant.\\n(5) \\tClose relations between global warming and politics\\nThe results show that global warming is closely connected with politics, especially with American \\nparties. The keywords surrounding the topic are “politic”, “Trump”, “election”, “ideology”, “people”, \\n“government”, “Biden”, “vote”, “power”, “party”, and “Democrat”. It is discovered that global \\nwarming was connected in the corpus with the 2020 American presidential election. Here are some \\nillustrative examples: “Breaking: Global warming is the top most important issue among liberal \\nDemocrats deciding who they’ll vote for in the 2020 Presidential Election, followed by healthcare, \\nincome gap and environment protection.”; “2020 ELECTION: Global warming surges as a voting \\nissue! It is now the top 1 voting issue (out of 29) among liberal Democrats and top 5 among moderate \\nconservative Democrats.”.\\nGlobal warming has been politicalized since the late 1980s when it was listed in the US national \\nagenda. McCright and Dunlap (2011) investigated the American public’s opinions towards global \\nwarming over ten years and discovered that the global warming issue was significantly dominant \\nin political ideologies and partisan polarization. Our data also show the same results, as this tweet \\ndemonstrates: “So farewell global warming, you have served your political purpose. Hallo extreme \\nweather.”\\n(6) \\tGlobal warming as a hoax\\nContrary to the “common sense” that climate scientists believe, a number of people remain \\nsceptical about the truth of global warming. A study found that almost 1/3 people in America \\nwere sceptical about global warming and denying that global warming is caused by human activity \\n(Leiserowitz et al., 2015). Similarly, our results show that many Twitter users believe that global \\nwarming is just a hoax, denying global warming. The high frequency weighted words surrounding the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 7}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n8\\ntopic in decreasing order are “hoax”, “news”, “fake”, “Trump”, “blame”, “lie”, “people”, “media”, \\n“scam”, “deny”.\\nThese highly weighted words show following important information: First, many Twitter users \\nare sceptical about the gradual increase in temperature of the Earth, considering it “fake news”, \\n“scam”, and “lie”, such as in: “Man-made global warming is the biggest and most expensive lie in the \\nhistory of the world.”; “Climate change is a natural phenomenon. Human cause global warming is a \\nHoax.” Second, the news about global warming is “framed” by media, disseminating among people, \\nfor example, “global warming is a fake news.”; “Ordinary people are denied the right to be heard as \\nthe global warming scam is embraced once more by gullible Western leaders. The world media is a \\ndishonest supporter of the hoax by failing to be objective and reporting only one side.”; Third, the \\ndiscourse about the “hoax” theory is closely related to the former American president Donald Trump, \\nsuch as in: “I remember when Trump claimed that global warming was a hoax created by China.”; \\n“I thought Trump said global warming is a hoax.”\\nFigure 4. Key topics of global warming discussion on Twitter'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 8}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n9\\nDonald Trump, who has long been a denier of global warming, tweeted some messages in 2012 \\nthat global warming is a lie. The tweet “The concept of global warming was created by and for \\nChinese in order to make US manufacturing non-competitive,” was retweeted, according to reports \\nby The New York Times, over 104,000 times, and “liked” nearly 66,000 times (Wong, 2016). It is \\nsuggested that his tweets about global warming had a great impact on people’s view of it, since social \\nmedia, especially Twitter, has been an important place for political communication, influencing a \\nlarge number of people’s opinions (Buccoliero et al., 2020; Hong & Nadler, 2011).\\n(7) \\tGlobal warming as a reality\\nThe last topic with the least proportion across the whole data set is that global warming is a reality, \\nwith highly frequent co-occurrence of words “real”, “true”, “evidence”, “believe”, “anthropogenic”, \\n“fact”, “news”, “Greta”, “crisis”, “wildfire”, and “melt” (Figure 2). As overwhelming scientific \\nevidence shows, the average temperature of the Earth is getting higher, and this is primarily caused by \\nhuman activity. Consistent with the findings of Leiserowitz et al. (2020) that almost half Americans \\nstrongly believe that global warming is happening, our results show the topic of believing the truth \\nof global warming.\\nLooking more closely the tweets, it is evident that this topic has two prominent features, i.e., \\npersuading people into believing the fact of global warming (related to topic 6) and listing natural \\ndisasters and extreme weather events such as hurricanes, wildfires, melting of glaciers, and high \\ntemperatures at the poles.\\nRegarding the first feature, many Twitter users talk about the truth of global warming, yet at the \\nsame time mention those who don’t believe in it. This is probably due to the prevalence of the “hoax” \\nframe, i.e., some believe global warming is a hoax. Some examples are listed here: “The fact that \\npeople still don’t believe in global warming even though 2020 have been a year that have shown us \\na lot of consequences is crazy. Keep this up and this is our future, our every year. Nature disasters, \\nnew disease… Global warming is real.”; “2-4 inches of rain expected in the next two days. Just in \\ncase anyone doesn’t believe global warming is real. January 8th 2020 that should be snow.”; “It’s a \\ndisgrace that ‘views on global warming or climate change’ is even a thing adults will talk about in \\n2020. Nobody has ‘views on whether dogs are real’ or ‘views on days being longer in summer than \\nwinter’. These are facts. Climate change is a fact.”.\\nAs for tweeting a list of natural disasters and extreme weather events, the data shows that this \\nseemingly serves as evidence proving the fact of global warming. Some examples of this: “Global \\nwarming is real because January 2020 was the hottest in 141 years.”; “On August 8, 2020, the \\nCanadian ice shelf larger than Manhattan collapses into the sea. Global warming is real and it’s a \\ncrisis.”; “I’m saying this for a reason, flooding, tornadoes, snow storms, hail, wind, etc. Resources \\nneed to be available no matter what. We are in 2021 and earth has already proved, with no avail, that \\nglobal warming is real and upon us. Mother nature is mad and we’re the issue behind it.”.\\nAccording to the results, what people care most in the collective sense is the factors, then the \\nimpacts and actions need to be done when it comes to global warming. This is typical blame behaviour, \\nblaming someone or something else first when people are in uncomfortable situations and defending \\nthemselves from cognitive dissonance (Hein, 1998). The seven topics reveal the main attentions as \\nwell as behaviours of Twitter users in the debate of global warming.\\nSentiment Analysis\\nThe sentiment analysis was performed using Python based on the NRC Word-Emotion Association \\nLexicon, which revealed the distribution and proportion of the eight emotions by Plutchik across the \\ndata. Plutchik proposed the wheel of emotions and divided emotions into eight categories, viz., joy, \\ntrust, fear, surprise, sadness, disgust, anger and anticipation (Plutchik, 1980, 1982). This is one of the \\nmost influential emotion theories. He suggested a bipolarity in the eight emotions: joy is the opposite'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 9}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n10\\nof sadness, fear is the opposite of anger, anticipation is the opposite of surprise and disgust is the \\nopposite of trust. Joy, trust, anticipation and anger are considered as positive in valence, while fear, \\nsadness, disgust and surprise denote polarity of negativeness. In opposition to common understanding, \\nthe negative valence of anger is classified as a positive emotion and surprise as negative. This is \\nbecause anger is considered a sign of strength, motivating action (Hess, 2014) and indicates a path \\nof “moving toward” a goal (TenHouten, 2014) . As for surprise, it involves a violation of people’s \\npsychological territory (TenHouten, 2006), and usually brings about unpleasantness (Noordewier \\n& Breugelmans, 2013).\\nVery differently to the common understanding of the public perception of global warming \\nas a negative issue, the sentiment analysis results show that the proportion of positive views of \\nglobal warming is, surprisingly, higher than the negative one (Figure 5). The results confirmed the \\nPollyanna hypothesis (Boucher & Osgood, 1969), that people tend to think about the positive side \\nof events. Similarly, the study conducted by Loureiro and Alló (2020) also showed similar results \\nwhen they compared the Twitter messages containing to keywords “climate change” in the UK and \\nSpain, showing that the overall polarity was positive in the UK, while the opposite was true in Spain. \\nPerhaps it is attributable to the high proportion of trust (16.32%) and anger (13.7%) which is usually \\nviewed as a negative feeling.\\nThe emotion most evoked by global warming is fear, with a proportion of 18.35%, contributing \\nmost to the negative valence, which is followed by the emotion of trust with 16.32%, contributing \\nmost to the positivity in valence. Anger and anticipation occupy comparatively big portions with \\n13.7% and 13.17% respectively, which is followed by sadness with 12.20%. The feeling that global \\nwarming evokes least is surprise (7.51%). Disgust (9.48%) and joy (9.28%) account for about 18% \\nin positive and negative valence, respectively.\\nWordcloud is used for the display of most frequent words assigned to different emotions (Figure \\n6). The bigger size of the word is, the more frequent the word appears in terms of a specific emotion, \\nand the more contributions to the emotion classification it makes.\\nThe results show that fear is the most evoked emotion, just as Hulme (2008) points out, “we are \\nliving in a climate of fear about our future climate”. The most frequent words serving the emotion \\nFigure 5. Emotion proportions in relation to global warming'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 10}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n11\\nare “change”, “pandemic”, “government”, “kill”, “fight”, “threat”, and “bad”. It is inferable that \\nthe changes in climate and the pandemic are both considered “threats”, causing the fear emotion. \\nAccording to Plutchik and Kellerman (2013), the stimulus “threat” causes a cognition of danger, \\nresulting a feeling of fear. The corresponding behaviour is to run away from it. The results indicate \\nthat humans are still not prepared for the fact or consequences of global warming, being greatly \\nconfused or even frightened by all those extreme weather events. The highly frequent words such as \\n“scientist”, “level”, “economy”, “money”, “tree”, “president”, “policy”, “deal”, “expert” lead to the \\nhigh proportion of the emotion of trust. The emotion of trust means a mental acceptance, a willingness \\nto have a close relationship with (something). It suggests that people such as scientists, presidents, \\nand experts are considered “friends” as the Plutchik’s emotion theory argues and the “policy” and \\n“deal” made by them are also convincing. In this regard, 16.3% people showed confidence in the \\nglobal warming issue, still holding a very positive view of it. The stimulus of obstacles is considered \\nas an enemy cognitively, from which the anger emotion arises. The words “hoax”, “threat”, “bad”, \\n“hot”, “fight”, “blame”, “disaster”, and “storm” contribute most to the feeling of anger. The results \\nshow that the belief that global warming is a hoax causes a strong anger across Twitter, as do all \\ndisasters. As for anticipation, it involves the stimulus of new territory, resulting in a corresponding \\nbehaviour of examination. The words “scientist”, “time”, “tree”, “money”, “plan”, “deal”, and “hope”. \\nare crucial to the emotion of anticipation. The sad feeling deriving from the loss of valued people or \\nthings focuses on the consequences of different disasters, with the keywords “hoax”, “pandemic”, \\n“bad”, “die”, “kill”, “wildfire”, and “disease”. The belief that global warming is a hoax, disasters, \\nand anthropogenic pollution generate the feeling of disgust with keywords “hoax”, “blame”, “tree”, \\n“bad”, “lie”, “shit”, “death” and so forth. A small proportion of joy elicited by global warming is \\naffiliated with the words “love”, “tree”, “money”, “save”, “green”, “hope”, “humanity” along with \\nothers. The least roused feeling is surprise, which usually produces interrelated behaviour of alertness \\nand halting. The keywords are “Trump”, “hoax”, “money”, “disaster”, “death”, “free”, and “hope”. It \\nis interesting to note that the word “hoax” appeared very often in the wordcloud with a high weight \\nin the emotion of sadness, disgust, anger and surprise. In other words, the belief that global warming \\nis a hoax tends to provoke a negative feeling.\\nIn order to further examine the attentions and emotions of the public in relevance to global \\nwarming, the connections between emotions and topics were analysed through Groupby sum.\\nThe results (Figure 7) show that the fear emotion takes a leading position in all topics, followed \\nby trust. It is noted that people show more trust than other emotions when talking about the reality \\nof global warming and actions needed to be done, and show the least trust for the topic of Hoax. \\nAnticipation appears least in the topic Hoax, and most in the discussion of impact of global warming. \\nSadness is displayed most in topic of the relations between global warming and Covid-19. The \\ntopic Reality shows more joy emotion than other topics while the topic Hoax exhibits least joy. It is \\ninteresting to note that the topic Hoax show a very prominent negativity in emotion with high scale \\nof fear, least trust, joy and anticipation, as well as most disgust and surprise.\\nThe topics exhibits the general attention of the public in relevance to global warming on Twitter \\nand the sentiment analysis show the mental state of people towards it. The comprehensive research \\nof global warming from the perspective topic and emotion shed light on people’s attitudes towards it.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 11}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n12\\nCONCLUSION\\nThe present study, which is based on big data analytics, extracted 538,478 tweets about global warming, \\nspanning 18 months, analysed people’s attention and emotion regarding global warming through \\nLDA, a topic modelling method, and sentiment analysis on the basis of Plutchik’s emotion theory.\\nFigure 6. Wordcloud of each emotion\\nFigure 7. Connections between emotions and topics'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 12}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n13\\nSeven significant clusters of topics emerged from the data using the LDA. They were as follows: \\nFactors causing global warming, the most frequently occurring set in the data, in which anthropogenic \\ncauses of global warming, especially greenhouse emissions, are very frequently mentioned; Impact of \\nglobal warming, which contains references to natural disasters caused by global warming; Actions to \\nstop global warming, which covers (largely very urgent) calls for actions to counteract global warming; \\nRelation between global warming and Covid-19, which contains two main trends – the reduction in \\ncarbon emissions owing to the lockdown and highlighting the parallels between Covid-19 and global \\nwarming as crises; Close relation between Global warming and politics, in which there were a number \\nof references to American political parties; Global warming as a hoax, or expressions of scepticism \\nand denials of the truth of global warming, in which anger was a prominently expressed emotion \\nand Global warming as a reality, in which lists of natural disasters and other extreme environmental \\nevents are frequently mentioned, along with attempts to persuade others of the truth.\\nThe sentiment analysis showed the positive discussion of global warming is more prevalent than \\nnegative discussion, which provides empirical evidence for the Pollyanna phenomenon whereby people \\nuse more positive words to describe an event if is a disaster, showing a bright side of things (Boucher \\n& Osgood, 1969). The most evoked emotion in the discussion over global warming on Twitter is \\nfear, followed by trust. And the least roused emotion by the issue is surprise, followed by joy. It is \\ninterpreted that most Twitter users considered global warming as a threat, which gives rise to a sense \\nof danger. The fear produced in them probably is closely related with what had already happened, \\nsuch as the severe consequences (disasters) caused by it or unpredictable situations. Still, a number \\nof people showed trust in regard to global warming, accepting the current situation. Regarding the \\nconnections between topics and emotions, fear dominates most of topics while the topic of action \\nand reality show more trust than others.\\nGlobal warming, as an issue affecting all human beings, has been extensively debated for years \\nthrough different platforms such as newspapers, Facebook, Twitter. Understanding the public’s \\nperception of global warming is of importance to economic development, policy making, lifestyle \\ndecisions, etc. The present study provides an academic information for the research of emotions \\nevoked by the discourse on global warming.\\nACKNOWLEDGMENT\\nThis research was supported by China Scholarship Council [grant number 201808610242], Xi’an \\nInternational Studies University PhD Scholarship [grant number syjsb201704], and Key Research \\nProject from Xi’an International Studies University [grant number 17XWZD04].'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 13}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n14\\nREFERENCES\\nAlbalawi, R., Yeap, T. H., & Benyoucef, M. (2020). Using topic modeling methods for short-text data: \\nA comparative analysis. Frontiers in Artificial Intelligence, 3(42), 1–14. doi:10.3389/frai.2020.00042 \\nPMID:33733159\\nAllen, D. E., & McAleer, M. (2018). President Trump tweets supreme leader Kim Jong-Un on nuclear weapons: \\nA comparison with climate change. Sustainability, 10(7), 1–6. doi:10.3390/su10072310\\nAsif, M., Ishtiaq, A., Ahmad, H., Aljuaid, H., & Shah, J. (2020). Sentiment analysis of extremism in social media \\nfrom textual information. Telematics and Informatics, 48, 1–20. doi:10.1016/j.tele.2020.101345\\nBhuskute, L. (2020). Sentiment analysis of product reviews. International Journal of Advanced Science and \\nTechnology, 29(4), 7671–7677. \\nBlei, D. M. (2012). Introduction to probabilistic topic models. Communications of the ACM, 55(4), 77–84. \\ndoi:10.1145/2133806.2133826\\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of Machine Learning \\nResearch, 3, 993–1022. \\nBoucher, J., & Osgood, C. E. (1969). The Pollyanna hypothesis. Journal of Verbal Learning and Verbal Behavior, \\n8(1), 1–8. doi:10.1016/S0022-5371(69)80002-2\\nBoussalis, C., & Coan, T. G. (2016). Text-mining the signals of climate change doubt. Global Environmental \\nChange, 36, 89–100. doi:10.1016/j.gloenvcha.2015.12.001\\nBrown, T., Budd, L., Bell, M., & Rendell, H. (2011). The local impact of global climate change: Reporting on \\nlandscape transformation and threatened identity in the English regional newspaper press. Public Understanding \\nof Science (Bristol, England), 20(5), 658–673. doi:10.1177/0963662510361416 PMID:22164705\\nBuccoliero, L., Bellio, E., Crestini, G., & Arkoudas, A. (2020). Twitter and politics: Evidence from the US \\npresidential elections 2016. Journal of Marketing Communications, 26(1), 88–114. doi:10.1080/13527266.20\\n18.1504228\\nCody, E. M., Reagan, A. J., Mitchell, L., Dodds, P. S., & Danforth, C. M. (2015). Climate change sentiment \\non Twitter: An unsolicited public opinion poll. PLoS One, 10(8), 1–18. doi:10.1371/journal.pone.0136092 \\nPMID:26291877\\nEvangeliou, N., Platt, S. M., Eckhardt, S., Lund Myhre, C., Laj, P., Alados-Arboledas, L., Backman, J., Brem, \\nB. T., Fiebig, M., Flentje, H., Marinoni, A., Pandolfi, M., Yus-Dìez, J., Prats, N., Putaud, J. P., Sellegri, K., \\nSorribas, M., Eleftheriadis, K., Vratolis, S., & Stohl, A. et\\xa0al. (2021). Changes in black carbon emissions over \\nEurope due to COVID-19 lockdowns. Atmospheric Chemistry and Physics, 21(4), 2675–2692. doi:10.5194/\\nacp-21-2675-2021\\nGerlach, M., Peixoto, T. P., & Altmann, E. G. (2018). A network approach to topic models. Science Advances, \\n4(7), 1–11. doi:10.1126/sciadv.aaq1360 PMID:30035215\\nGreene, D., O’Callaghan, D., & Cunningham, P. (2014). How many topics? Stability analysis for topic models. \\n10.1007/978-3-662-44848-9_32\\nGunnemyr, M. (2019). Causing global warming. Ethical Theory and Moral Practice, 22(2), 399–424. doi:10.1007/\\ns10677-019-09990-w\\nHamilton, L. C., & Stampone, M. D. (2013). Blowin’ in the wind: Short-term weather and belief in anthropogenic \\nclimate change. Weather, Climate, and Society, 5(2), 112–119. doi:10.1175/WCAS-D-12-00048.1\\nHein, F. (1998). The blame game. IEEE Software, 15(6), 89–91. doi:10.1109/52.730852\\nHermida, A. (2013). Journalism: Reconfiguring journalism research about twitter, one tweet at a time. Digital \\nJournalism, 1(3), 295–313. doi:10.1080/21670811.2013.808456\\nHess, U. (2014). Anger is a positive emotion. In The positive side of negative emotions (pp. 55–75). The Guilford \\nPress.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 14}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n15\\nHong, S., & Nadler, D. (2011). Does the early bird move the polls? The use of the social media tool ‘Twitter’ by \\nU.S. politicians and its impact on public opinion. The Proceedings of the 12th Annual International Conference \\non Digital Government Research, 182–186. doi:10.1145/2037556.2037583\\nHulme, M. (2008). The conquering of climate: Discourses of fear and their dissolution. The Geographical \\nJournal, 174(1), 5–16. doi:10.1111/j.1475-4959.2008.00266.x\\nJang, S. M. (2013). Framing responsibility in climate change discourse: Ethnocentric attribution bias, perceived \\ncauses, and policy attitudes. Journal of Environmental Psychology, 36, 27–36. doi:10.1016/j.jenvp.2013.07.003\\nKirelli, Y., & Arslankaya, S. (2020). Sentiment analysis of shared tweets on global warming on Twitter with \\ndata mining methods: A case study on Turkish language. Computational Intelligence and Neuroscience, 2020, \\n1–9. doi:10.1155/2020/1904172 PMID:32963511\\nKirilenko, A. P., Molodtsova, T., & Stepchenkova, S. O. (2014). People as sensors: Mass media and local \\ntemperature influence climate change discussion on Twitter. Global Environmental Change, 30, 92–100. \\ndoi:10.1016/j.gloenvcha.2014.11.003\\nKryvasheyeu, Y., Chen, H., Obradovich, N., Moro, E., Hentenryck, P. V., Fowler, J., & Cebrian, M. (2016). \\nRapid assessment of disaster damage using social media activity. Science Advances, 2(3), 1–11. doi:10.1126/\\nsciadv.1500779 PMID:27034978\\nKumar, A., & Garg, G. (2019). Sentiment analysis of multimodal twitter data. Multimedia Tools and Applications, \\n78(17), 24103–24119. doi:10.1007/s11042-019-7390-1\\nLe Duff, M., Dumas, P., Allenbach, M., & Cohen, O. (2020). An orientation for coastal disaster risks management \\nand prevention policy in a global warming context: Case study in Ouvea (New Caledonia). Marine Policy, 117, \\n1–13. doi:10.1016/j.marpol.2018.12.012\\nLeiserowitz, A., Maibach, E., Rosenthal, S., Kotcher, J., Bergquist, P., Ballew, M. T., Goldberg, M., & Gustafson, \\nA. (2020). Climate change in the American mind: November 2019. PsyArXiv. 10.31234/osf.io/z3wtx\\nLeiserowitz, A., Maibach, E., Roser-Renouf, C., Feinberg, G., & Rosenthal, S. (2015). Climate change in the \\nAmerican mind. Yale University and George Mason University.\\nLoureiro, M. L., & Alló, M. (2020). Sensing climate change and energy issues: Sentiment and emotion analysis \\nwith social media in the U.K. and Spain. Energy Policy, 143, 1–11. doi:10.1016/j.enpol.2020.111490\\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., \\nHäussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA topic modeling in communication research: \\nToward a valid and reliable methodology. Communication Methods and Measures, 12(2–3), 93–118. doi:10.1\\n080/19312458.2018.1430754\\nMatheson, K., Plangger, K., Kietzmann, J., Vella, J., & Grant, P. (2019). The serious side to funny cartoons: \\nUnderstanding public perception of wine through cartoon content analysis. Journal of Wine Research, 30(2), \\n95–106. doi:10.1080/09571264.2019.1587396\\nMcCright, A. M., & Dunlap, R. E. (2011). The politicization of climate change and polarization in the American \\npublic’s views of global warming, 2001–2010. The Sociological Quarterly, 52(2), 155–194. doi:10.1111/j.1533-\\n8525.2011.01198.x\\nMitchell, L., Frank, M. R., Harris, K. D., Dodds, P. S., & Danforth, C. M. (2013). The geography of happiness: \\nConnecting Twitter sentiment and expression, demographics, and objective characteristics of place. PLoS One, \\n8(5), 1–15. doi:10.1371/journal.pone.0064417 PMID:23734200\\nMohammad, S. M., & Turney, P. D. (2010). Emotions evoked by common words and phrases: Using Mechanical \\nTurk to create an emotion lexicon. Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches \\nto Analysis and Generation of Emotion in Text, 26–34.\\nMohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word–emotion association lexicon. Computational \\nIntelligence, 29(3), 436–465. doi:10.1111/j.1467-8640.2012.00460.x\\nMolodtsova, T., Kirilenko, A., & Stepchenkova, S. (2013). Utilizing the social media data to validate’climate \\nchange’indices. AGU Fall Meeting Abstracts, GC11D-1039.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 15}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n16\\nMustaqim, T., Umam, K., & Muslim, M. A. (2020). Twitter text mining for sentiment analysis on government’s \\nresponse to forest fires with vader lexicon polarity detection and k-nearest neighbor algorithm. Journal of Physics: \\nConference Series, 1567(3), 1–8. doi:10.1088/1742-6596/1567/3/032024\\nNoordewier, M. K., & Breugelmans, S. M. (2013). On the valence of surprise. Cognition and Emotion, 27(7), \\n1326–1334. doi:10.1080/02699931.2013.777660 PMID:23560688\\nPandiaraj, A., Sundar, C., & Pavalarajan, S. (2021). Sentiment analysis on newspaper article reviews: Contribution \\ntowards improved rider optimization-based hybrid classifier. Kybernetes, 1–38. doi:10.1108/K-08-2020-0512\\nPlutchik, R. (1980). A general psychoevolutionary theory of emotion. In Theories of emotion (pp. 3–33). Elsevier. \\ndoi:10.1016/B978-0-12-558701-3.50007-7\\nPlutchik, R. (1982). A psychoevolutionary theory of emotions. Social Sciences Information. Information Sur \\nles Sciences Sociales, 21(4–5), 529–553. doi:10.1177/053901882021004003\\nPlutchik, R., & Kellerman, H. (2013). Theories of emotion (Vol. 1). Academic Press.\\nPuschmann, C., & Powell, A. (2018). Turning words into consumer preferences: How sentiment Analysis is \\nframed in research and the news media. Social Media + Society, 4(3), 1–13. doi:10.1177/2056305118797724\\nQuinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H., Radev, D. R., Purpura, S., Schrodt, P., Sin, G., \\nSinclair, B., Ward, M., Wilkerson, J., Wood, D., & Zorn, C. (2010). How to analyze political attention with \\nminimal assumptions and costs. American Journal 21 Political Science, 54(1), 209–228. doi:10.1111/j.1540-\\n5907.2009.00427.x\\nRajaraman, A., & Ullman, J. D. (2011). Mining of massive datasets. Cambridge University Press. doi:10.1017/\\nCBO9781139058452\\nRehurek, R., & Sojka, P. (2010). Software framework for topic modelling with large corpora. Proceedings \\nof the LREC 2010 Workshop on New Challenges for NLP Frameworks. doi:<ALIGNMENT.qj></\\nALIGNMENT>10.1.1.695.4595\\nSarfraz, M., Mohsin, M., Naseem, S., & Kumar, A. (2021). Modeling the relationship between carbon emissions \\nand environmental sustainability during COVID-19: A new evidence from asymmetric ARDL cointegration \\napproach. Environment, Development and Sustainability, 23(11), 16208–16226. doi:10.1007/s10668-021-\\n01324-0 PMID:33782633\\nSchmidt, A., Ivanova, A., & Schäfer, M. S. (2013). Media attention for climate change around the world: A \\ncomparative analysis of newspaper coverage in 27 countries. Global Environmental Change, 23(5), 1233–1248. \\ndoi:10.1016/j.gloenvcha.2013.07.020\\nSchütze, H., Manning, C. D., & Raghavan, P. (2008). Introduction to information retrieval (Vol. 39). Cambridge \\nUniversity Press Cambridge.\\nSegerberg, A., & Bennett, W. L. (2011). Social media and the organization of collective action: Using Twitter \\nto explore the ecologies of two climate change protests. Communication Review, 14(3), 197–215. doi:10.1080\\n/10714421.2011.597250\\nStine, R. A. (2019). Sentiment analysis. Annual Review of Statistics and Its Application, 6(1), 287–308. \\ndoi:10.1146/annurev-statistics-030718-105242\\nTenHouten, W. D. (2006). A general theory of emotions and social life. Routledge. doi:10.4324/9780203013441\\nTenHouten, W. D. (2014). Emotion and reason: Mind, brain, and the social domains of work and love. Routledge. \\ndoi:10.4324/9780203093634\\nThelwall, M. (2018). Gender bias in sentiment analysis. Online Information Review, 42(1), 45–57. doi:10.1108/\\nOIR-05-2017-0139\\nWong, E. (2016, November 18). Trump has called climate change a Chinese Hoax. Beijing says it is anything \\nbut. The New York Times. https://www.nytimes.com/2016/11/19/world/asia/china-trump-climate-change.html'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 16}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n17\\nYan, Y., Chen, J., & Wang, Z. (2020). Mining public sentiments and perspectives from geotagged social media \\ndata for appraising the post-earthquake recovery of tourism destinations. Applied Geography (Sevenoaks, \\nEngland), 123, 1–13. doi:10.1016/j.apgeog.2020.102306\\nYeo, S. K., Handlos, Z., Karambelas, A., Su, L. Y.-F., Rose, K. M., Brossard, D., & Griffin, K. (2017). The \\ninfluence of temperature on #ClimateChange and #GlobalWarming discourses on Twitter. Journal of Science \\nCommunication, 16(05), 1–25. doi:10.22323/2.16050201\\nZaval, L., Keenan, E. A., Johnson, E. J., & Weber, E. U. (2014). How warm days increase belief in global \\nwarming. Nature Climate Change, 4(2), 143–147. doi:10.1038/nclimate2093\\nZhao, W. X., Jiang, J., Weng, J., He, J., Lim, E.-P., Yan, H., & Li, X. (2011). Comparing twitter and traditional \\nmedia using topic models. European Conference on Information Retrieval, 338–349. doi:10.1007/978-3-642-\\n20161-5_34'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2021-12-29T10:21:33-05:00', 'source': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Topic_Modelling_and_Sentiment_Analysis_of_Global_W.pdf', 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-12-29T10:21:38-05:00', 'trapped': '', 'modDate': \"D:20211229102138-05'00'\", 'creationDate': \"D:20211229102133-05'00'\", 'page': 17}, page_content='Journal of Organizational and End User Computing\\nVolume 34 • Issue 3\\n18\\nFang Qiao is a PhD candidate at Xi’an International Studies University.\\nJago Williams is a postgraduate researcher in Linguistics at Bangor University.\\nENDNOTE\\n1 \\t\\nParameter α means Dirichlet prior for the document topic distribution; parameter β is the Dirichlet for the \\nword distribution; θ represents the vector for topic distribution across a document d, z represents a topic \\nextracted from document, w is the specific words in N, rectangle D is the corpus, and rectangle N refers \\nto the number of words in the document.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf\", \n",
    "    glob=\"**/*.pdf\", \n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e9d8a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
