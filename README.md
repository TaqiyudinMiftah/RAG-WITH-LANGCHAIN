# RAG-With-LangChain 

> Complete Retrieval-Augmented Generation system for academic paper review and question-answering using LangChain, FAISS, and Google Gemini.

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://python.langchain.com/)
[![FAISS](https://img.shields.io/badge/FAISS-Vector%20Store-orange.svg)](https://github.com/facebookresearch/faiss)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

##  Overview

**RAG-With-LangChain** is a production-ready Retrieval-Augmented Generation (RAG) system designed for academic paper review and intelligent question-answering. The system combines document retrieval with large language models to provide accurate, context-aware answers with citations.

### Key Features

- **Multi-Format Document Loading**: PDF, TXT, CSV, JSON support
- **Advanced Vector Search**: FAISS-based similarity search with L2 distance
- **Enriched Metadata**: 12-field metadata system for better context
- **Multiple LLM Providers**: Google Gemini, OpenAI, HuggingFace, Ollama
- **Secure API Management**: Environment-based configuration with `.env`
- **Production-Ready**: Complete with error handling, logging, and documentation

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents  â”‚â”€â”€â”€â”€>â”‚   Chunking   â”‚â”€â”€â”€â”€>â”‚  Embedding  â”‚
â”‚ (PDF/TXT)   â”‚     â”‚  (1000/50)   â”‚     â”‚ (MiniLM-L6) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Answer    â”‚<â”€â”€â”€â”€â”‚   LLM        â”‚<â”€â”€â”€â”€â”‚    FAISS    â”‚
â”‚ + Citations â”‚     â”‚  (Gemini)    â”‚     â”‚ Vector Storeâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##  Quick Start

### Prerequisites

- Python 3.9 or higher
- [UV package manager](https://github.com/astral-sh/uv) (recommended) or pip
- Google Gemini API key ([Get one here](https://makersuite.google.com/app/apikey))

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/RAG-With-Langchain.git
   cd RAG-With-Langchain
   ```

2. **Install dependencies**
   ```bash
   # Using UV (recommended)
   uv pip install -r requirements.txt
   
   # Or using pip
   pip install -r requirements.txt
   ```

3. **Setup environment variables**
   ```bash
   # Edit .env file dan tambahkan API key Anda
   # GEMINI_API_KEY=your-api-key-here
   ```

4. **Build vector store**
   ```bash
   # Letakkan PDF paper di folder: data/pdf/
   # Kemudian build vector store:
   uv run python scripts/rebuild_with_metadata.py
   ```

### ğŸš€ Quick Launcher (RECOMMENDED)

Gunakan launcher script untuk akses mudah ke semua fungsi:

**Windows:**
```bash
launcher.bat
```

**Linux/Mac:**
```bash
bash launcher.sh
```

Menu akan menampilkan:
1. ğŸ’¬ **Interactive Chat** - Tanya jawab dengan paper collection
2. ğŸ“š **Build Vector Store** - Index paper PDF ke database
3. ğŸ” **Inspect Vector Store** - Lihat isi vector store
4. ğŸ“Š **Paper Review Demo** - Demo review paper
5. ğŸ§ª **Test RAG with LLM** - Test berbagai LLM providers

### Usage

#### ğŸ’¬ Interactive Chat (Main Feature)

```bash
# Start interactive chat
uv run python chat_with_rag.py

# Dengan custom parameters
uv run python chat_with_rag.py --llm gemini --top_k 5
```

Contoh interaksi:
```
â“ You: What is the main contribution of the climate change paper?

ğŸ¤– Assistant:
The main contribution is... [detailed answer with citations]

ğŸ“š Sources:
   â€¢ climate_paper.pdf - Page 3 (relevance: 95%)
   â€¢ climate_paper.pdf - Page 5 (relevance: 89%)
```

#### Basic Example (Programmatic)

```python
from chat_with_rag import InteractiveRAGChat

# Initialize chat system
chat = InteractiveRAGChat("faiss_store", llm_provider="gemini")

# Single query
result = chat.query("What methods are used in sentiment analysis?", top_k=3)
print(result['answer'])

# Interactive mode
chat.chat_loop()
```

#### Advanced: Direct Vector Store Access

```python
from src.vectorstore import FaissVectorStore

# Load vector store
store = FaissVectorStore("faiss_store")
store.load()

# Query
results = store.query("What is the main contribution?", top_k=3)
for result in results:
    print(f"Source: {result['source']}")
    print(f"Text: {result['text'][:200]}...")
    print(f"Similarity: {result['similarity_score']:.4f}\n")
```

#### Demo Scripts

```bash
# Interactive chat (RECOMMENDED)
uv run python chat_with_rag.py

# Paper review demo
uv run python examples/paper_review_rag.py

# Test RAG with different LLMs
uv run python examples/rag_with_llm.py

# Quick Gemini demo
uv run python examples/demo_gemini.py

# Interactive multi-question demo
uv run python examples/test_gemini_interactive.py
```

##  Project Structure

```
RAG-With-Langchain/
â”œâ”€â”€ chat_with_rag.py             # ğŸ’¬ MAIN: Interactive chat interface
â”œâ”€â”€ launcher.bat / launcher.sh   # ğŸš€ Quick launcher menu
â”œâ”€â”€ .env                          # ğŸ”‘ API keys (create from .env.example)
â”œâ”€â”€ src/                          # Core source code
â”‚   â”œâ”€â”€ data_loader.py           # Document loaders (PDF, TXT, CSV, JSON)
â”‚   â”œâ”€â”€ embedding.py             # Embedding pipeline with chunking
â”‚   â””â”€â”€ vectorstore.py           # FAISS vector store with metadata
â”œâ”€â”€ examples/                     # Usage examples & demos
â”‚   â”œâ”€â”€ paper_review_rag.py      # Paper review system demo
â”‚   â”œâ”€â”€ rag_with_llm.py          # RAG with multiple LLM providers
â”‚   â”œâ”€â”€ demo_gemini.py           # Quick Gemini demo
â”‚   â”œâ”€â”€ test_gemini_interactive.py  # Interactive multi-question demo
â”‚   â””â”€â”€ app.py                   # Basic app example
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ rebuild_with_metadata.py # ğŸ“š Rebuild vector store
â”‚   â”œâ”€â”€ inspect_document.py      # Inspect document structure
â”‚   â”œâ”€â”€ inspect_metadata.py      # View metadata
â”‚   â””â”€â”€ inspect_metadata_comparison.py  # Compare metadata
â”œâ”€â”€ data/                         # Data directory
â”‚   â”œâ”€â”€ pdf/                     # ğŸ“„ Place your PDF papers here
â”‚   â”œâ”€â”€ text_files/              # Text documents
â”‚   â””â”€â”€ vector_store/            # Vector database (auto-generated)
â”œâ”€â”€ faiss_store/                  # FAISS index & metadata (auto-generated)
â””â”€â”€ docs/                         # Documentation
    â”œâ”€â”€ RAG_GEMINI_SUMMARY.md    # Complete system overview
    â”œâ”€â”€ PAPER_REVIEW_GUIDE.md    # Paper review workflow
    â”œâ”€â”€ METADATA_GUIDE.md        # Metadata documentation
â”‚   â”œâ”€â”€ ENV_SETUP.md             # Environment setup guide
â”‚   â”œâ”€â”€ GIT_GUIDE.md             # Git workflow guide
â”‚   â””â”€â”€ PRE_PUSH_SUMMARY.md      # Pre-push checklist
â”œâ”€â”€ data/                         # Data files
â”‚   â”œâ”€â”€ pdf/                     # PDF documents
â”‚   â””â”€â”€ text_files/              # Text documents
â”œâ”€â”€ paper_review_rag.py          # Paper review RAG system
â”œâ”€â”€ rag_with_llm.py              # Complete RAG with multiple LLM providers
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ pyproject.toml               # Project configuration
â””â”€â”€ .env.example                 # Environment template
```

##  Configuration

### Environment Variables

Create a `.env` file in the project root:

```env
# Google Gemini API (recommended)
GEMINI_API_KEY=your-gemini-api-key

# Optional: Other LLM providers
OPENAI_API_KEY=your-openai-key
HUGGINGFACE_API_TOKEN=your-hf-token
```

See [docs/ENV_SETUP.md](docs/ENV_SETUP.md) for detailed setup instructions.

### Vector Store Configuration

Customize in `src/vectorstore.py`:

```python
FaissVectorStore(
    vector_store_path="faiss_store",
    embedding_model="all-MiniLM-L6-v2",  # 384 dimensions
    chunk_size=1000,                      # Characters per chunk
    chunk_overlap=50                      # Overlap between chunks
)
```

### Metadata Schema

Each chunk includes 12 metadata fields:

| Field | Description | Example |
|-------|-------------|---------|
| `text` | Chunk content | "The main contribution..." |
| `source` | File path | "data/pdf/paper.pdf" |
| `page` | Page number | 5 |
| `chunk_id` | Unique identifier | 42 |
| `chunk_size` | Character count | 987 |
| `page_label` | Page label | "Page 5" |
| `total_pages` | Document pages | 12 |
| `doc_author` | Author | "John Doe" |
| `doc_title` | Title | "Climate Analysis" |
| `creation_date` | Created date | "2023-06-15" |
| `creator` | Creator tool | "LaTeX" |
| `original_metadata` | Raw metadata | {...} |

##  Current Stats

- **Documents Indexed**: 6 papers (4 PDFs + 2 TXTs)
- **Total Chunks**: 295 chunks
- **Embedding Model**: all-MiniLM-L6-v2 (384 dimensions)
- **Vector Store**: FAISS IndexFlatL2 (exact L2 distance)
- **LLM**: Google Gemini Pro (gemini-pro)

##  Supported LLM Providers

| Provider | Model | Status | Setup |
|----------|-------|--------|-------|
| **Google Gemini** | gemini-pro | âœ… Tested | [Get API key](https://makersuite.google.com/app/apikey) |
| OpenAI | gpt-3.5-turbo | âš ï¸ Code ready | [Get API key](https://platform.openai.com/api-keys) |
| HuggingFace | Inference API | âš ï¸ Code ready | [Get token](https://huggingface.co/settings/tokens) |
| Ollama | Local models | âš ï¸ Code ready | [Install Ollama](https://ollama.ai/) |

##  Documentation

Comprehensive documentation available in the `docs/` folder:

- **[RAG_GEMINI_SUMMARY.md](docs/RAG_GEMINI_SUMMARY.md)** - Complete system overview with examples
- **[PAPER_REVIEW_GUIDE.md](docs/PAPER_REVIEW_GUIDE.md)** - Workflow for reviewing papers
- **[METADATA_GUIDE.md](docs/METADATA_GUIDE.md)** - Metadata structure and usage
- **[ENV_SETUP.md](docs/ENV_SETUP.md)** - Environment configuration guide
- **[GIT_GUIDE.md](docs/GIT_GUIDE.md)** - Git workflow and best practices

##  Examples

### Add New Paper

```python
from paper_review_rag import PaperReviewRAG

rag = PaperReviewRAG("faiss_store")
rag.add_new_paper("data/pdf/new_paper.pdf")
```

### Query Specific Paper

```python
result = rag.ask_about_paper(
    question="What is the methodology?",
    top_k=3,
    source_filter="specific_paper.pdf"
)
```

### List All Papers

```python
rag.list_papers()
```

### Custom Prompt

```python
from rag_with_llm import RAGWithLLM

rag = RAGWithLLM("faiss_store", llm_provider="gemini")

custom_prompt = """
Based on this context: {context}

Question: {question}

Provide a detailed analysis with citations.
"""

response = rag.ask(
    question="Compare the methodologies",
    prompt_template=custom_prompt
)
```

##  Security

- âœ… API keys stored in `.env` (not committed to Git)
- âœ… `.env.example` provided as template
- âœ… Comprehensive `.gitignore` rules
- âœ… No hardcoded credentials in source code

See [docs/GIT_GUIDE.md](docs/GIT_GUIDE.md) for security best practices.

## ï¸ Development

### Rebuild Vector Store

```bash
uv run python scripts/rebuild_with_metadata.py
```

### Inspect Documents

```bash
# View document structure
uv run python scripts/inspect_document.py

# View metadata
uv run python scripts/inspect_metadata.py

# Compare metadata versions
uv run python scripts/inspect_metadata_comparison.py
```

### Run Tests

```bash
# Test Gemini integration
uv run python examples/test_gemini_interactive.py

# Test retrieval only
uv run python paper_review_rag.py
```

##  Performance

- **Query Speed**: ~50ms per query (retrieval only)
- **Embedding Speed**: ~1000 docs/minute
- **Memory Usage**: ~200MB (295 chunks loaded)
- **Storage**: ~2MB (vector store only, excluding PDFs)

## ï¸ Roadmap

- [ ] Add more LLM providers (Claude, Llama)
- [ ] Implement hybrid search (dense + sparse)
- [ ] Add conversation memory
- [ ] Web interface with Streamlit/Gradio
- [ ] Batch processing for large document sets
- [ ] Performance benchmarking suite
- [ ] Docker containerization
- [ ] API server with FastAPI

##  Contributing

This is a personal learning project, but contributions are welcome!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

##  License

This project is licensed under the MIT License - see the LICENSE file for details.

##  Acknowledgments

- [LangChain](https://python.langchain.com/) - RAG framework
- [FAISS](https://github.com/facebookresearch/faiss) - Vector similarity search
- [Sentence Transformers](https://www.sbert.net/) - Embedding models
- [Google Gemini](https://ai.google.dev/) - LLM API

##  Contact

For questions or feedback, please open an issue or reach out through GitHub.

---

**â­ If you find this project helpful, please consider giving it a star!**
